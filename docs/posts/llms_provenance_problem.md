---
authors:
  - jamescbury
date:
  created: 2025-08-01
draft: false
categories:
  - Agentic AI Research
tags:
  - ai research
  - context engineering
  - llm
  - enterprise ai
  - provenance
comments: true
---

# LLMs Donâ€™t Have a Memory Problem â€” They Have a Provenance Problem

*The 2025 **Context Engineering Survey**, which reviewed more than 200 research papers and enterprise pilots, cautions: **â€œSimply enlarging an LLMâ€™s context window does not guarantee reliable attribution or auditability; we still need explanation systems, audit mechanisms, and governance structures.â€** ([Context Engineering Survey 2025 Â§4](https://arxiv.org/html/2507.13334v1)). Put differently, the problem isnâ€™t raw memory capacity â€” itâ€™s the provenance of the information we cram inside.*  This is exactly the rationale we followed when designing Context Units for our Pyrana platform.

<!-- more -->

## The New Field of Context Engineering (and Whatâ€™s Missing)

Reviewing the survey hardened my stance that there are 'those building the models' and 'those using the models'.  for the past year or two the *users* were focused on prompt engineering - how do we ask a model to do something in a way that gets the results I want.  We quickly realized that this is not a one-shot experience and users began iterating with the model, engaging with it (or arguing with it as my colleagues and I like to say) and this little back and forth started to max out the context window pretty quick.  So the *builders* came up with novel ways (mostly short cuts) to cram more content into the context window - hence the term *context engineering*.  These efforts include developing longer context windows, external memory buffers, structured prompts, retrieval augmentation â€” the list goes on. The current thinking is simple: feed our models more information so they **remember** more. Advanced **Memory Systems** now let LLMs retain persistent information across chats, transforming them from stateless bots into agents with (simulated) longâ€‘term memory. And yes, these systems can be impressive â€” they page knowledge in and out of an LLM much like a computerâ€™s virtual memory, helping the model carry over facts from one query to the next.

So, more memory should solve everything, right? Not exactly. The survey uncovered a stubborn gap: even as we stuff models with more context, they struggle to **generate** equally sophisticated long outputs. In other words, cramming more into an LLMâ€™s head doesnâ€™t guarantee it can *use* that knowledge effectively. And from a practical perspective, simply increasing the raw amount of context has diminishing returns. **Why?** Because the real world isnâ€™t just about hoarding facts; itâ€™s about trusting them.  

In an enterprise setting, itâ€™s not enough for an AI to recall a policy document from six months ago â€” we need to know *which* version of the policy it used, who approved that content, and whether the modelâ€™s answer can be traced back to a reliable source. **Thatâ€™s provenance.**

If youâ€™ve ever debugged an AIâ€™s output and wished you could see an itemized receipt of *â€œhereâ€™s exactly what I was fed and where it came from,â€* youâ€™re feeling the provenance problem. Sure, memory bandwidth is great, but if an AIâ€™s â€œmemoriesâ€ are a jumbled scrapbook with no labels, weâ€™re just moving the mess from one place to another.

The missing piece in this context engineering is **governance**. How do we **control** and **track** what goes into these everâ€‘expanding context windows? The 2025 survey authors stress the need for â€œexplanation systems, audit mechanisms, and governance structuresâ€ to keep our context engineering on the rails. In plain terms: we donâ€™t just need bigger memory; we need better memory **management**. Enter **Context Units (CxUs)**.

## Context Units: Packaging Knowledge with Provenance

Iâ€™ve written before about Context Units (CxUs) as the â€œatomic fact cardsâ€ of a knowledge base. Think of a CxU as a biteâ€‘sized, immutable packet that captures exactly one idea or fact you care about â€” plus the evidence and metadata that back it up. Each CxU is selfâ€‘contained (claim and context travel together) and **hashâ€‘addressed**, meaning its unique ID is a cryptographic hash of its content. Why go to those lengths? Because it gives each unit a stable identity and tamperâ€‘evidence. If anyone changes a CxUâ€™s content by even one character, its hash (and ID) changes too. No silent edits, no sneaky tweaks. In other words, *truth comes with a checksum*.

Each CxU carries metadata about its origin and status: who created it, when, what version, and even links to source documents or other CxUs it builds on. This builtâ€‘in provenance means you donâ€™t just have a blob of text; you have a **verifiable, structured packet of knowledge** with an audit trail. Itâ€™s like the difference between a paragraph on a blog site versus a notarized fact on an index card â€” one is just text, the other is text with a certificate attached. (except for our blog site since we retain a git commit for every post...)

**Concrete example**  
Suppose we have a regulatory guideline: *â€œIn the EU, a shipped product must include a Declaration of Conformity before customs clearance.â€* In a CxU form, this would be stored as:

* **Claim:** â€œIn the EU, a shipped product must include a Declaration of Conformity before customs clearance.â€  
* **Supporting Context:** â€œDefined in EU RegulationÂ 765/2008, ArticleÂ 30, to ensure product safety and compliance.â€  
* **References:** [EURâ€‘LexÂ 765/2008Â ArticleÂ 30](https://eur-lex.europa.eu/eli/reg/2008/765/oj).  
* **Metadata:** tagged as `regulation`, `EU`, `compliance`, with a version number and timestamp.

Because this CxU is hashed and immutable, if the EU updates their regulation next year, that update becomes a *new* CxU with a different ID (and a pointer back to the prior version). The old fact doesnâ€™t silently morph; itâ€™s versioned like code in Git. We can always trace which version of the rule was applied in any AI interaction. Over time you get a **version chain** â€“ essentially a linked list of how that fact evolved. If an AI answer was based on theÂ 2024 rule and not theÂ 2025 amendment, youâ€™ll know, and you can decide if thatâ€™s a problem. This is **context governance** in action.

Crucially, CxUs also let us compress knowledge *without losing trust*. A 3â€¯000â€‘word policy document might boil down to, say, ten wellâ€‘crafted CxUs capturing its key points. Youâ€™ve just shrunk the context by an order of magnitude, but each CxU still carries a link back to the relevant source text. You gain **knowledge density** (more signal per token) without sacrificing provenance. The modelâ€™s prompt stays focused and efficient, and you maintain an audit trail for every snippet of info included. *More context* isnâ€™t always better; **betterâ€‘curated context** is better.

> CxUs arenâ€™t a silver bullet. Theyâ€™re a design tool â€” a discipline that enforces trustable context.

### The CxU Workflow in Pyrana

1. **Author / Extract**Â â†’ draft CxUs from docs, meetings, or code.  
2. **Validate**Â â†’ schema + rule checks (e.g., conflict detection, required fields).  
3. **Store**Â â†’ contentâ€‘addressable, deduplicated, immutable.  
4. **Retrieve**Â â†’ query by tags, semantic similarity, or explicit IDs.  
5. **Pack**Â â†’ inject selected CxUs (or just their claims+IDs) into the LLM prompt.  
6. **Audit**Â â†’ log every `cxu_id` used so you can reconstruct the chain of thought.

Every step is deterministic and inspectable â€” a far cry from dumping a PDF into a prompt and hoping the model pays attention to the right parts.

## From Lab to Enterprise: CxUs in Action

Letâ€™s make this concrete. Imagine an AI assistant for supplyâ€‘chain compliance. Instead of stuffing it with every regulation PDF, we curate a **library of compliance CxUs** up front. When a user asks, *â€œCan I ship this product to Germany?â€*, our retrieval picks five or six relevant CxUs â€” the EU declaration rule, a German customs clause, and an internal carrier policy. Those units go into the prompt, and the model replies with an answer that cites them:

> â€œYes, you can ship to Germany, but you must include a Declaration of Conformity per EU RegulationÂ 765/2008Â ArticleÂ 30 and use a certified carrier per Internal PolicyÂ XYZ.â€

Because we logged the CxU IDs, a compliance officer can audit exactly which facts were used. The model isnâ€™t just spouting off â€” itâ€™s effectively **citing its chain of thought**.

This scaleâ€‘up beautifully in agentâ€‘oriented architectures. Multiple agents can pass CxUs (or references) instead of freeâ€‘form text, preserving traceability across a complex workflow. Think of it as **endâ€‘toâ€‘end provenance plumbing** for AI.

## Implications and Open Questions for Researchers

*Is this really new?*  
In one sense, no â€” weâ€™re merging ideas from knowledge graphs, contentâ€‘addressable storage, and version control. But the integration is timely. It answers the surveyâ€™s call for **explanation systems and audit mechanisms** in context engineering.

**Research frontiers**

* **Memory Architectures:** Can longâ€‘term LLM memories store *provenanceâ€‘rich* items (CxUs or indices) rather than fuzzy text blobs?  
* **Selective Forgetting:** With provenance metadata, an agent could decide what to keep or purge intelligently.  
* **Standardization:** Could we define a crossâ€‘org CxU schema so companies share knowledge like openâ€‘source libraries?  
* **Humanâ€‘inâ€‘theâ€‘Loop:** What interfaces make authoring and maintaining CxUs painless enough that teams actually do it?

## Conclusion: Toward Trustworthy Context Engineering

More memory isnâ€™t a cure if your memories are a mess. Provenance, versioning, traceability â€” these boring, hardâ€‘toâ€‘implement facets are what make the difference between a demo and a dependable tool. Context Units bake those facets in from first principles. They force us to decide *what we actually know* and *where it came from* **before** we ever prompt the AI.

**Key takeaway:** Better context beats more context. LLMs donâ€™t need an infinite scrapbook; they need a curated set of facts that carry their own receipts. Focus on provenance, not just persistence, and youâ€™ll end up with AI systems you can trust â€” and maybe even take to an audit committee without breaking a sweat.

_Tried structured context or CxUs in your own projects? Iâ€™d love to hear your war stories or questions. Drop them in the comments!_

<!-- BLOG_GIT_METADATA START -->

<div class="blog-git-metadata" style="margin-top: 2rem; padding-top: 1rem; border-top: 1px solid var(--md-default-fg-color--lightest);">
  <details style="background: var(--md-code-bg-color); padding: 0.5rem 1rem; border-radius: 0.2rem;">
    <summary style="cursor: pointer; font-weight: 500; color: var(--md-default-fg-color--light);">
      ğŸ“ Content Provenance
    </summary>
    <div style="margin-top: 1rem; font-size: 0.9em;">
      <p style="margin: 0.5rem 0;"><strong>Created:</strong> 2025-08-03</p>
      <p style="margin: 0.5rem 0;"><strong>Last Modified:</strong> 2025-08-03</p>
      <p style="margin: 0.5rem 0;"><strong>Total Revisions:</strong> 1</p>
      <p style="margin: 0.5rem 0;"><strong>File SHA-256:</strong> <code style="font-size: 0.85em;">87e1ae038eecf8b9...</code></p>
      
      <div style="margin-top: 1rem;">
        <p style="margin: 0.5rem 0; font-weight: 500;">Recent Changes:</p>
        <table style="width: 100%; font-size: 0.85em; margin-top: 0.5rem;">
          <thead>
            <tr style="border-bottom: 1px solid var(--md-default-fg-color--lightest);">
              <th style="text-align: left; padding: 0.25rem;">Date</th>
              <th style="text-align: left; padding: 0.25rem;">Author</th>
              <th style="text-align: left; padding: 0.25rem;">Change</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="padding: 0.25rem;">2025-08-03</td>
              <td style="padding: 0.25rem;">James Canterbury</td>
              <td style="padding: 0.25rem;">added LLM Provenance article</td>
            </tr>
          </tbody>
        </table>
      </div>
      
      <p style="margin-top: 1rem; margin-bottom: 0;">
        <a href="https://github.com/zeroth-tech/blogs/blob/b9419ce5299242a41df9572414a7e2e6dd8eecf8/docs/posts/llms_provenance_problem.md" target="_blank" style="color: var(--md-primary-fg-color); text-decoration: none;">
          View Full History on GitHub â†’
        </a>
      </p>
    </div>
  </details>
  
  <div style="margin-top: 0.5rem; font-size: 0.8em; color: var(--md-default-fg-color--lighter);">
    <p style="margin: 0;">
      <em>This metadata provides cryptographic proof of this document's creation and modification history. 
      The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time.</em>
    </p>
  </div>
</div>

<!-- BLOG_GIT_METADATA END -->

