{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Zeroth Technology blog space","text":"<p>This is where we share our latest thinking, rants, and some ramblings... it's also the archive for everything we've been talking about for years.</p> <p>Our work spans three key areas of research and development:</p> Agentic AI (Pyrana) <p>AI agents, context engineering, and intelligent systems</p> Identity &amp; Authenticity (Vero) <p>Digital identity, attestation, and trust systems</p> Supply Chain &amp; Tokenization <p>Supply chain transparency and tokenization systems</p> <p>Scroll down for a complete listing of all blogs.</p>"},{"location":"tags/","title":"Tags","text":"<p>return to index</p> <p>We add tags to help group our writings.  You can find a list of them here:</p>"},{"location":"tags/#tag:ai-agents","title":"AI agents","text":"<ul> <li>            Data Without Context is Useless          </li> </ul>"},{"location":"tags/#tag:ai-research","title":"ai research","text":"<ul> <li>            LLMs Don\u2019t Have a Memory Problem \u2014 They Have a Provenance Problem          </li> </ul>"},{"location":"tags/#tag:architecture","title":"architecture","text":"<ul> <li>            The Context Unit Blueprint: Architecture, Validation &amp; Graph Retrieval in Pyrana          </li> </ul>"},{"location":"tags/#tag:attested-sensors","title":"attested sensors","text":"<ul> <li>            Attested Sensors &amp; Source Data Synthesis Under Zero Knowledge          </li> </ul>"},{"location":"tags/#tag:authentication","title":"authentication","text":"<ul> <li>            Friction Makes Smoke, and Where There's Smoke...          </li> </ul>"},{"location":"tags/#tag:autonomous-worlds","title":"autonomous worlds","text":"<ul> <li>            Interobjectivity... ?          </li> </ul>"},{"location":"tags/#tag:blockchain","title":"blockchain","text":"<ul> <li>            Friction Makes Smoke, and Where There's Smoke...          </li> </ul>"},{"location":"tags/#tag:content-addressable","title":"content-addressable","text":"<ul> <li>            The Role of Content Addressability in Context Units: Building Distributed Knowledge Systems          </li> </ul>"},{"location":"tags/#tag:context","title":"context","text":"<ul> <li>            Data Without Context is Useless          </li> </ul>"},{"location":"tags/#tag:context-engineering","title":"context engineering","text":"<ul> <li>            LLMs Don\u2019t Have a Memory Problem \u2014 They Have a Provenance Problem          </li> </ul>"},{"location":"tags/#tag:context-windows","title":"context windows","text":"<ul> <li>            Context Efficiency          </li> </ul>"},{"location":"tags/#tag:context-units","title":"context-units","text":"<ul> <li>            The Role of Content Addressability in Context Units: Building Distributed Knowledge Systems          </li> </ul>"},{"location":"tags/#tag:cxu","title":"cxu","text":"<ul> <li>            Context Efficiency          </li> <li>            The Context Unit Blueprint: Architecture, Validation &amp; Graph Retrieval in Pyrana          </li> </ul>"},{"location":"tags/#tag:data","title":"data","text":"<ul> <li>            Data Without Context is Useless          </li> </ul>"},{"location":"tags/#tag:digital-truth","title":"digital truth","text":"<ul> <li>            Nexus: A counter argument to blockchain's \"Achilles heel\"          </li> </ul>"},{"location":"tags/#tag:digital-identity","title":"digital-identity","text":"<ul> <li>            Friction Makes Smoke, and Where There's Smoke...          </li> </ul>"},{"location":"tags/#tag:distributed-systems","title":"distributed-systems","text":"<ul> <li>            The Role of Content Addressability in Context Units: Building Distributed Knowledge Systems          </li> </ul>"},{"location":"tags/#tag:enterprise-ai-agents","title":"enterprise AI agents","text":"<ul> <li>            Are Enterprise AI Agents just RPA rebranded?          </li> </ul>"},{"location":"tags/#tag:enterprise-ai","title":"enterprise ai","text":"<ul> <li>            LLMs Don\u2019t Have a Memory Problem \u2014 They Have a Provenance Problem          </li> </ul>"},{"location":"tags/#tag:hardware","title":"hardware","text":"<ul> <li>            Attested Sensors &amp; Source Data Synthesis Under Zero Knowledge          </li> </ul>"},{"location":"tags/#tag:identity-fraud","title":"identity fraud","text":"<ul> <li>            Enhancing Digital Identity Verification          </li> </ul>"},{"location":"tags/#tag:immutable-storage","title":"immutable-storage","text":"<ul> <li>            The Role of Content Addressability in Context Units: Building Distributed Knowledge Systems          </li> </ul>"},{"location":"tags/#tag:knowledge-density","title":"knowledge density","text":"<ul> <li>            Context Efficiency          </li> </ul>"},{"location":"tags/#tag:knowledge-management","title":"knowledge-management","text":"<ul> <li>            The Role of Content Addressability in Context Units: Building Distributed Knowledge Systems          </li> </ul>"},{"location":"tags/#tag:live-streaming","title":"live streaming","text":"<ul> <li>            Blinky Light Thing: An Approach for Verifiable Real-Time Video          </li> <li>            Enhancing Digital Identity Verification          </li> <li>            The Blinky Light Thing          </li> </ul>"},{"location":"tags/#tag:llm","title":"llm","text":"<ul> <li>            LLMs Don\u2019t Have a Memory Problem \u2014 They Have a Provenance Problem          </li> </ul>"},{"location":"tags/#tag:machine-learning","title":"machine learning","text":"<ul> <li>            Product Token Metadata Profiling          </li> </ul>"},{"location":"tags/#tag:personal-ai-agents","title":"personal AI agents","text":"<ul> <li>            Are Enterprise AI Agents just RPA rebranded?          </li> </ul>"},{"location":"tags/#tag:process-modeling","title":"process modeling","text":"<ul> <li>            Are Enterprise AI Agents just RPA rebranded?          </li> </ul>"},{"location":"tags/#tag:provenance","title":"provenance","text":"<ul> <li>            LLMs Don\u2019t Have a Memory Problem \u2014 They Have a Provenance Problem          </li> </ul>"},{"location":"tags/#tag:robotic-process-automation","title":"robotic process automation","text":"<ul> <li>            Are Enterprise AI Agents just RPA rebranded?          </li> </ul>"},{"location":"tags/#tag:security","title":"security","text":"<ul> <li>            Friction Makes Smoke, and Where There's Smoke...          </li> </ul>"},{"location":"tags/#tag:specification","title":"specification","text":"<ul> <li>            The Context Unit Blueprint: Architecture, Validation &amp; Graph Retrieval in Pyrana          </li> </ul>"},{"location":"tags/#tag:supply-chain","title":"supply chain","text":"<ul> <li>            Contract Wallets as Containers          </li> <li>            Data Asset Token vs. Product Token          </li> <li>            Digital Twin \u2260 Product Token          </li> <li>            Does Knowing Make You Accountable?          </li> <li>            Interobjectivity... ?          </li> <li>            Product Token Metadata Profiling          </li> <li>            Product-Oriented Data Structure          </li> <li>            Wallets vs. Token Metadata          </li> <li>            Why hasn\u2019t blockchain solved our supply chain issues yet?          </li> </ul>"},{"location":"tags/#tag:tokenization","title":"tokenization","text":"<ul> <li>            Contract Wallets as Containers          </li> <li>            Data Asset Token vs. Product Token          </li> <li>            Digital Twin \u2260 Product Token          </li> <li>            Does Knowing Make You Accountable?          </li> <li>            Product Token Metadata Profiling          </li> <li>            Product-Oriented Data Structure          </li> <li>            Wallets vs. Token Metadata          </li> <li>            Why hasn\u2019t blockchain solved our supply chain issues yet?          </li> </ul>"},{"location":"tags/#tag:trust","title":"trust","text":"<ul> <li>            Friction Makes Smoke, and Where There's Smoke...          </li> </ul>"},{"location":"tags/#tag:vero","title":"vero","text":"<ul> <li>            Friction Makes Smoke, and Where There's Smoke...          </li> </ul>"},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/","title":"Blinky Light Thing: An Approach for Verifiable Real-Time Video","text":"<p>Zeroth Technology, Inc</p> <ul> <li>James Canterbury</li> <li>Patrick Macom</li> <li>Ankur Garg</li> </ul> <p>https://zeroth.technology</p> <p>August 1st, 2024</p>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#executive-summary","title":"Executive Summary","text":"<p>This white paper introduces the Blinky Light Thingy (BLT), a novel approach for real-time video authentication in live-streaming environments. The BLT embeds cryptographic messages into the physical environment being recorded, making it difficult to modify the video feed in real-time and avoiding the reliance on specialized trusted hardware. This approach addresses the growing challenge of distinguishing between authentic and generative content, particularly in the context of live video streams where it is becoming more common place for someone to \"wear\" the filter of another person and perpetuate fraud in what is typically considered a trusted environment. By utilizing Ethereum Attestation Services, a \"do-it-yourself\" hardware device, and open sourced encoding/decoding software, BLT provides a robust method for verifying both the identity of the presenter and the contemporaneousness (real-time nature) of the content.</p>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#abstract","title":"Abstract","text":"<p>We propose an approach for real-time video authentication in live-streaming environments. The approach, referred to as the BLT, embeds cryptographic messages into the physical environment being recorded, making it difficult to modify the video feed in real-time and avoiding the reliance on specialized trusted hardware. This approach addresses the growing challenge of distinguishing between authentic and generative content, particularly in the context of live video streams where it is becoming more common place for someone to \"wear\" the filter of another person and perpetuate fraud in what is typically considered a trusted environment. By utilizing Ethereum Attestation Services, a \"do-it-yourself\" hardware device, and open sourced encoding/decoding software, BLT provides a robust method for verifying both the identity of the presenter and the real-time nature of the content.</p> <p></p>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#1-issue","title":"1. Issue","text":"<p>The proliferation of deep fake technology and real-time video filters has made it increasingly difficult to trust the authenticity of live video streams. Current methods of verification rely heavily on the reputation of presenters or broadcasters, which can be easily exploited. As deep fake technology advances, there is a pressing need for a system that can verify both the identity of the presenter and the real-time nature of the content.<sup>1</sup>  Viewers of live-streaming deep fakes are particularly vulnerable because humans have a tendency to believe what they see and up until this point in our technical development that maxim has typically held true.  Even though the general population is becoming more skeptical of digital content, when faced with real-time decisions of what is true vs. what is not, it has not become a reflex for us to verify; a large contributor to the lack of this reflex is an easy means of doing so.  Consider the scenario of a conference call between you and your banker, perhaps someone you have met in real life several times.  A victim of fraud is much more likely to share private information, or consent to executing a financial transaction, if they are engaged in a video call (e.g., a Zoom meeting) where the person they are talking to looks and sounds like the banker they know and trust.  Consider the same scenario between an individual and a lawyer, a doctor, law enforcement, or a family member.  Not to mention the increasing prevalence of the population who trusts information shared in live-streaming video from our governments, authorities, and social media influencers.</p> <p>Live deep fakes are accomplished by applying real-time filters, similar to those used for entertainment purposes where a person might make themselves look like a talking pickle. However, these filters are becoming increasingly sophisticated and can now map the vocal, facial and body movements of an actor onto a model of the target, creating a convincing impersonation. [include references here].  While these filters can predict what a person would look and sound like if they were saying some fictitious content, they cannot predict changes in the physical environment, particularly if those changes are tied to a digital identity and using a method that is extremely unlikely to be predicted.  This weakness provides an opportunity for authentication.</p> <p>Note</p> <p>Deep fake video vs. live-streaming deep fakes</p> <p>We would like to call attention at this point to the differentiation between \"live-streaming deep fakes\" and deep fake videos that are generated and then later released to the audience.  A video that is recorded first and then released will always be subject to post-recording changes such as video editing, compression, and metadata updates prior to distribution.  Because of these production steps there is a different set of potential authentication methods, such as those laid out by C2PA.org and other video authentication approaches, that are more applicable.  Current solutions to the authentication of live-streaming events (such as pay-per-view boxing or NFL Sunday Football) rely on customized client-server encryption methods - which were developed for the purposes of constraining delivery to paying customers - these also ensure that the content is authentic but are intended for broadcast audiences and require significant IT infrastructure.  The BLT is focused on those video narrowcasts that are to one-on-one or one-to-several streams though. they can be equally applied to one-to-many.</p> <p>To fully address the issue we analyze the approaches to propagate live deep fakes using the following adversarial model:</p> <ol> <li>Honest Presenter: Someone currently live-streaming as themselves.</li> <li>Dishonest Presenter: Someone claiming to be live streaming when in fact the video was prerecorded. And/or someone claiming to be someone else.</li> <li>Honest Audience(s): One or more people watching the content produced by the presenter.</li> <li>Dishonest Audience(s): One or more people who claim the content they are consuming is different from what the presenter is producing (less common but included for completeness).</li> </ol> <p>The BLT is focused on exposing dishonest presenters but it also has the effect of making it difficult for a dishonest audience to change a live-stream that they might have recorded, as discussed in more detail below by adding a closing attestation.</p>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#2-objective","title":"2. Objective","text":"<p>The primary objectives of the BLT approach are:</p> <ol> <li>To provide a reliable method for verifying the identity authenticity of video content producers or presenters.</li> <li>To ensure the contemporaneousness of live video streams.</li> <li>To create an open source, openly verifiable system that is resistant to real-time video manipulation techniques.</li> </ol>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#3-approach","title":"3. Approach","text":"","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#31-hardware-components","title":"3.1 Hardware Components","text":"<p>The BLT approach introduces cryptographic elements into the physical environment being recorded. This is achieved by:</p> <ol> <li>Obtaining a unique identifier (UID) from Ethereum Attestation Services (EAS) <sup>3</sup> which includes specific information about the encrypted message within the live-stream and is signed using the presenter's address.  The creation of a UID is dependent upon the nonce of the block in which it is created, making the UID impossible to predict ahead of time.</li> <li>Broadcasting this UID by converting it to a binary string and blinking a light into the physical environment during the live stream.</li> <li>Enabling viewers to decode the message and match it to the attestation through pixel analysis.</li> </ol> <p>This approach makes it extremely difficult to modify the video feed in real-time, as:</p> <ul> <li>The cryptographic message is embedded in the physical environment itself</li> <li>The message is impossible to determine ahead of time as it relies on the blockchain nonce</li> <li>The message is cryptographically signed by the presenter using their private wallet address</li> </ul> <p>Further, by signing an additional closing attestation the presenter can commit a hash of the presented video content to the blockchain (a common method for proving video authenticity post-production), that references the initial UID effectively bookending the broadcast in a way that makes it probabilistically impossible for someone other than the signer to have produced the video broadcast.</p> <p>Our approach is guided by the following principles:</p> <ol> <li>The production of verifiable live-streaming content should be based on public infrastructure (i.e., Ethereum Attestation Services) that is independent of the both the presenter and the audience.</li> <li>Verification of live-streaming video must be available to any potential viewer and is strengthened by having multiple independent clients that can verify.</li> <li>The methods for embedding and detecting the message should be as simple as possible.</li> </ol>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#32-software-components","title":"3.2 Software Components","text":"<p>The software component of the BLT system consists of several key elements:</p> <ol> <li> <p>BLT Configuration: Establishes a baseline of pixel composition to optimize BLT performance.  It utilizes your webcam and is intended to be run before each broadcast session to ensure that your message can be verified by your audience. It leverages the webcam you plan to use to present, and assumes that your background and lighting will not change significantly.  During the configuration test, a predetermined test message is passed to the device and blinked out for detection.  The user has the option to configure the following parameters of the BLT device to optimize the detection rate (or similarity score) of your planned broadcast message:</p> <ul> <li>Message Color: The color LED selected to represent the message being broadcast.</li> <li>Terminator Color: The color LED selected to represent the beginning of the message broadcast or the beginning of a message broadcast chunk.</li> <li>Blink Rate: The period for which the message color LED is either lit or unlit (blinked) representing either a 1 or a 0 respectively.</li> <li>Message Chunks: A selection of either 1, 2, 4, 8 that indicates the number of \"chunks\" the broadcast message will be split into.  See Message Chunking below for more details.</li> </ul> <p>The BLT Configuration leverages both the Encoding and Decoding Algorithms to help optimize the detection rate. The output of the configuration test is a baseline RGB representing the average mixture of red, green, and blue pixels of your broadcast when there is no message being blinked.  This assists in the blink detection and the RGB Max filter described in Blink Detection below.</p> </li> <li> <p>Wallet Connect: Provides functionality to connect (or create) a crypto wallet that will be used for the purposes of signing the attestation.  It is important that this wallet address be publicly associated with the presenter as further discussed in Aliases and Reputations.</p> </li> <li>Create Attestation: Utilizes Ethereum Attestation Services to create a unique attestation for each broadcast session. The attestation includes the configuration of the BLT device, and a baseline of the red, green, and blue pixels (the RGB baseline). Optional fields include your full name and a description.  As one of the objectives of the BLT is to prove the authenticity of the speaker inclusion of your full name, or the name your audience knows you as, is highly recommended; however, as discussed in Aliases and Reputations below this is optional. The project team has defined schemas on EAS for the BLT on most of their available networks, the appropriate schema can be selected when connecting - alternate schemas can be used as well, and for private broadcasts the attestation can be done off-chain as well.  The unique identification (UID) generated by EAS when the attestation is complete is the message that will be broadcast.  Of critical importance to proving contemporaneousness is the fact that this message is dependant on the nonce of the network which has an extremely low likely hood of being derived ahead of time.</li> <li>Encoding Algorithm: Converts the 64 character hexadecimal Attestation UID into a 256 character binary string (the \"encoded message\") which can be interpreted as a series of light blinks.  Each bit (blink) of the string is represented by a specific light state (1=on or 0=off) for a defined duration (the blink rate) in a specific message color (red, green, or blue).  To assist in expediency of detecting a blink, and to minimize the effect of synchronization errors, the encoded message is \"chunked\" in to a set of 2, 4, or 8 substrings. Each substring, including the beginning of a message, is preceded by a \"terminator\" blink of a specific color that is different than the message color.</li> <li>Retrieve Attestation: Uses the EAS UID generated in the Create Attestation component to access the attestation on EASscan.org and retrieve the BLT configuration which is needed to assist in the detection of the message.  It is important to note a key design decision here - this assumes the presenter is able to share the UID with the audience (this can be done through a link or QR code during the presentation).  This approach effectively gives the expected message to the audience there by letting them verify against something known.  As opposed to trying to detect a full message string without knowing the detected message and verifying later.  This approach lets us work with a confidence interval (represented as a similarity score) which can be used to account for minor changes in the video lighting, video compression, broken feeds, etc.</li> <li> <p>Parsing Config: This simply breaks down the returned attestation data from EAS and stores it locally as variables for:</p> <ul> <li>Message Color</li> <li>Terminator Color</li> <li>Blink Rate</li> <li>RGB Baseline</li> </ul> </li> <li> <p>Decoding Algorithm: Analyzes the video feed by applying an RGB max filter to the live-stream video and taking a sample at a set interval (the sample rate - which must be less than or equal to the blink rate).  This filter maximizes the dominant color in each pixel and reduces the other colors to 0.  For example if a given pixel is R130, G120, B4 the filter will convert it to R255, G0, B0.  The filter baseline is then derived for red, green, and blue by taking the aggregate RGB values in proportion to the number of pixels. Later this RGB baseline is subtracted from the aggregate RGB values for a given sample to determine if there is a \"spike\" in either the red, green, or blue pixel count.  If no blink is detected it is assumed that the message for that sample is a \"0\", if a spike is detected in the message color the message for that sample is a \"1\", if a spike is detected in the terminator color the message for that sample is the first letter of the terminator color (R,G,B).  Each detected message is added to a continuously growing detected message string.  When a terminator color is detected the position in the string is temporarily stored.  When a second terminator message is detected the string between the previous terminator message and the current terminator message is extracted (assuming it is greater than 80% of the expected chunk length).  The detected message chunk is then passed to the verification component and the terminator position is updated.</p> </li> <li> <p>Verification: Attempts to match the detected message of the live-stream video feed to the expected message of the UID.  To facilitate this the expected message is \"expanded\" by utilizing the ratio of the \"sample rate\" at which the streaming video is analyzed to the \"blink rate\" at which the message is being broadcast (for example, a transmission of '0110' at a blink rate of 100 milliseconds per bit and a sample rate of 50 milliseconds against the live stream video feed would convert to an expanded message of '00111100').   The verification module compares each detected message (the series of 0's and 1's between the terminator blinks) to each \"message chunk\" of the expected message and returns a similarity score based on how many successful matches to detected blinks versus expected blinks were found.  The verification module uses a \"fuzzy similarity matching\" based on the Jaccard Similarity approach to also detect nearby matches.  This can be helpful to account for video buffering or compression due to broadcast latency.  A 100% match would indicated an exact mapping to a given message chunk.</p> <p>The ultimate goal of the software components is to generate an \"authenticity score\" that is based on the similarity to scores of each successful chunk match, the order in which the chunks were matched, and the number of times the matching was repeated.  This a high authenticity score indicates a high probability that the message detected in the live video stream is in fact the message being broadcast by the presenter.  Our target is a similarity score of 70% which would require matching 180 bits or more out of 256. The probability of this occurring by chance is approximately 1.52 \u00d7 10^-15, which is vanishingly small.</p> </li> <li> <p>Close Live-Stream: As an optional step when the live stream is complete the presenter can create another attestation (using a different EAS Schema) that references the earlier attestation.  This has the effect of \"bookending\" the live stream with two attestations.  The presenter has the option to include a hash of the presented stream in the closing attestation which could serve as a convenient starting point for any post-live-stream editing they might do under an attributable video editing scheme like C2PA.org.</p> </li> </ol> <p></p>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#33-cryptographic-principles","title":"3.3 Cryptographic Principles","text":"<p>The BLT approach leverages cryptographic principles to ensure the integrity and authenticity of the broadcast message. The following principles are key to the BLT approach:</p> <ol> <li>Cryptographic Signature: The UID generated by Ethereum Attestation Services is cryptographically signed by the presenter's private wallet address. This signature ensures that the UID was generated by the presenter and cannot be tampered with.</li> <li>Nonce Dependency: The UID is dependent on the nonce of the block in which it is created. The nonce is a random number that is generated for each block in the Ethereum blockchain. Since the nonce is not known until the block is mined, the UID cannot be predicted ahead of time.</li> <li>Public Infrastructure: The BLT approach relies on public infrastructure (Ethereum Attestation Services) that is independent of the presenter and the audience. This ensures that the UID is generated in a trustworthy and verifiable manner.</li> <li>Open Verification: Verification of the live-streaming video must be available to any potential viewer. This is strengthened by having multiple independent clients that can verify the broadcast message.</li> </ol>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#4-software-design","title":"4. Software Design","text":"<p>The software design of the BLT system is based on the following components:</p> <ol> <li>BLT Configuration: Establishes a baseline of pixel composition to optimize BLT performance.</li> <li>Wallet Connect: Provides functionality to connect (or create) a crypto wallet that will be used for the purposes of signing the attestation.</li> <li>Create Attestation: Utilizes Ethereum Attestation Services to create a unique attestation for each broadcast session.</li> <li>Encoding Algorithm: Converts the 64 character hexadecimal Attestation UID into a 256 character binary string (the \"encoded message\") which can be interpreted as a series of light blinks.</li> <li>Retrieve Attestation: Uses the EAS UID generated in the Create Attestation component to access the attestation on EASscan.org and retrieve the BLT configuration which is needed to assist in the detection of the message.</li> <li>Parsing Config: Breaks down the returned attestation data from EAS and stores it locally as variables for the BLT configuration.</li> <li>Decoding Algorithm: Analyzes the video feed by applying an RGB max filter to the live-stream video and taking a sample at a set interval (the sample rate - which must be less than or equal to the blink rate).</li> <li>Verification: Attempts to match the detected message of the live-stream video feed to the expected message of the UID.</li> <li>Close Live-Stream: As an optional step when the live stream is complete the presenter can create another attestation (using a different EAS Schema) that references the earlier attestation.</li> </ol>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#message-chunking","title":"Message Chunking","text":"<p>A 64 character hexadecimal string converted to a binary string that can be \"blinked out\" is 256 characters in length plus any terminator signals that might be added.  To match an encoded string to a detected message with 100% similarity, the individual bits must be completely aligned.  There is a \\(1:2^{256}\\) probability of a random string matching the encoded message.  However, this can be difficult to do for long strings because the detector may miss the starting point.  To reduce this risk, and to give more frequent feedback on the detection success, an encoded message can be \"chunked\" into equal substrings of lengths 128, 64, or 32 characters; creating either 1, 2, 4, or 8 chunks.  The drawback of this approach is that when matching a detected chunk to an expected chunk, one must check each chunk for similarity.  Further, because the odds of correctly guessing a binary string scale exponentially as \\(2^{stringLength}\\), having larger chunks is more secure.  However, if it can be shown that a match is made in chunk 1, followed by chunks 2, 3 &amp; 4, it is effectively the same as matching the full 256 length string.</p>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#blink-detection","title":"Blink Detection","text":"<p>Blink detection is the identification of a color source in the live-stream video that is produced by either the message color LED or the terminator color LED.  Ideally this detection occurs in the area of the video that is the presenters face which has three benefits.</p> <ol> <li>It is more difficult to alter the light source on a filter that is changing someone's appearance than it is to alter the light source in the background.</li> <li>It allows us to create a smaller canvas to sample from and calculate the RGB levels which makes verification much easier.</li> <li>The skin reflectiveness of a presenter does not change often, but the clothes they wear, or the background they are in front of does.  This lets the presenter keep similar configuration setting between multiple live-streams.</li> </ol> <p>For the purposes of this paper we did this by simply creating a canvas where the presenter's head is.  In future versions we would likely employ face detection software that can isolate the presenters face even further <sup>2</sup>.  For our filter we chose to apply a simple RGB approach, but this too can be improved in the future by isolating specific tones of to the message and terminator blink colors.</p> <p>The below examples show the RGB Max filter applied in different lighting conditions</p> <p> </p> <p>There are 3 additional factors that can be configured in the bLT that assist in optimizing the detection rate of the encoded message.</p> <ol> <li>LED Intensity:  This simply modifies how bright the LEDs are which is more of a user preference.  The intensity is highly dependent on the ambient light source.</li> <li>Message Color and Terminator Color: Allowing the user to select both the message color and the terminator color (so long as they are not the same) provides options that account for varying RGB baselines.  Since there is the most sensitivity in detecting the encoded message it is recommended that the least dominant color in the RGB baseline be selected for the message color.  This provides the highest contrast to the other colors.</li> <li>Blink Rate: The blink rate is the duration for which the message color LED will be on to indicate a 1 and off to indicate a 0.  Setting a longer duration allows for more redundancy to occur in the detection, but also effects the amount of time it takes to detect one full message. If the blink rate is too short the verification module may not be able to pull enough samples to detect it.  At a minimum the blink rate must be longer that the frame rate of the live-stream video which is typically 30 frames per second or about 33.33 ms per frame.  This of course is also the minimum sample rate for detection, so we recommend that the minimum blink rate be at least double.  A terminator blink is three successive on blinks of the terminator color.  The terminator is blinked prior to each message chunk.</li> </ol> <p>The following formula can be used to determine the amount of time it would take to fully transmit a message:</p> \\[(256*blinkRate)+(3*blinkRate*messageChunks)\\] <p>For example, a message split into 4 chunks at a blink rate of 100 ms would take 28.6 seconds.</p>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#aliases-and-reputations","title":"Aliases and Reputations","text":"<p>A large part of proving authenticity relies on the fact that the presenter is already known to their audience.  In using a crypto wallet that is owned by the presenter we are able to prove that only them (or whoever has access to the private key for that wallet) is able to generate the EAS UID.  This necessarily ties the presenter to that wallet which may not be desireable in all cases as they can then be publicly linked to the entire history of that wallet.  If the presenter creates a new wallet just for the purposes of presenting then they will need to gain some history in order for their audience to know that wallet is in fact controlled by them.  One way to help mitigate these issues is for the presenter to hold a credential in their wallet that is issued by a trusted third party or is generated by some reputation system such as gitcoin passport.  Adding identity and reputation components to the wallet is beyond the scope of this paper, but is an important factor to consider.</p>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#5-hardware-design","title":"5. Hardware Design","text":"<p>The BLT hardware is designed to be simple yet effective.  Our tests have found that using dedicated hardware that has ultra-bright LEDs has a higher success rate of message detection.  However, there is also an available option to use a static website (available on mobile) to make the screen of your phone or monitor blink out the encoded message and the corresponding terminators.  Our experience tells us that while these approaches can be effective, they are quite distracting to the presenter.</p> <p>The dedicated hardware consists of:</p> <ol> <li>Light Source: A controllable LED or similar light-emitting device.  Our design uses three ultra bright LED - Red, Green, Blue [LED Specs].  Some boards also include a white LED which can help to offset the harshness of the RGB LEDs in some lighting conditions.</li> <li>Controller: A micro controller to manage the light blinking pattern based on the encoded UID.  The controller should be capable of connecting via a bluetooth low energy (BLE) connection to a PC or mobile device (note, iOS prevents the integration of web enabled bluetooth connections without the installation of additional third part softwares.  However, android mobile devices and most desktop web browsers (including those for MacOS) have this capability.  </li> <li>Power Supply: Ensures consistent operation throughout the streaming session.  We recommend a rechargeable LiPo battery that meets the specification for your selected micro controller or two (2) AAA Batteries and a 5v step up converter to power the micro controller and LEDs.  An external v5 Power supply can used if your selected micro controller supports it.</li> <li>Switch: To turn on/off the power to the device.</li> <li>Mounting System: Allows for easy positioning and directing of the light source.</li> </ol>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#6-build","title":"6. Build","text":"<p>The BLT can be constructed using readily available components. A step-by-step guide for building a BLT is in development. Alternatively, pre-built units can be obtained from the project team.</p> <p>For those unable to use a dedicated BLT device, a smartphone or computer screen can serve as a substitute light source, although with reduced effectiveness.</p>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#7-results","title":"7. Results","text":"<p>  Using a BLT device built from an ESP32C microcontroller, a 3.7v LiPo battery, a red, green, and a blue LED and matching resistors as shown in the circuit diagram, we tested the BLT in various lighting conditions and with various configurations.  A test hex string of:</p> <p>AAAAAAAAAAAAAAAACCCCCCCCCCCCCCCCF0F0F0F0F0F0F0F0FF00FF00FF00FF00</p> <p>was used, which converts to a binary string of:</p> <p>1010101010101010101010101010101010101010101010101010101010101010 1100110011001100110011001100110011001100110011001100110011001100 1111000011110000111100001111000011110000111100001111000011110000 1111111100000000111111110000000011111111000000001111111100000000</p> <p>This string was designed to give a wide variation between rapid blinks (on/off/on/off) and successive blinks (on/on/on/on/off/off/off/off) to determine if the verification module was sensitive enough to capture both.</p> <p>A baseline detection was done in an indoor room with steady ambient light (a scenario most likely to be similar to an office or work room).  Using a blink rate of 100 milliseconds, a sample rate of 50 milliseconds, message chunks of 4, the blue LED for the message, and the green LED for the terminator, we were able to achieve a consistent 90+ similarity match across the four chunks. Below is an example output of a verification that ran for about 2 minutes. The message detection rate is a measure of the possible number of messages to be detected in a given time frame vs. the actual number of messages detected (these are reported as X out of X).  Note that if detection of a broadcast message starts midway through a message chunk and ends midway through a message, it may be perfectly reasonable to have \"missed\" the beginning and ending message chunk; therefore, the target is to detect the total possible message chunks minus 2.</p> <p>Average Similarity: 0.91</p> <p>Message Detection Rate: 18:20</p> Chunked Message: Count Similarity Message Detection [1]1010101010101010101010101010101010101010101010101010101010101010 5 0.83 5:5 [2]1100110011001100110011001100110011001100110011001100110011001100 5 0.89 5:5 [3]1111000011110000111100001111000011110000111100001111000011110000 4 0.94 4:5 [4]1111111100000000111111110000000011111111000000001111111100000000 4 0.98 4:5 <p>In this configuration, the message will broadcast every 26.8 seconds or 4.47 times in two minutes.  Note that chunks 1 and 2 were detected 5 times where as 3 and 4 were detected 4, this indicates that the test was stopped midway through the 5th broadcast.  As out target is a similarity score of .7 or greater this baseline is considered a success.</p> <p>Below is a summary of our observations when testing various configurations:</p> <ol> <li>Lower LED Intensity: As expected in nearly every scenario this fell below our threshold similarity score with the exception of detecting in a dark room. Conclusion: while a lower intensity is not as distracting to the presenter the results where greatly diminished.  Unless we can enhance the detection through other efficiencies, it is recommended to keep the intensity on high.</li> <li>Smaller Chunks: Increasing the number of chunks to 8 (thereby reducing the chunk length to 32) showed an increase in the average similarity score of 0.05 giving.  The trade off is slightly more processing power as each detected chunk is matched against all chunks in our current verification component.  Conclusion:  More chunks of shorter length seem to be better.  Enhancements to the matching algorithm can be made to look for the next expected chunk first and return if the match is over a certain threshold.</li> <li>Larger Chunks:  Reducing the number of chunks to 2 or 1 (thereby increasing the chunk length to 128 or 256) resulted in less than 50% of the detected strings being of sufficient length to verify resulting in a similarity score of less than 40%.  In addition, because the chunks were so long, missing a terminator had the effect of needing to wait for nearly an entire message transmission.</li> <li>Faster Blink Rate: Reducing the blink rate by 50% (to 50 ms) and reducing the sample rate to 25 ms resulted in a significant drop in similarly score to 50%.  Of specific note was that a number of encoded messages were missed and that the chunk one (consisting of rapid 0101 blinks) was the most likely to be detected.</li> <li>Slower Blink Rate: Blink rates of 200ms and 400ms both returned average similarity scores of 0.96 and 0.98 respectively indicating that longer blinks are easier to detect.  However it was noted that this also increases the time it takes to broadcast an entire message completely, a 400ms takes 1.15 minutes.  Conclusion:  The trade off between a slightly higher similarity score versus a longer message time (and less opportunity for the message to broadcast multiple times) indicates that a slow blink rate should only be used to offset poor lighting conditions.</li> <li>Faster Sample Interval: Reducing the sample interval had the effect of expanding the detected message (similar to increasing the chunk length).  This resulted in less detected messages reaching the minimum length to be matched.  It also introduced more variance detection, for example if lighting conditions caused an increase in green pixels for even a short amount of time the lower sample interval was more likely to pick it up and indicated a false positive termination blink.  Conclusion: In theory a faster sample interval should result in better matches, however we would need to refine our matching algorithm to take this into account.  Considering that faster sample intervals also require more frequent processing this path may not be worth pursuing.</li> <li>Different Ambient Lighting Conditions: Lighting has the single largest effect on the the ability to detect the broadcast message (as expected).  The BLT blinks LEDs that reflect off of the presenters face while presenting.  It follows that high levels of white light also reflecting off of the presenter, or directly shining into the webcam will reduce the detectable blinks.  In stable lighting conditions we found that the BLT could be positioned in such a way that the message can be detected in most scenarios, but in changing lighting conditions (such as outdoors) it was much more difficult.  Similarly different skin reflectivity<sup>4</sup> effects blink detection.  The reflectance of skin varies based on factors like melanin content, blood flow, and hydration levels.  For fun we also tested the BLT in a dark room - the results were a perfect similarity score.  Conclusion: Future enhancements are expected to greatly improve the range of conditions in which the BLT can operate effectively - different configurations discussed in the further explorations below will also have a big impact in neutralizing background light.  However, for the current version it is recommended using the BLT in a room with a slightly lower level of ambient light.</li> </ol> Configuration Blink Rate Chunk Size Sample Rate Average Similarity Score Message Detection Rate Baseline Test 100 ms 4 chunks 50 ms 0.91 18:20 Medium LED Intensity 100 ms 4 chunk 50 ms N/A N/A Low LED Intensity 100 ms 4 chunks 50 ms N/A N/A Fewer Chunks 100 ms 2 chunks 50 ms 0.94 9:10 More Chunks 100 ms 8 chunks 50 ms 0.96 34:40 Faster Blink Rate 50 ms 4 chunks 50 ms 0.51 23:40 Slower Blink Rate 400 ms 4 chunks 50 ms 0.97 10:10 Faster Sample Detection 100 ms 4 chunks 10 ms 0.91 1:20","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#8-some-things-that-didnt-work","title":"8. Some Things That Didn't Work...","text":"<p>In the design of the BLT, we tried several different configurations.  We share them here so that others might avoid them or perhaps see the error in our approach and help correct them.</p> <ol> <li>Using an infrared LED as the message color.  The intention here was to use an LED color that outside of the visible light spectrum so that the blinking message would be less distracting.  However, this approach violated our principal of not requiring customized hardware to record the video.  Most webcams have an IR filter built in, and in order for this to work well we needed to remove that filter.  While it is true that some IR is detected - we found that it needed to be directly pointed at the webcam and was not sufficient to reflect off of the presenters face.  Further we found that the small amount of IR that was detected is rendered as red in the video which is difficult to detect.</li> <li>Using ultra-violet LEDs as the message color.  Along a similar line as the infrared LED, we experienced greater success using the UV LEDs.  But to do so we needed to increase the intensity and this was very visible to both the presenter and the audience.  We also found that that the video renders the UV light as bluish which in effect increases the blue pixel count in our detection module.  After testing different configurations we determined that using a simple blue LED was more effective, and it reduces the risk of the presenter getting sunburn.</li> <li>Multiple LEDs configured in different patterns.  We found that this had limited effect on the ability to detect the message other than increasing the intensity.</li> <li>Including white LEDs to mute the embedded message.  While this worked to make the blinking LED less distracting it also muted the ability to detect the message.</li> <li>Embedding the message in the audio signal as well.  We briefly explored this path and were stifled by the myriad of noise cancelling technology used to clarify audio which were eliminating the frequencies we had tried to broadcast on.  However, there is promise in this approach and it significantly enhances the security of the BLT as it gives a second mode of embedding a message.  For this reason we've included this in the future explorations section below.</li> </ol>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#8-further-explorations","title":"8. Further Explorations","text":"<p>Future developments and research areas for the BLT system include:</p> <ol> <li>Enhanced Encoding Techniques: Exploring more sophisticated methods of embedding information in video streams, where we can make use of correcting and calibration signals so that less information is required to be passed to the detection module.</li> <li>Enhanced Detection Techniques: Application of face detection or more advanced filters to reduce errors in blink detection.  This in effect would allow for a faster blink rate and shorter message duration.</li> <li>Custom Lighting Solutions: Embedding the BLT into lighting solutions (i.e., ring lights, studio lights, etc.) that are commonly used by presenters.  This could in effect allow for the BLT to reflect off of multiple surfaces and make it easier to detect while making it less distracting for the presenter.</li> <li>Integration with Existing Platforms: Developing plugins or APIs for popular video conferencing and streaming platforms.</li> <li>Multi-Device Synchronization: Enabling multiple BLT devices to work in concert for more complex authentication scenarios.</li> <li>AI-Resistant Features: Continuously updating the system to stay ahead of advancements in deep fake technology<sup>5</sup>.</li> <li>Legal and Regulatory Compliance: Ensuring the BLT system meets emerging standards for video content verification in legal and official contexts.</li> </ol>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#process-steps","title":"Process Steps","text":"<p>To use the BLT system, follow these steps:</p> <ol> <li>Connect to BLT: Establish a connection between your streaming device and the BLT hardware.</li> <li>Configure BLT: Run a configuration test to set a baseline of your pixel composition. This optimizes the BLT performance and should be reset whenever background lighting or clothing changes significantly.</li> <li>Create EAS Attestation: Generate an attestation from an account that is provably yours using Ethereum Attestation Services. This attestation includes the configuration of your BLT.</li> <li>Broadcast Your Live-stream: Begin your live stream with the light from the BLT visible within the frame.</li> <li>Verify the Live Stream: Viewers can use the BLT decoding software to verify the authenticity and contemporaneousness of the stream in real-time.</li> </ol> <p>Below is a process flow for using the BLT.</p> <p></p>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#conclusion","title":"Conclusion","text":"<p>The Blinky Light Thing presents a novel approach to addressing the growing challenge of live-streaming video authentication in the age of deep fakes. By embedding cryptographic messages directly into the physical environment of a video stream, BLT provides a robust method for verifying both the identity of presenters and the real-time nature of content. This approach is unique in that it does not require custom hardware such as attested sensors or specialized cameras (the actual BLT device is simple by comparison). The major drawback to the BLT is that it is distracting to presenters and potentially to audiences.  To some, this may be viewed as a symbol of authenticity, but to most, it is just annoying. To a few, exposure to rapidly blinking lights is dangerous, and we would exercise caution in its use.  We believe that with some modifications, the BLT approach can become less obtrusive and more ubiquitous to streaming communications.  As deep fake technology continues to advance, systems like BLT will play a crucial role in maintaining trust in digital communications.</p>","tags":["live streaming"]},{"location":"2024/08/01/blinky-light-thing-an-approach-for-verifiable-real-time-video/#references","title":"References","text":"\ud83d\udcdd Content Provenance      <p>Created: 2024-09-24</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 6</p> <p>File SHA-256: <code>5633cf10e46b0a31...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-11-01 James Canterbury posting data asset tokens blog and updates to BLT 2024-10-08 James Canterbury updates to BLT white paper and publish <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p> <ol> <li> <p>Tolosana, R., Vera-Rodriguez, R., Fierrez, J., Morales, A., &amp; Ortega-Garcia, J. (2020). Deepfakes and Beyond: A Survey of Face Manipulation and Fake Detection. Information Fusion, 64, 131-148. https://www.sciencedirect.com/science/article/abs/pii/S1566253520303110 \u21a9</p> </li> <li> <p>Viola, P., &amp; Jones, M. (2004). Robust Real-Time Face Detection. International Journal of Computer Vision, 57(2), 137-154. https://www.face-rec.org/Algorithms/Boosting-Ensemble/16981346.pdf \u21a9</p> </li> <li> <p>Ethereum Attestation Service. (n.d.). EAS Documentation. Retrieved from https://docs.attest.sh/ \u21a9</p> </li> <li> <p>Anderson, R. R., &amp; Parrish, J. A. (1981). The Optics of Human Skin. Journal of Investigative Dermatology, 77(1), 13-19. https://pubmed.ncbi.nlm.nih.gov/7252245/ \u21a9</p> </li> <li> <p>Nguyen, T. T., Nguyen, C. M., Nguyen, D. T., Nguyen, D. T., &amp; Nahavandi, S. (2021). Deep Learning for Deepfakes Creation and Detection: A Survey. Computer Vision and Image Understanding, 203, 103150. https://www.scirp.org/journal/paperinformation?paperid=109149 \u21a9</p> </li> </ol>","tags":["live streaming"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/","title":"The Context Unit Blueprint: Architecture, Validation &amp; Graph Retrieval in Pyrana","text":"<p>My previous post introduced the idea of Context Units - this is the blueprint. </p> <p>TL;DR</p> <p>Context Units (CxUs) are immutable, hash-addressed knowledge atoms. This post describes their schema, lifecycle, and the engine that lets you create, validate, version, and retrieve them at scale.</p> <p>Pyrana integration</p> <p>Everything described here is a sneak peak inside of the context engine of Pyrana\u2014our context-engineering platform. The lifecycle stages below map to Pyrana micro-services (<code>authoring-ui</code>, <code>doc-extractor</code>, <code>cxu-validator</code>, <code>content-store</code>, <code>query-api</code>, and <code>prompt-pack</code>). If you\u2019re using Pyrana you already have the full CxU toolchain out of the box.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#1-high-level-architecture","title":"1. High-Level Architecture","text":"<ol> <li>Authoring Tools (Pyrana <code>authoring-ui</code>, <code>doc-extractor</code>) \u2013 UI forms, document extractors, or API imports create draft CxUs.</li> <li>CxU Validator (Pyrana <code>cxu-validator</code> service) \u2013 Enforces schema + higher-order rules (conflict detection, derivability).</li> <li>CxU Store (Pyrana <code>content-store</code>) \u2013 Deduplicates via SHA-256 multihash, persists immutable objects.</li> <li>Query API (Pyrana <code>query-api</code> GraphQL gateway) \u2013 Filters, sorts, and groups CxUs, optionally producing Merkle-rooted sets.</li> <li>Prompt Pack (Pyrana <code>prompt-pack</code> library) \u2013 Serialises selected CxUs (or sub-fields) for downstream LLM calls.</li> </ol>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#2-canonical-schema-v100","title":"2. Canonical Schema (v1.0.0)","text":"<pre><code>{\n  \"cxu_id\": \"&lt;multihash&gt;\",\n  \"cxu_object\": {\n    \"claim\": \"string\",\n    \"supporting_context\": \"string | object\",\n    \"metadata\": {\n      \"knowledge_type\": \"axiom | derived | prescribed\",\n      \"claim_type\": \"definition | hypothesis | requirement | \u2026\",\n      \"keywords\": [\"string\", \u2026]\n    },\n    \"version\": {\n      \"number\": \"integer\",\n      \"created_at\": \"ISO-8601 date\",\n      \"created_by\": \"author id | name\",\n      \"prior_cxu_id\": \"&lt;multihash&gt; | ''\"\n    }\n  },\n  \"references\": [\"url | cid | cxu_id\", \u2026],\n  \"tags\": [\"string\", \u2026]\n}\n</code></pre> <p>Why multihash?</p> <p>A multihash embeds the hashing algorithm in the digest, future-proofing the identifier if we ever move beyond SHA-256.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#field-rules","title":"Field Rules","text":"Field Required Notes <code>claim</code> \u2714 Single declarative sentence. <code>supporting_context</code> \u2714 1-2 sentences; may embed markdown. <code>knowledge_type</code> \u2714 Guides validator rule set. <code>claim_type</code> \u2714 Determines canonical sentence frame. <code>keywords</code> \u274c Helpful for search; must appear in <code>claim</code> or <code>supporting_context</code>. <code>references</code> \u274c External sources or other CxU ids. <code>tags</code> \u274c Derivable grouping labels; no new info.","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#3-lifecycle-version-control","title":"3. Lifecycle &amp; Version Control","text":"<ol> <li>Create \u2013 New CxU \u2192 hash generates <code>cxu_id</code>.</li> <li>Update \u2013 Author clones object, edits, increments <code>version.number</code>, sets <code>prior_cxu_id</code>, saves \u2192 new hash.</li> <li>Propagate \u2013 Engine rewrites incoming references pointing to the old <code>cxu_id</code>.</li> <li>Audit \u2013 Version chain is a linked list; diffs are stored for transparency.</li> </ol> <p>Content-addressability unlocks portability</p> <p>Because every CxU\u2019s ID is its hash, you can store the same object in anything from IPFS to an S3 bucket keyed by <code>cxu_id</code>\u2014or as a node in Neo4j/JanusGraph. The digest guarantees byte-level deduplication and lets heterogeneous systems reference the unit without a central registry.</p> <p>Immutability by design</p> <p>Old versions remain read-only. Deleting a CxU only removes the pointer; the hash history is still provable.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#4-validation-engine","title":"4. Validation Engine","text":"<p>The validator applies two stacks of checks:</p> <ol> <li>Schema Validation \u2013 JSON-schema enforcement.</li> <li>Semantic Rules \u2013 Domain logic, e.g. \u201ctags must be derivable from content\u201d or \u201caxioms cannot reference derived CxUs.\u201d</li> </ol> <p>Rules are declarative JSON expressions and version-controlled alongside the code so that past validations are reproducible.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#beyond-schemas-policy-precedence-rules","title":"Beyond Schemas: Policy-Precedence Rules","text":"<p>Unlike a typical JSON-schema check, a semantic rule can look at multiple CxUs at once and decide whether they coexist peacefully. A common use case is enforcing the precedence hierarchy that exists in many organisations:</p> <ol> <li>Corporate Policy  \u279c highest authority</li> <li>Standard Operating Procedure (SOP)</li> <li>Work Instruction  \u279c lowest authority</li> </ol> <p>If two claims conflict, the validator must keep the higher-precedence unit and quarantine the other for review.</p> <pre><code>// excerpt from pyrana/validator/policy_precedence.rule.json\n{\n  \"rule_id\": \"policy-precedence-v1\",\n  \"description\": \"Reject or quarantine lower-precedence CxUs that contradict higher-level documents\",\n  \"applies_to\": { \"metadata.claim_type\": [\"policy\", \"sop\", \"work_instruction\"] },\n  \"precedence\": [\"policy\", \"sop\", \"work_instruction\"],\n  \"conflict_logic\": {\n    \"type\": \"json-logic\",\n    \"expression\": {\n      \"and\": [\n        {\"==\": [\"${claim}\", \"${other.claim}\"]},\n        {\"&lt;\": [\"${precedence_rank}\", \"${other.precedence_rank}\"]}\n      ]\n    }\n  },\n  \"on_violation\": \"quarantine_lower_precedence\"\n}\n</code></pre> <p>Walk-through</p> <ol> <li>Validator groups incoming CxUs by normalised <code>claim</code> text.</li> <li>For each group it assigns a numeric rank based on <code>claim_type</code>.</li> <li>If two versions clash, the one with the higher rank (smaller number) is accepted; the other is flagged with status <code>quarantined</code> and a helpful message:</li> </ol> <pre><code>\u26a0 SOP-123 contradicts Corporate-Policy-42. Move to draft or update claim wording.\n</code></pre> <p>Because rules are data files, teams can review them in pull requests and roll back to earlier rule sets when auditing past ingestions.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#5-retrieval-filtering-set-creation","title":"5. Retrieval, Filtering &amp; Set Creation","text":"<p>API supports:</p> <ul> <li><code>filter</code>: tags, authors, date range, knowledge_type, claim_type</li> <li><code>sort</code>: created_at, claim alphabetic, relevance</li> <li><code>fields_to_return</code>: e.g., <code>[\"claim\"]</code> for prompt efficiency</li> <li><code>save_as_set</code>: builds deterministic Merkle root over ordered <code>cxu_ids</code></li> </ul> <p>Why Merkle trees?</p> <p>They allow independent parties to verify that a retrieved set hasn\u2019t been tampered with \u2014 essential when sharing context across trust boundaries.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#sets-as-graphs","title":"Sets as Graphs","text":"<p>Saving a list of <code>cxu_ids</code> is only the first step. Inside Pyrana each saved set also lives in a lightweight graph database. Tags and explicit <code>references</code> turn your list into a network where every hop explains why two ideas belong together.</p> <pre><code>graph LR\n    Topic -- tag --&gt; A[CxU A]\n    Topic -- tag --&gt; B[CxU B]\n    A -- reference --&gt; C[CxU C]\n    C -- reference --&gt; D[CxU D]\n    C -- reference --&gt; E[CxU E]\n    B -- reference --&gt; F[More CxUs]</code></pre> <p>In the UI you can click any node to chase its chain of reasoning. The API lets you ask for:</p> <ul> <li><code>depth=n</code> \u2013 return all CxUs within n hops of a starting node</li> <li><code>confidence&gt;0.7</code> \u2013 filter by how often a unit helped produce correct answers</li> </ul>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#growing-stronger-with-use","title":"Growing Stronger with Use","text":"<p>Every time a set guides an LLM or a human to a correct prediction, Pyrana bumps a confidence score on the units and the links they travelled. Think of it like \u201clikes\u201d for knowledge:</p> <ul> <li>Units that prove useful more often float to the top of search results.</li> <li>Weak or unused links fade, highlighting them for review or retirement.</li> </ul> <p>Over time your graph focuses on the beliefs that actually work in practice\u2014turning static documentation into an adaptive prediction engine.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#6-import-export","title":"6. Import / Export","text":"<ul> <li>Import \u2013 <code>.json</code>, <code>.csv</code>, or Markdown files with front-matter.</li> <li>Export \u2013 Same formats + optional Merkle root, proofs, and diff metadata.</li> <li>Idempotent: re-importing an exported set yields zero new hashes.</li> </ul>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#7-example-workflow","title":"7. Example Workflow","text":"<ol> <li>Researcher drops a PDF into the extractor.</li> <li>Extractor produces 42 candidate CxUs \u2192 validator runs.</li> <li>38 pass; 4 need human fixes \u2192 manual form edits them.</li> <li>Approved CxUs are hashed and stored.</li> <li>SME queries <code>tag=scm risk</code> &amp; <code>fields=[\"claim\",\"cxu_id\"]</code> \u2192 gets 12 CxUs, saves as set <code>supply-risk-v1</code>.</li> <li>LLM prompt includes those 12 claims + <code>supply-risk-v1</code> Merkle root.</li> <li>Output references <code>cxu_ids</code>; auditors trace answers back to original PDF.</li> </ol>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#8-open-questions","title":"8. Open Questions","text":"<p>When is a tag not derivable?</p> <p>We\u2019re exploring NLP heuristics that can propose tags automatically and then verify derivability rather than relying on authors to think about it.</p> <p>Hash collisions</p> <p>While practically unlikely with SHA-256, the design still treats a collision as a critical alert and halts ingestion to prevent poisoned graphs.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#9-authoring-templates-best-practices","title":"9. Authoring Templates &amp; Best Practices","text":"<p>The following starter templates cover most early projects. They\u2019re not exhaustive - we are adding new ones as Pyrana expands into other domains.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#91-definition-axiom","title":"9.1 Definition / Axiom","text":"<p>Use this when you are pinning down what something is or stating a timeless first principle.</p> <pre><code>{\n  \"cxu_object\": {\n    \"claim\": \"In Scrum, a Sprint is a time-boxed iteration of one month or less.\",\n    \"supporting_context\": \"From the 2020 Scrum Guide \u2014 applies to all Scrum implementations.\",\n    \"metadata\": { \"knowledge_type\": \"axiom\", \"claim_type\": \"definition\" }\n  }\n}\n</code></pre> <p>Guidelines: single predicate; present-tense; no conditions.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#92-causal-hypothesis","title":"9.2 Causal / Hypothesis","text":"<p>Ideal for statements that could be falsified.</p> <pre><code>{\n  \"cxu_object\": {\n    \"claim\": \"If onboarding is &lt; 2 clicks, then trial-to-paid conversion rises by \u226510 % in B2B SaaS.\",\n    \"supporting_context\": \"Conclusion from 3-month A/B test across 1 200 sign-ups.\",\n    \"metadata\": { \"knowledge_type\": \"derived\", \"claim_type\": \"hypothesis\" }\n  }\n}\n</code></pre> <p>Guidelines: explicit If/Then framing; include scope where possible.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#93-goal-intent-user-story","title":"9.3 Goal / Intent (User Story)","text":"<p>Captures why a user (human or agent) cares about an outcome.</p> <pre><code>{\n  \"cxu_object\": {\n    \"claim\": \"As a site-reliability engineer, I need alert noise below 5 % so that I can triage incidents within 2 minutes.\",\n    \"supporting_context\": \"Aligns with SLA-99.9 % objective approved in Q3 OKRs.\",\n    \"metadata\": { \"knowledge_type\": \"derived\", \"claim_type\": \"requirement\" }\n  }\n}\n</code></pre>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#94-checklist-for-a-well-written-cxu","title":"9.4 Checklist for a Well-Written CxU","text":"Test Ask yourself\u2026 Atomicity Can I split this into two truths? If yes\u2014split it. Clarity Would a peer interpret the same meaning without extra context? Tagging Are all tags derivable from <code>claim</code> or <code>supporting_context</code>? Template fit Does the wording match a known template or do we need a new template? Verifiability Can evidence be cited (now or later) to prove or refute it? Scope stated Is the domain or audience obvious (e.g. \u201cIn EU-region\u2026\u201d)? No hidden references The claim stands alone even if external links break. <p>Run this list before committing; Pyrana\u2019s authoring UI surfaces it as an inline linter.</p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/the-context-unit-blueprint-architecture-validation--graph-retrieval-in-pyrana/#conclusion","title":"Conclusion","text":"<p>CxUs give us a trustable foundation for high-density, auditable context. With the schema, validator, and version control in place, the next milestones are smarter extraction and richer set operations. Contributions welcome \u2014 just remember, one truth per hash.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2025-07-23</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>80f775921310b2d6...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2025-07-23 James Canterbury Added CxU posts and custom Admonitions <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["cxu","specification","architecture"]},{"location":"2025/07/22/context-efficiency/","title":"Context Efficiency","text":"<p>It's funny I've spent years talking about tokens in all of my blockchain work. Whether it be NFTs or stablecoins, it seemed like tokens were everywhere.  Now that we're diving in deep and trying to fix some problems with context in AI, it seems the topic of tokens (albeit a very different kind) is still center stage.  The phrase \"tokenization of the world\" has taken on a new meaning to me.   </p> <p>crypto tokens \u2260 AI context tokens</p> <p>Just to be clear - these are not the same thing, but they do share some commonalities.  For example, both tokens were created as a way of driving efficiency in interactions with the underlying systems.  </p> <p>In the case of blockchain, it's more straightforward - tokens represent something of value or utility, and and the amount of them available is meant to drive an economy that will incentivize certain behaviors (Unfortunately, those behaviors are not always the greatest... But that's a different blog post.) </p> <p>In modern large language models (and AI in general) tokens are the way that we communicate with the model. Tokenization is simply a text-segmentation strategy that turns bytes/chars into sub-word units so models can operate efficiently. It's easy to think of each word we include in a prompt as being a token, though it's worth understanding that that is a very rough analogy.  Words are not always efficient in conveying context. So oftentimes many filler words can be combined into a single token, whereas very unique words may require several tokens. The order in which those words are fed to a model help the model predict the order of tokens that it should respond with (mapped back to words of course).  The number of tokens fed to a model in a given prompt is referred to as the amount of context in that prompt.  And the amount of context that the model is capable of handeling is referred to as the \"context window\".</p> <p>To give you a sense of token size this side note is 267 words and can be represented in 319 tokens.</p>","tags":["cxu","context windows","knowledge density"]},{"location":"2025/07/22/context-efficiency/#size-matters","title":"Size Matters","text":"<p>The size of context window is important.  Transformer-based LLMs only remember what you can cram into a fixed-size window. The bigger the window, the more it costs to run \u2014 and the more chances you have to break something during prompt gymnastics. Large language models with large context windows require lots of compute.  We live in a world where:</p> <ol> <li>Teams want richer, auditable knowledge injected into every AI interaction, and</li> <li>Compute is limited - meaning costs can add up fast.</li> </ol> <p>That tension demands a smarter way to package knowledge.</p>","tags":["cxu","context windows","knowledge density"]},{"location":"2025/07/22/context-efficiency/#enter-context-units-cxus","title":"Enter Context Units (CxUs)","text":"<p>A Context Unit is a bite-sized, immutable packet that captures exactly one statement you believe to be true, plus the evidence and metadata that back it up. Think of it as the atomic \u201cfact card\u201d in your personal knowledge deck. Each CxU is hashed, version-controlled, and reference-able, so you can trace any answer back to its source.  A complete collection of our CxUs could be said to represent our knowledge or our worldview.  I don't want to make it sound as if this were an easy thing to do.  Individual worldviews are messy and not easy to share - see my previous post on distributed knowledge systems - but CxUs are the place to start.</p> <p>The context unit engine is a key part of what we built with pyrana.ai. </p>","tags":["cxu","context windows","knowledge density"]},{"location":"2025/07/22/context-efficiency/#key-properties","title":"Key Properties","text":"Property Why It Matters Immutable If the content changes, so does the hash. No silent edits. Self-contained Claim, support, and metadata travel together \u2014 no loose ends. Composable Multiple CxUs chain into a worldview you can inspect or share. Cryptography helps us create and re-use CxU sets <p>One truth per hash</p> <p>A CxU should read like a sentence on a sticky note. If you can split it into two independent truths, you probably should.</p>","tags":["cxu","context windows","knowledge density"]},{"location":"2025/07/22/context-efficiency/#context-knowledge-density","title":"Context &amp; Knowledge Density","text":"<p>\u201cMore context\u201d isn\u2019t always better. What we actually need is higher knowledge density \u2014 the maximum useful information in the minimum number of tokens. CxUs raise that density by stripping out narrative fluff and keeping only the parts of knowledge that an LLM can reason with.</p> <p>Picture a 3,000-word white paper boiled down to ten or so CxUs. You\u2019ve compressed the signal without losing proof or provenance. The result? Your prompt stays small, the model stays focused, and you still get traceability back to the source.  Reverse prompting the white paper based on the CxUs can help us measure the completeness of the extracted CxUs.</p>","tags":["cxu","context windows","knowledge density"]},{"location":"2025/07/22/context-efficiency/#efficiency-in-the-prompt-pipeline","title":"Efficiency in the Prompt Pipeline","text":"<ol> <li>Authoring \u2013 Write or extract CxUs from docs, meetings, or code.</li> <li>Selection \u2013 Query the CxU store for the subset relevant to your task.</li> <li>Packing \u2013 Inject those CxUs (or even just their claims and IDs) into the prompt.</li> <li>Auditing \u2013 Associate a set of cxu_ids to your prompt so you can prove where the answer came from.</li> </ol> <p>Each step is deterministic, inspectable, and cheap compared to shoving the whole document into the model every time.</p>","tags":["cxu","context windows","knowledge density"]},{"location":"2025/07/22/context-efficiency/#cxus-vs-common-knowledge-artifacts","title":"CxUs vs. Common Knowledge Artifacts","text":"Artifact Typical Form &amp; Pain Points What It Looks Like as CxUs Traditional notes Free-form paragraphs, personal shorthand, hard to merge Atomic claims + explicit metadata; query-able and deduplicated Academic papers 10-30 pages of dense prose, citations buried in footnotes Each key claim distilled into a CxU linked to its source; higher knowledge density Company policies Legalistic language, versioned by filename, diff-unfriendly Each clause hashed; version chain + audit-friendly diffs Slide decks Bullet points &amp; images; meaning lost without presenter Each slide broken into one or several claims with references to each other Meeting minutes Chronological logs; action items buried Action items extracted as requirement-type CxUs; searchable by tag Email threads Nested replies, tangents, mixed topics Relevant statements distilled into CxUs with references to message IDs","tags":["cxu","context windows","knowledge density"]},{"location":"2025/07/22/context-efficiency/#practical-example","title":"Practical Example","text":"<p>Imagine you\u2019re building an AI assistant for supply-chain compliance. Instead of pasting entire regulations into every prompt, you create a library of CxUs such as:</p> <p>\u201cIn the EU, a shipped product must include a Declaration of Conformity before customs clearance.\u201d</p> <ul> <li>Claim: In the EU, a shipped product must include a Declaration of Conformity before customs clearance.</li> <li>Supporting context: This requirement is defined in EU Regulation 765/2008, Article 30, to ensure product safety and compliance.</li> <li>References: EUR-Lex 765/2008 Article 30</li> </ul> <p>Tag it <code>supply-chain</code>, <code>eu</code>, <code>regulation</code>. When a user asks about exporting to Germany, the retrieval engine surfaces that specific CxU (and maybe five others) \u2014 not the entire regulation text.</p>","tags":["cxu","context windows","knowledge density"]},{"location":"2025/07/22/context-efficiency/#takeaways","title":"Takeaways","text":"<p>Key Points</p> <ol> <li>CxUs maximize knowledge density per token.</li> <li>They make LLM reasoning traceable and auditable.</li> <li>They turn messy documents into modular, query-able facts.</li> </ol> <p>We\u2019re just scratching the surface here. In the next post we\u2019ll roll up our sleeves and explore the nuts and bolts: schemas, version control, tagging strategies, and the engine that makes CxUs tick.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2025-07-23</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>d16319f49c79bf88...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2025-07-23 James Canterbury Added CxU posts and custom Admonitions <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["cxu","context windows","knowledge density"]},{"location":"2025/07/10/data-without-context-is-useless/","title":"Data Without Context is Useless","text":"<p>Enterprises plough well over $200 billion every year into data-warehouses, BI dashboards, and analytics suites,- and analysts now forecast the big-data analytics market to top $1.1 trillion by 2033\u00b9. Financial-services firms alone spend $44 billion a year just on market-data feeds\u00b2. Yet studies keep showing that roughly half of the information companies collect never gets used\u00b3.</p> <p>The result? A widening chasm between what organizations\u00a0capture\u00a0and what they can\u00a0confidently act on. The core culprit is simple:\u00a0data, when stripped of context, is inert. Without the surrounding business intent and situational nuance, even the most beautiful dashboard struggles to answer the one question every executive eventually asks:\u00a0\u201cSo what should we do?\u201d</p>","tags":["data","context","AI agents"]},{"location":"2025/07/10/data-without-context-is-useless/#how-humans-actually-think-a-context-first-operating-model","title":"How humans actually think: a context-first operating model","text":"<p>Cognitive science tells us that human reasoning is not a big relational table queried on demand; it is an always-evolving tapestry of episodic memories, semantic knowledge, and live situational cues. When you glance at a report that says\u00a0Inventory = 12 days, your brain instantly cross-references:</p> <ul> <li>Past stock-out scares  </li> <li>This morning\u2019s news about a port strike  </li> <li>Your company\u2019s cash-flow posture  </li> <li>An email from sales forecasting a promotion next month</li> </ul> <p>Only\u00a0after\u00a0that web of context comes together do you decide whether twelve days is safe or catastrophic.\u00a0\u00a0Humans reason context-first, data-second: facts gain meaning only through their relationships to everything else we know\u2074.</p>","tags":["data","context","AI agents"]},{"location":"2025/07/10/data-without-context-is-useless/#traditional-systems-data-first-context-starved","title":"Traditional systems: data-first, context-starved","text":"<p>Classic analytics stacks were never designed to capture this richness. They assume:</p> <ol> <li>Stable schemas\u2014but real life is messy.  </li> <li>Deterministic logic\u2014edge-cases explode exponentially.  </li> <li>The user supplies the context\u2014dashboards show the\u00a0what, analysts explain the\u00a0why.</li> </ol> <p>These constraints surface as familiar pain points:</p> <ul> <li>Silo proliferation.\u00a0\u201cCustomer\u201d means one thing to Marketing and another to Finance.  </li> <li>Query-driven discovery.\u00a0You must\u00a0know\u00a0the question to ask it\u2014hard when you lack context, and the people that are best with the data are often different than the people with the business knowledge.   </li> <li>Last-mile decision bottlenecks.\u00a0Analysts become human middleware translating raw numbers into narratives leaders trust (or are inclined to want to trust\u2026).</li> </ul> <p>At scale that friction makes deriving actionable value out of analytics data feel like tilting at windmills. Companies sit atop petabytes of elegant yet\u00a0context-starved piles of unactionable data.</p>","tags":["data","context","AI agents"]},{"location":"2025/07/10/data-without-context-is-useless/#a-better-path-agentic-ai-that-reasons-with-context","title":"A better path: agentic AI that reasons with context","text":"<p>Large-language models and generative AI hint at an escape hatch\u2014but monolithic models hit a\u00a0reasoning cliff\u00a0on long or specialized tasks. Apple\u2019s \u201cIllusion of Thinking\u201d research showed that even when given the exact algorithm, LLMs falter as complexity rises\u2075. The emerging consensus is\u00a0orchestration: swarms of specialized agents, each great at one thing, coordinated by a higher-level planner.</p> <p>For those agents to behave like seasoned employees\u2014not interns\u2014they need explicit, machine-readable\u00a0context:</p> <ul> <li>Company strategy and guardrails  </li> <li>Domain-specific heuristics (\u201c70 % gross-margin is our pricing floor\u201d)  </li> <li>Regulatory limits and risk appetite  </li> <li>Real-time signals (supply shocks, promotions, weather)</li> </ul> <p>Armed with that scaffolding, agentic systems can triangulate knowledge threads the way humans do\u2014only at cloud scale and millisecond speed.</p>","tags":["data","context","AI agents"]},{"location":"2025/07/10/data-without-context-is-useless/#operationalizing-context-context-units-cxus","title":"Operationalizing context: Context Units (CxUs)","text":"<p>Our answer at\u00a0Pyrana\u2122\u00a0is our proprietary\u00a0Cognitive Action Framework\u2122, built on two atomic building blocks\u2014Context Units (CxUs)\u00a0and\u00a0Action Units (AXUs).</p> <p>Context Units are verifiable, structured packets of knowledge or situational context, each carrying its own provenance metadata. They ground agent reasoning in transparent, auditable information and can be composed, versioned, and independently verified the way Lego bricks snap into larger structures.</p> <p>A CXU might encode:</p> <ul> <li>\u201cCalifornia\u2019s 2025 privacy law restricts cross-border transfer of user-level health data\u201d (source, effective date, confidence)  </li> <li>\u201cSKU A\u2019s reorder point is 15 days of cover\u201d (owner, last review, linked BI query)  </li> <li>\u201cCorporate brand guidelines forbid green checkout buttons\u201d (design-system version)</li> </ul> <p>Because every prompt against your data carries with it a set of CxUs, which are tightly version controlled, agents can\u00a0cite their chain of thought\u2014a critical audit trail for regulators, customers, and internal risk teams.</p>","tags":["data","context","AI agents"]},{"location":"2025/07/10/data-without-context-is-useless/#the-pyrana-stack-context-powered-orchestration","title":"The Pyrana stack: context-powered orchestration","text":"<p>Pyrana assembles CXUs into a knowledge graph, then pairs them with\u00a0Action Units\u00a0(skills such as \u201chit the ERP API,\u201d \u201crun a what-if simulation,\u201d or \u201cdraft a supplier email\u201d). A policy-driven orchestrator decides which agents and actions to invoke\u2014always\u00a0consulting the relevant CXUs first.</p> <p>Key capabilities:</p> <ul> <li>Context-first reasoning.\u00a0Agents read guardrails before acting.  </li> <li>Modular skills.\u00a0Swap in a new vision model or forecasting algorithm without rewriting the workflow.  </li> <li>End-to-end observability.\u00a0Every prompt, decision, and data fetch is logged, searchable, and attributable\u2014all the way back to the originating CXU.</li> </ul> <p>In short, the platform lets enterprises layer human-grade context atop their existing data\u2014no rip-and-replace\u2014so that data finally becomes actionable.</p>","tags":["data","context","AI agents"]},{"location":"2025/07/10/data-without-context-is-useless/#conclusion-invest-where-insight-lives","title":"Conclusion: invest where insight lives","text":"<p>Enterprises were right to build analytics foundations; raw data is indispensable. But\u00a0raw\u00a0is all it is. Value emerges only when data is entwined with the policies, memories, and nuances that seasoned employees reach for instinctively. AI without context is like giving your kid a bucket of parts and expecting them to build a rocket.</p> <p>Agentic AI infused with\u00a0Context Units\u00a0delivers the missing piece of the puzzle:</p> <ol> <li>Human-grade reasoning\u00a0that scales beyond human cognitive and capacity limits  </li> <li>Transparent, auditable decisions\u00a0that beat black-box predictions  </li> <li>Future-proof flexibility\u2014new data streams or policies become new CXUs, not months-long engineering projects</li> </ol> <p>If your dashboards still tell you\u00a0what\u00a0happened but never\u00a0why\u00a0or\u00a0what next, it\u2019s time to admit the obvious:\u00a0data without context is useless. With Pyrana\u2122, context is engineered into every solution so data finally becomes actionable.</p>","tags":["data","context","AI agents"]},{"location":"2025/07/10/data-without-context-is-useless/#footnotes","title":"Footnotes","text":"<ol> <li>Astute Analytica, \u201cBig Data Analytics Market to Reach Valuation of US$ 1,112.57 Billion by 2033,\u201d\u00a0GlobeNewswire, 13 May 2025.\u00a0globenewswire.com </li> <li>\u201cMarket Data Spend Hits Another Record as Complexity Grows,\u201d TRG Screen, 2025.\u00a0trgscreen.com </li> <li>Zylo,\u00a02024 SaaS Management Index: average organisations waste $18 million annually in unused licences.\u00a0zylo.com </li> <li>IBM, \u201cWhat Is Dark Data?\u201d (quoting Gartner\u2019s definition that most data goes unused), accessed June 2025.\u00a0ibm.com </li> <li>Apple, \u201cThe Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity\u201d, June 2025. apple.com</li> </ol>        \ud83d\udcdd Content Provenance      <p>Created: 2025-07-23</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>db4b7f1c028067d5...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2025-07-23 James Canterbury Added CxU posts and custom Admonitions <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["data","context","AI agents"]},{"location":"2024/09/21/nexus-a-counter-argument-to-blockchains-achilles-heel/","title":"Nexus: A counter argument to blockchain's \"Achilles heel\"","text":"<p>Often we have books by great thinkers that shape the way we view the world and influence our outlook on the future.  For me Yuval Harari is one such thinker.  In this post I share my reaction/counterpoint to a statement that Prof. Harari makes in his new book \"Nexus: A Brief History of Information Networks from the Stone Age to AI\".</p> <p>Let me start this post by saying that I am a huge fan of Prof. Harari.  Sapiens is one of the most influential books I have ever read - it opened my mind to the concept of intersubjective reality and the power of the stories we tell  - both how those stories can rule our lives and make our species capable of great things.</p> <p>I just finished reading Nexus and it too is a great work, I tell my friends that part 1 is a history lesson and perhaps a bit of a slog, but just as Prof. Harari does in Sapiens and Homo Deus his thorough historical perspective sets the foundation for his later arguments.  Yes the book is a bit alarmist, but I don't think it is due to an algorithm instructing him to use outrage to sell copy... What he shares (in my opinion) is an alarm that needs to be heard.</p> <p>I am not a book critic, but I think this work can be as influential as Sapiens and therefor I need to correct something that is near and dear to my heart.  In chapter 10 Prof. Harari discusses the view of using blockchain to as a \"technological check on such totalitarian tendencies\" and goes on to describe why blockchain is not a good fit for this:  \u201cIn a blockchain system, decisions require the approval of 51 percent of users. That may sound democratic, but blockchain technology has a fatal flaw. The problem lies with the word \u201cusers.\u201d If one person has ten accounts, she counts as ten users.\u201d  The sources he references for this statement are legitimate, but are actually referring to voting systems built on top of blockchains - or as in the case for \u201cEstonia\u2014the Digital Republic Secured by Blockchain\u201d, the i-voting system is more accurately described as a secure digital voting infrastructure that incorporates some blockchain-inspired elements rather than being built on a specific blockchain platform.</p> <p>He then shares a short story of Roman emperor Caracalla who murders his brother Geta and then attempts to erase him from history through some pretty extreme measures.  And also of Stalin who attempted to purge unfavorable facts from all historical records.  Using these stories he makes the statement \u201cThis degree of erasure demanded a huge manual effort. With blockchain, changing the past would be far easier. A government that controls 51 percent of users can disappear people from history at the press of a button.\u201d</p> <p>This is simply not true.</p> <p>As a caveat to my following statements Prof. Harari does not state which blockchain(s) he is referring to in his examples.  His sources site several blockchain-like systems and private networks that could allow his statements to be correct.  But his conclusions about the \"achilles heel\" and the ease of erasure do not apply to nearly any modern decentralized public blockchain networks.  What Prof. Harari is referring to is what we would call a 51% attack and if such an attack were successful, then executing a finality-reversion - these are in effect two different issues.</p> <p>In one of my other favorite books, \"Read Write Own\" by Chris Dixon, describes a 51% attack as something that:</p> <p>\"...can disrupt transaction processing or enable attackers to \u201cdouble spend\u201d the same money in multiple places. These attacks are known as 51 percent attacks because their conspirators must gain control of more than half a system\u2019s validators to be successful.  Feeble systems, like Ethereum Classic and Bitcoin SV, have succumbed to 51 percent attacks. Successfully attacking a major blockchain, like Bitcoin or Ethereum, would, in comparison, be so prohibitively costly as to be infeasible.\u201d</p> <p>To put that in context, to control the Ethereum network would require one to gain 51% of the stake which can be measured in Total Locked Value.  As of today (9/21/24) that would require you to hold over 20M Eth which would cost you upwards of $52B USD.  But this assumes the entity already controls that amount of stake.  If they had to acquire it, this would drive the cost significantly higher and it would be obvious what was going on.  In a public network things are, well, public... Part of the genius of blockchain is how the incentives are stacked such that it will cost you far more to attack then network then you could gain from corrupting it and that the community always has the ability to fork it away from you (of course this is much easier said than done).</p> <p>Which leads me to Prof. Harari's second statement about erasure.  Erasing anything is never easy - even the historical examples he uses to make his point have survived erasure.  If one entity were to gain control of a networks they would only be able to manipulate the current state, and any future states while they have control.  A blockchain, by definition, does not allow you to rewrite previous states.  We call this \"finality-reversion\" and the deeper something is committed into the chain the more difficult this becomes.  To \"erase someone\" you would not only need to gain control, but you would need to convince the entire network (100%) to resync their validators back to the point that you wanted to change.  Nothing is impossible, but this is far more difficult than \"pushing a button\".</p> <p>For what it's worth, the more likely risk of someone gaining 51% control is their ability to exclude, or censor, transactions on the network.  For this to be effective they need to maintain the \"public trust\" in the network while they quietly censor things out. This takes much coordination and stealth on behalf of the corrupt entity.  Back in July at Eth CC Vitalik Buterin discussed this exact topic. he shared a plan to for Ethereum validators to run software that could automatically prevent such things from happening.</p> <p>I wanted to point this out not to poke holes in Prof. Harari's excellent work, but to strengthen the argument that blockchain is one of out best AI resistant technology.  It is not a silver bullet of course, and we need to design everything with caution, but if done correctly it can be the solid footing on which the truth of us humans can be distinguished from the truth of the AI.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-09-23</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 7</p> <p>File SHA-256: <code>8c6dc1220410debd...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-09-24 James Canterbury changed to prof. harari 2024-09-23 James Canterbury Update Nexus_blockchain_counter.md <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["digital truth"]},{"location":"2023/10/10/attested-sensors--source-data-synthesis-under-zero-knowledge/","title":"Attested Sensors &amp; Source Data Synthesis Under Zero Knowledge","text":"<p>TLDR; attested sensors can be used to digitally sign source data at the moment it is collected by a IOT device.\u00a0 Blockchain can be used to notarize that signature and apply a tamper proof time stamp.\u00a0 To protect sensitive data from leaving the source, a zero knowledge_ \u00a0circuit can be used to produce a synthesis of the raw data and generate a proof that the synthesis was done correctly.</p> <p>Securing data at the point of origin and delivering data providence throughout its lifecycle is a key component of data integrity.\u00a0 Assuming that data will travel across multiple systems/networks it is important to develop methods that can travel with the data and independently attest to its source attribution and immutability.\u00a0 Note that attesting to the validity of the data in the first place is a separate issue of configuration and physical control.</p> <p>Sensors are hardware that capture evidence or take measurements in the real world and produce a digital output.\u00a0 An example is a digital thermometer that will measure the ambient temperature of its surroundings and provide output data on a periodic basis.\u00a0 Another example is a digital photo that captures an image through a lens and produces an array of colored pixels that can be used to render the image in digital media.\u00a0 Still another example a biometric sensor that can read a human iris or fingerprint and return a digital identifier for that individual.</p> <p>Attested sensors are sensors that have additional hardware which contains a public/private key pair, the public key being a universal identifier for the sensor.\u00a0 When the sensor takes a reading (or a photo for example) the private key is used to sign the digital output of the sensor.\u00a0 This typically includes a hash of the data itself plus an encryption that can only be produced with the private key but can be easily verified by anyone that knows the public key.\u00a0 The hardware must be carefully designed so that there is no option to alter the source data prior to the signature being generated.\u00a0 Coupling the sensor with a network protocol, such as a blockchain, can serve to provide a time stamp for the signature and to store the data hash in an immutable database; this coupling requires internet connectivity for the sensor.</p> <p>Subsequent changes to the data must be handled through careful orchestration of software and permissions to develop a \u2018manifest\u2019 that tracks all alterations in the form of a change log.\u00a0 The work currently being done by CP2A.org is an example of what must be done to ensure edited copies of digital photos are legitimate and attributable.\u00a0 While not as prominent outside of photography, video, and audio, a similar \u2018manifest\u2019 approach could be applied to any sensor reading.</p> <p>Through the function of registering the public key of the sensor to a network (likely the same network used for time stamping and notarization of the data) it is later possible to attribute data back to its source, if the data is to be used as part of a training population for machine learning this is critical component to ensuring data authenticity (and possibly deriving percent attribution which may be used for calculating royalties).</p> <p>Figure 1: Attested sensors with timestamping and signatures </p>","tags":["attested sensors","hardware"]},{"location":"2023/10/10/attested-sensors--source-data-synthesis-under-zero-knowledge/#data-capture-with-synthetic-data","title":"Data Capture with Synthetic Data","text":"<p>Some sensors will inherently capture data that is considered sensitive or \u2018Personally Identifiable Information\u2019 (PII) such as biometric scanners \u2013 while the scanner itself may not capture PII, the function of coupling that data with an established identity would.\u00a0 In general, any sensor data may be coupled with non-sensor information such as identity, location, etc. It is that correlation between several data elements that makes the dataset useful; but this dataset also creates privacy concerns.</p> <p>An attested sensor with data synthesis capabilities generates both a raw data set and a synthesized data set for each reading.\u00a0 The sensors synthesis circuit anonymizes the PII from the raw data using a zero knowledge cryptographic circuit which accepts the raw data as input and generates a synthesized data set in addition to a cryptographic proof which is signed by the attested sensor.</p> <p>In this method we are pushing the synthesis of data out to the edges of the network by generating it at the source.\u00a0 This then allows the synthetic data to be aggregated for model training without the risk of sensitive data leakage.\u00a0 The source data can then be encrypted and stored locally in a secure manner, or possibly destroyed.</p> <p>Figure 2: Attested sensors with data synthesis </p> <p>Example 1: Converting raw data sets to synthetic data set  </p> <p>There may be an alternate way of writing the synth circuit then flashing it onto some non-volatile memory in the sensor, it could be part of the attestation process\u2026</p> <p>But then again ZKPs are cool.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-06-15</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 5</p> <p>File SHA-256: <code>bceae388a0566875...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-06-15 James Canterbury added a bunch of old blogs... <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["attested sensors","hardware"]},{"location":"2024/07/04/the-blinky-light-thing/","title":"The Blinky Light Thing","text":"<p>The objective of the Blinky Light Thing (that's the technical term) is to embed proof that audio and video content is both authentic and contemporaneous with the time at which it is being recorded by introducing elements into the physical environment that are very difficult replicate and are captured by the original recording devices (i.e., camera + microphone).  The intention of BLT is to supply a low-tech solution that is not dependent on specific hardware/software and is easily (and freely) verifiable.</p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#whats-the-problem-with-live-streaming-video","title":"What's the problem with live-streaming video?","text":"<p>It's hard to know what's real these days.  We've all seen deep fake video and CGI actors in the movies.  The ability for humans to detect authentic audio and/or video content from generative content just by \"eyeballing\" it is rapidly diminishing.  But this is no surprise, and it's not an new issue.  Ever since digital distribution of media became popular in the 80's - 90's we have been introducing more and more sophisticated ways of trying to prove something is authentic - pretty much all of these approaches boil down to some form of watermarking.</p> <p>Watermarking is a way to imbed something unique into the video and audio components of digital media.  Sometimes they are noticeable by the audience (like a floating tag in a video) and sometimes they are not (like extra embedded data in an audio file).  But they are almost always something added to the file after it has been created by the presenter and is then verified before sharing with the audience.  Traditionally watermarking has been used to prove that content is authentic and officially licensed from the owning entity that produced it (i.e., not a pirated movie or album).  But when a presenter is addressing an audience directly to convey a specific message - such as Elon Musk discussing Tesla earnings with shareholders, or a witness giving a deposition in a court trial, or the US President addressing the nation - we want a different kind of authentic...</p> <p>We want to know that what we are seeing and hearing is real - or more precisely that who we are seeing is in fact who we believe them to be, and what they are saying is in fact the words coming out of their mouth.  In many cases we also want to know that when they are saying it is contemporaneous when when the content was created.  That is to say if it is \"live-streaming\" we want to know that it is happening now.  If the content claimed to be pre-recorded we want to know that the recording date is consistent with what they state it is (consider the impact of back-dating a recorded prediction or a set of instructions such as a last will and testament).</p> <p>To visualize this it's best to start with the mental model of how we handled all of this before digital content.  Ahh the good old days...</p> <p></p> <p>When we are physically present the proof points above are ubiquitous.  The audience sees the presenter, so they know it's them (body doubles and undercover agent theories aside).  They can hear what is being said as the presenter is saying it.  Generally there is no doubt that the message is both authentic and contemporaneous.</p> <p>In live-streaming we introduce quite a few more variables.  The whole point in live streaming is that the audience and the presenter do not need to be physically co-located.  There is actually a lot more that goes into this, but the most basic form would be something like the graphic below.</p> <p></p> <p>There are of course many ways to secure the capture and distribution of live-streaming media. But they all require control points around hardware and/or network; and most of them require some trusted intermediary to \"prove authenticity on your behalf\" (just trust the broadcast network and watch their ads in return for them doing this service for you).  While this may be ok for large scale productions such as the US President addressing the nation, or live streaming sports, it almost never happens with small scale productions like an individual trying to make a personal statement \"on the record\".  Or, as has become more common, an individual trying to prove that their statement is not a deep-fake.  This post will not go into all the technological advances being made for producing live-streaming deep-fake video, but it's scary.  And just because the presenter says it's not a fake and the word \"LIVE\" is watermarked into the video doesn't cut it anymore.</p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#what-to-do-about-it","title":"What to do about it?","text":"<p>BLT works by introducing detectable messages into the the physical environment where the content is being captured, thereby embedding the message into the raw audio and video of the content stream without relying on any of the capture and distribution infrastructure.  While the messages are detectable, we are going to great lengths to make them not distracting or necessarily apparent.   The light source \"blinks\" the encoded message and the audio source \"chirps\" the same message.  Included in the message is enough relevant information for the audience to determine how to decode it (i.e., convert the blinks and chirps back into the original message), but most likely this will be more easily done by the presenter sharing a QR code with the audience which will give them specific instructions.</p> <p></p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#design-principles","title":"Design Principles","text":"<p>There are a couple of things necessary for BLT to scale.  </p> <p>1) Mainly that the core concept is available for anyone to use and that messages are transmitted using as broad a standard as possible.  This means that the content of the message is available information to the audience, if the audience is unknown, then that information is publicly available.</p> <p>2) The method for interpreting the received message must also be free for the audience to use and ideally there would be several software clients that could independently verify the same message.</p> <p>3) The \"authenticity\" of the presenter (i.e., they are who they say they are) requires a form of digital identity.  There are both strong and weak forms of identity that can be leveraged.  </p> <p>4) The presenter must make an \"attestation\" when beginning their broadcast session to generate the message and irrevocably tie it to their identity.</p> <p>5) To prove contemporaneousness the attestation must include a time stamp from an unbiased, highly available, and secure network (i.e., the Ethereum blockchain) and that same network can be used include information that would be impossible for the presenter to know prior to their broadcast attestation.</p> <p>5a) Optionally the presenter can include a termination attestation and the end of their broadcast session which can later be used to verify the completeness of the broadcast.</p> <p>6) The broadcast message will be blinked and chirped through out the broadcast session, for longer broadcasts it may be desirable to update the broadcast message periodically.</p> <p>7) Because of network latency and unexpected physical environment changes (i.e., a drastic change in lighting or background noise) the broadcast message must be kept as brief as possible and include sufficient redundancy so that a completed message can be received by the audience. HOwever we must also consider the limitations of video transfer and design a message that can be transmitted using less than 30 frames per second.</p> <p>8) The format for encoding the broadcast message into blinks and chirps must be widely known and easily verified.  </p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#whats-in-the-message","title":"What's in the message","text":"<p>The encoded broadcast message contains the minimal amount of information required by the audience to verify the message.  By leveraging a blockchain service for attestations (specifically the Ethereum Attestation Services) the broadcast message can simply include a link to the broadcast initiation attestation on EAS.  The EAS attestation is an immutable record that is signed by the presenters private key (meaning that only the presenter can sign it).  By accessing the UID a viewer can verify who signed it and when.  The attestation also can carry some additional information such as the blink rate which is important to know for when trying to decode the message. The data included in the attestation is arbitrary and can follow any structured schema chosen by the presenter.  So long as the private key used to sign the attestation is provably tied to the presenter (this can even be done in a custodial manner) all that is needed to meet our proof points is the attestation UID as it is impossible to know before hand and references a timestamp that cannot be modified.</p> <p>[example Attestation UID: 0x144fec63505680f8f0a6dee7e3b16f0f147715586aa0fe6e7ee65b940d23c570]</p> <p>The Attestation UID is a 64 character hexidecimal string which represents 256 bits of information (each character is 4 bits). For video the limiter is the frame rate of the of the video feed which is generally a minimum of 30 frames per second (FPS).  For audio files a minimum sample rate (SR) should be twice the highest frequency in the audio signal (Nyquist theorem), so about 40 kHz.  Knowing these values we can calculate the length of time it takes to transmit the message.</p> <p>Minimum Video Message Duration (in seconds):</p> <p>Duration = \\((\\frac{1}{FPS})\\) bits = \\((\\frac{1}{30})256\\) = 8.533 seconds</p> <p>Minimum Audio Message Duration (in seconds):</p> <p>Duration = \\((\\frac{1}{SR})\\) bits = \\((\\frac{1}{40})256\\) = 6.4 seconds</p> <p>However, we are transmitting sequential bits as either a 1 or a 0 (light on, light off...) and sequential bits of the same value (111000) do not show a detectable difference, so for this reason we need to include a blink rate as part of our message encoding.  Theoretically the blink rate just needs to be greater than the minimum message durations calculated above.  practically it needs to be about 5x longer to give ample time in detecting a state change.  We've been running tests at a blink rate of 0.2 seconds which gives us a message transmission rate of 51.2 seconds.</p> <p>Note</p> <p>In designing BLT we tried to account poor audio and video quality since the encoded message is passed via the media stream. At the beginning of a broadcast the presenter must share the configuration of their BLT - the decoder needs to know what color light it is looking for our which amplification frequency to tune into. We also intend for the presenter to share the UID - thus giving the viewer the encoded message it is seeking to verify.  This changes the approach from trying to decode an unknown message to trying to verify a known one and allows us to apply a probability to account for poor transmissions.  To give further redundance the blink rate and the chirp rate (which are the same) are purposely out of sequence so that if one bit is missed it can be inferred from the other stream.  Finally the BLT uses terminators at the beginning/end of each transmission consisting of three blinks from a different light source or three chirps at a different frequency.</p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#whats-special-about-this","title":"What's special about this?","text":"<p>Our intention is to not just overlay the Attestation UID on top of the raw audio and video files, the BLT imbeds it into the actual media by reflecting the blinking light source onto the face of the presenter, and amplifying their actual voice.  In this way any alterations to the video content after the broadcast, or post capture of the live stream (such as a filter) would obfuscate the encoded message making it indecipherable and hence invalid.  While BLT does require a blinking light and a chirper, we intentionally designed it to work with any camera/microphone to avoid the need for customized hardware.</p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#how-do-we-make-this-not-annoying","title":"How do we make this not annoying?","text":"<p>In some cases the presenter may want both the blinking light and the audible chirp to be heard by the audience in a way that is not too distracting, but noticeable. Most likely they will want both forms of message transmission to be innocuous.  For the blinking light we accomplish this flooding the presenter with a strong white light (similar to a ring lamp or side lit LEDs that are common for web productions) but including the blinking LED in a specific color that becomes very obvious when a common video color correction filter is applied.  Specifically by creating an arbitrarily high gamma and hue for the video.  A similar audio filter is applied to detect the amplified sound waves from the audio transmission. In the end there may be a tiny bit of perceptible flickering or distortion to the audio or video file, but nothing noticeable.</p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#live-stream-verification","title":"Live-stream Verification","text":"<p>In theory the attestation UID will have all the information needed for the viewer to verify the authenticity of the live stream, but this would require them to watch and listen to the video with a set of filters applied which amplify the encoded message.  Our intention is to have several open source verification tools available for anyone to be able to verify the media for free.  Zeroth Technology will offer one such tool as a browser based service for videos watched through our website.  Or as an extension for any videos watched through y our favorite browser.</p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#post-stream-verification","title":"Post-stream Verification","text":"<p>We also designed BLT to let presenters prove the content was authentic even after they've completed their broadcast.  This is done by generating a closing attestation in EAS referencing the opening attestation. We are also working on a way to quickly generate a content hash of the raw audio/video file to include in the closing attestation.  From there we plan to leverage the C2PA manifest approach to prevent any further alterations.</p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#what-if-there-are-multiple-people-broadcasting","title":"What if there are multiple people broadcasting","text":"<p>Our approach works well in scenarios where there are multiple presenters and audiences such as a virtual meeting.  In these scenarios one facilitator would create a session and generate a UID.  Each participant would reference that UID in their opening attestation linking the participant together.  Each participant is both a presenter and a viewer, so they will individually broadcast their messages while and generate a closing attestation when the session is complete.  Similarly the session facilitator can create a closing attestation that references all participating UIDs as well as a content hash for the entire session.</p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#blt-in-a-closed-network","title":"BLT in a Closed Network","text":"<p>It's likely there are cases where the public attestation component of the BLT is not desirable (e.g., court hearings, private meetings, etc).  In these instances the presenter's identity can be established through traditional mechanisms by a third party and the the Attestation UID is created on their behalf.  There is an extra step here to encrypt the additional data that is included in the attestation and distributing a key to specific audience members so that they can decrypt that data for verification.  Assuming in this scenario the content distribution infrastructure is also controlled, these aspects can be integrated into the \"log-in to the meeting\" process.</p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#why-bother-with-this","title":"Why bother with this?","text":"<p>We think it's pretty obvious, but live streaming deep fakes is definitely going to be a thing. Does the BLT fix everything? absolutely not, but it's a step in the right direction.  Just like fake watermarks in produced media, digital authenticity is always a cat and mouse game.  We believe that by tying reputation (a.k.a digital identity) into on-chain attestation makes the best use of the tools that we currently have available.  We also think that sometimes to avoid technical vulnerabilities you need non-technical solutions.</p> <p>To inspire fear, here are a couple of scenarios that will be likely within the next 12 months:</p> <p>1) Proxy Speakers: People are very busy and value their time.  Many meetings and decisions might not require them to be present, but there is an expectation that they are.  Why not abdicate your say to someone you trust speak on your behalf, but instead of seeking approval for this just have them be a fake you... nobody has to know. 2) AI Speakers: Why try to find someone you trust - just have an AI that is trained to think, act, look and sound like you be your representative?  What could possibly go wrong. 3) Malicious Intent: Many of us are already pretty weary of trusting people over with personal information over the phone or in chats.  But hop on a video call and we get a pretty high level of comfort that we are speaking with someone legitimate.  Especially if they look and sound like you.  People are way more willing to give away private information to someone who \"looks\" like they are trying to help.</p> <p>I could keep going, but it makes me depressed...</p>","tags":["live streaming"]},{"location":"2024/07/04/the-blinky-light-thing/#our-vision","title":"Our vision","text":"<p>We need to start somewhere and Zeroth Technology is eager to do our part.  We don't have all the answers to this and we think it must evolve rapidly to stay a step ahead.  We look forward to community inputs to help us do a better job of face detection, noise filtering and color mixing.  We want to make a mobile version that is super simple to use and we want to have a host of independent verifying solutions.  </p> <p>We don't think every live stream needs to be authenticated, but we need to have the tools ready for the ones that do.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-07-08</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 5</p> <p>File SHA-256: <code>5391a7715d25e8c5...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-07-08 James Canterbury Uptated to BLT post <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["live streaming"]},{"location":"2025/06/18/the-role-of-content-addressability-in-context-units-building-distributed-knowledge-systems/","title":"The Role of Content Addressability in Context Units: Building Distributed Knowledge Systems","text":"<p>A technical exploration of how content-addressable storage principles enable robust, distributed knowledge management through immutable context units</p>","tags":["content-addressable","context-units","distributed-systems","knowledge-management","immutable-storage"]},{"location":"2025/06/18/the-role-of-content-addressability-in-context-units-building-distributed-knowledge-systems/#introduction","title":"Introduction","text":"<p>When we think about things our knowledge flows like water - it's hard to grasp why we know something or where we learned it (we sometimes don't stop to think about if it is even true), yet we are able to draw conclusions that we \"think\" are right. If we later ask \"why did I think that\" we can rationalize our thought process to explain/convince others, but that is usually done in hindsight. We like to think that logic and reason guided us to a conclusion, but most of our decisions are just made by our gut and we then justify them later. The more often we turn out to be right the less we are challenged and the more confident we are in our thinking.</p> <p>When we ask an AI agent to think about things we have a different set of expectations.  Because this is a different form of intelligence we don't yet apply the same norms of reputation and trust; want the agent to explain everything to us - the steps it took to come to the conclusion that it did - we treat it like a kid in grade school always asking it to show the work -even if we don't intend to review it.  From the AI's perspective it's a double standard, but from our perspective we are erring on the side of caution.</p>","tags":["content-addressable","context-units","distributed-systems","knowledge-management","immutable-storage"]},{"location":"2025/06/18/the-role-of-content-addressability-in-context-units-building-distributed-knowledge-systems/#content-addressable-context-units","title":"Content Addressable Context Units","text":"<p>In our recent projects we have been doing a lot of work on Context Units (CxUs) - structured packages of knowledge with built-in relationships and provenance - for building out the next age of knowledge management systems. We have been developing an entire ontology for classifying CxUs in a wide variety of applications and better understanding of how they are used to \"justify\" agent decision making. In doing this we realized that by design CxUs are immutable (this is a hard requirement for our governance and auditability), but sometimes CxUs do need to change. Just as humans can change our minds when faced with different facts and circumstances the agents need to be able to adapt their context too. This becomes a slippery slope; it creates the need for version control over context units.</p> <p>The last thing we want to do is create yet another tracking system for managing versions, and since we all love decentralized storage the team decided to steal a page out of the content addressability book and use a hash of the context unit as the identifier.  As our CxUs are stored as Binary Large Objects (blobs) the use of CA fits nicely with several design patterns that we see in GIT and decentralized file storage systems like IPFS.  This opens a new design space for the use of CxUs both within out platform (which we have dubbed Pyrana) and possibly outside of it.</p> <p>The 10,000 foot view shows us something powerful: a distributed knowledge system that maintains integrity while allowing for evolution and collaboration.  The details show us something a bit more complex than we expected - content addressability makes the CxUs less useful for humans --&gt; even the index becomes a mouthful.  If the opening paragraph of this post were represented by a multi-hash it would be <code>12205c952d547bedb359b34b8939be8a43f1495ac0c46697e8078bba03c678d0669f</code>.  As a human I struggle making sense of this, it's much easier for me to find something like <code>Intro_paragraph.blog_post_22.knowledge_water</code>.  And if we needed to modify that paragraph even a little (lets say by removing the trailing space at the end) our hash becomes <code>1220678c718d2a78bf71ae159832e19bbdcc5b1d7cbd6e21c05e489e5420d910080c</code>.  Content addressability enforces an encoding and an exactness that is difficult for humans, but great for computers.</p> <p>The compromise comes in the form of tagging and version control.  This lets us search CxU blobs by something other than their ID and it lets us resolve the \"latest\" CxU while preserving a nice history of changes to them (well, technically it doesn't tell you what has changed, just that something had changed so we always need helper tools for comparison...)</p> <p>The Blockchain Connection</p> <p>There is another benefit here too - wait for it - blockchain... of course I'd bring this up. It doesn't really need to be a blockchain, but the nature of using a hash for a CxU looks a lot like an ID for a non-fungible token. Because CxUs will require some level of access control, we can treat each one like an NFT and gate their access using wallets. This is probably something out on the horizon, but it opens up some interesting design spaces; programmable knowledge...</p>","tags":["content-addressable","context-units","distributed-systems","knowledge-management","immutable-storage"]},{"location":"2025/06/18/the-role-of-content-addressability-in-context-units-building-distributed-knowledge-systems/#what-can-we-use-this-for","title":"What Can We Use This For?","text":"<p>Feel free to drop this in GPT and see the long list of use cases that spit out. Here are a few that I think are pretty cool.</p>","tags":["content-addressable","context-units","distributed-systems","knowledge-management","immutable-storage"]},{"location":"2025/06/18/the-role-of-content-addressability-in-context-units-building-distributed-knowledge-systems/#knowledge-trails","title":"Knowledge Trails","text":"<p>Every change is preserved. We can trace the evolution of knowledge from its first assertion through all subsequent refinements. This is crucial for scientific knowledge, legal reasoning, and any domain where understanding develops over time.</p>","tags":["content-addressable","context-units","distributed-systems","knowledge-management","immutable-storage"]},{"location":"2025/06/18/the-role-of-content-addressability-in-context-units-building-distributed-knowledge-systems/#conflict-free-collaboration-git-for-research","title":"Conflict-Free Collaboration (Git for Research)","text":"<p>Multiple researchers can work on the same knowledge domain independently. When they share their work, the system can automatically detect conflicts and help resolve them through version merging.  This also makes research highly reporduceable.</p>","tags":["content-addressable","context-units","distributed-systems","knowledge-management","immutable-storage"]},{"location":"2025/06/18/the-role-of-content-addressability-in-context-units-building-distributed-knowledge-systems/#distributed-knowledge-networks","title":"Distributed Knowledge Networks","text":"<p>Content addressing enables true distribution. Since content is identified by what it is rather than where it lives, the same Context Unit can exist on multiple systems while maintaining its identity.  Distributed knowledge unlocks a lot of things!</p> <ul> <li> <p>Peer-to-Peer Knowledge Sharing Researchers at different institutions can share knowledge directly without central authorities. The content addressing ensures integrity - you know you're getting exactly what was shared, without corruption or tampering.</p> </li> <li> <p>Resilient Infrastructure No single point of failure exists. If one system goes down, the knowledge remains available on other systems. This is particularly important for critical knowledge that society depends on.</p> </li> <li> <p>Efficient Synchronization Systems only need to share what's actually different. Content addressing makes deduplication automatic - identical content has identical addresses, so systems can quickly identify what they already have.</p> </li> </ul>","tags":["content-addressable","context-units","distributed-systems","knowledge-management","immutable-storage"]},{"location":"2025/06/18/the-role-of-content-addressability-in-context-units-building-distributed-knowledge-systems/#conclusion","title":"Conclusion","text":"<p>Content addressability transforms Context Units from simple knowledge containers into building blocks for distributed intelligence. By identifying knowledge through its content rather than its location, we create systems that are naturally resilient, efficient, and tamper-evident.</p> <p>The combination of smart IDs, multi-hash content addressing, and immutable versioning provides a foundation for knowledge systems that can scale globally while maintaining integrity locally. As our information needs grow more complex and distributed, these principles become essential for managing the knowledge that drives human progress.</p> <p>Key Takeaway</p> <p>The future of knowledge management isn't about building bigger centralized databases - it's about creating better distributed protocols. Content-addressable Context Units show us how to get there: by making knowledge identify itself, prove its integrity, and maintain its relationships no matter where in the world it travels.</p> <p>This is more than a technical solution - it's a new way of thinking about knowledge itself. In a world where information is abundant but trust is scarce, content addressability provides the foundation for systems we can trust not because we trust their operators, but because we can verify their operation.</p> <p>The tools exist. The principles are proven. The question now is whether we'll use them to build the distributed knowledge systems our interconnected world needs.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2025-06-18</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>0f9602d03b99ac88...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2025-06-18 James Canterbury added CA-CxU <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["content-addressable","context-units","distributed-systems","knowledge-management","immutable-storage"]},{"location":"2024/07/22/contract-wallets-as-containers/","title":"Contract Wallets as Containers","text":"<p>There are interesting things happening with account abstraction, but one function I haven\u2019t heard much about is transferring the ownership of a wallet from one user to another.  There are a bunch of use cases where this could be useful, I'd like to share my thoughts on how this effects supply chain.</p>","tags":["tokenization","supply chain"]},{"location":"2024/07/22/contract-wallets-as-containers/#a-bit-of-background","title":"A bit of background","text":"<p>Account Abstraction (enabled by ERC-4337) lets us expand the usage of wallets in many ways.  There are some great primers out there on ERC-4337; the best source (as usual) is the ethereum.org page on the topic.  But for our purposes let\u2019s just say that in the past we interacted with the Ethereum network using Externally Owned Accounts (EOAs) which were public addresses that we controlled with our private keys - if you have access to the private keys you can do anything with the assets held by that EOA.  Contract Wallets under ERC-4437 let us create rules that control how the wallet can interact with tokens and how users can interact with the wallet.  This lets us do all kinds of cool things around social recovery, gas-less transactions, multi-signature transactions, etc. - I do think the primary benefit of account abstraction will be the enhanced safety and usability of wallets.  But ERC-4337 also introduces the concept of \u201cauthorized controllers\u201d (which are likely to be good old fashioned EOAs, but can also be other contract wallets) and bundled transfers which let us move many assets in one transaction.  For the purposes of this post lets assume all assets are represented by tokens which are held in wallets (accounts) which are controlled by people.</p> <p></p> <p>In a EOA world if for some reason I wanted to transfer custody of all the tokens in my wallet to someone else I would have to transfer them one by one to that persons account, that could be a lot of transaction fees.  Of course I could just give them my private keys. But this isn\u2019t really giving them custody\u2026 the keys would still work for me, it\u2019s just sharing access.  And it\u2019s really stupid and insecure.  </p> <p></p>","tags":["tokenization","supply chain"]},{"location":"2024/07/22/contract-wallets-as-containers/#wallets-as-containers","title":"Wallets as Containers","text":"<p>There are many cases where we might want to transfer an entire portfolio of digital assets from one person to another without having to move the assets one by one. I\u2019m using the word person here to be familiar, but this is more likely business to business, or entity to entity transaction.  This isn't to be confused with bundled transfers (another use case under ECS-4337 where we can optimize the transfer of many assets by bundling them together); bundled (or batch) transfers are important to our use case, but we want the container itself to hold a state.  This last statement will become more clear as we dive deeper.</p> <p>My background is in pharma supply chain, we spend a lot of time talking about aggregation and disaggregation.  This is when you put stuff in a box and put boxes on a pallet and then send the pallet downstream in your supply chain.  It's pretty rare that the \"physical handling\" steps of a supply chain deal with individual units.  In previous attempts to recreate supply chain transactions on blockchains we would tackle this by creating \"container tokens\" - NFTs (usually ERC-721s) would be minted to represent the \"stuff\" and a separate token would be minted to represent the container.  These would all be initially owned by a single EOA (let\u2019s call them Alice Corp).  Alice Corp would then \"map\" the stuff NFTs to the container NFT to create the aggregation, then she would transfer the container NFT to Bob Corp.  In theory, Bob Corp was now the proud owner of all the stuff in the container... </p> <p></p> <p>But it didn't really work well so well... To map the individual stuff tokens to Bob Corp we would need to transfer them individually and so this actually made the introduction of the \"container token\" a bit of un necessary overhead.  When we consider the fact that we generally put containers within containers (items in a carton, cartons in a box, boxes on a pallet, etc) and this adds even more levels of overhead.  Oh, and we also need to dis-aggregate, or unpack the pallets and boxes and repack them with different stuff.  This is usually what wholesalers do and is one of the biggest contributors to \"opaque supply chains\".</p> <p>Note</p> <p>It's worth noting here that GS1.org has done a ton of work in defining supply chains using a core business vocabulary in CBV 2.0.  One important, if not niche, distinction in aggregation is the usage of \"associations\" where the container is reusable and carries a history of it's own - I like the example of finishing whiskey by aging it in used red wine barrels... the container leaves a lasting imprint on the product, versus the box that you might store the bottles of whiskey in after they are finished.</p> <p>Container tokens don't work.  But maybe container wallets can.</p> <p>In supply chain we are primarily concerned with managing the \"state\" of products as they move through their lifecycle.  We think of state as the changing of relationships between entities (accounts) and products - in other posts we've discussed how this takes the form of claims on tokens.  But we didn't really solve for aggregation and the fact that supply chain events (the transition of state) almost always occur an an aggregate level.  Containers have state and that state needs to be associated to all of the assets within that container at a given point in time.  Container tokens require extensive mapping, but container contract wallets, enabled by ERC-4337, can facilitate this by letting us store additional attributes (and apply logic) within the wallet.</p> <p></p> <p>From here it's not too far of a stretch to use the container wallet as the method for embedding rights over the assets it contains - this has the potential so solve for the multiple claims of regulatory seizure (the right of a regulator to seize custody of an asset) or fiscal liens (the right of a bank to seize ownership of a financed asset).  Account abstraction allows us to embed many different logical functions into the wallets that hold our stuff...  They might even aid in our ability to transition state confidentially - though that is a topic for another post.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-07-23</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 3</p> <p>File SHA-256: <code>b866a5f63464fe28...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-07-23 James Canterbury adding the containers as wallets blog post <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["tokenization","supply chain"]},{"location":"2024/10/18/data-asset-token-vs-product-token/","title":"Data Asset Token vs. Product Token","text":"<p>I\u2019ve recently been thinking a bit about data as an asset and the concept of data asset tokens.  Much of my thinking was influenced by Michael Clark whom I had the pleasure of meeting at the recent Filecoin Uncharted event (thanks Porter).  This post is probably more about vocabulary than content, but vocabulary is important (at least that's what I tell my kids).</p> <p>We talk a lot about the importance of owing our own data, but what does that really mean?  The concept of ownership has been defined and redefined 1000 times over, I try to ground myself in a simple view - ownership is one form of relationship between an entity and an object; a simple test of ownership is: if I own an object I can destroy the object.  The common word to describe this type of relationship is a \u2018Right\u2019 and (just to round out our vocabulary) when you want to declare that you have a right you are making a \u2018Claim\u2019.  Often we need to make a 'Claim' before we can evoke an action allowed to us by our 'Right'.</p> <p>Of course the right of ownership also gives you the ability to assign other rights (custody, seizure, liens) either to yourself or to another entity; all these rights describe different relationships between entities and the object.  </p> <p>If this sounds familiar, then thank you for reading my blogs\u2026 We have been trying to articulate the nuanced role of rights and claims against physical assets for years.  In supply chain we call the objects \u2018products\u2019 and some people call them \u2018assets\u2019.  In a network we represent them as \u2018tokens\u2019.</p> <p>So when I heard the term \u2018data asset token\u2019 it made me think\u2026 could data be an object? Well of course we have object oriented programming (OOP), where we define a data structure to include the object's attributes or properties and we define the method (wether you call them functions or procedures) that dictate how that object behaves (I like to think of these as rules).  Central to OOP is the concept of encapsulation which lets us put a nice wrapper around the object, it's properties, and it's rules. At any point in time that encapsulated object has a state - but often this state is only valid within the application that enforces it's rules.  Blockchains have changed this by letting a protocol enforce the rules and thereby letting everyone trust the state.  But this starts to get a little wonky when we have states that change more rapidly than blocks being produced, or if that same object if governed by different rules over the course of it's lifecycle.</p> <p>If we are dealing with a physical object we can visualize state, and state changes, as transitions while the object moves through its life cycle. Relative to most things digital, a physical object\u2019s state changes very little.  Making, moving, and consuming products (minting, transferring, and burning tokens) happens in (mostly) discrete steps. And there are a finite number of \u201cstates\u201d that we need to manage.</p> <p>On the other hand natively digital data can change A LOT.  And they generally have poorly defined or amorphous states.  To even begin getting our heads around this we have to start thinking in terms of static and dynamic data.  And we also need to account for references and pointers\u2026 often the \u2018data\u2019 does not stand on its own - other data is needed to give it context.</p> <p>So to treat data (like your personal medical records) as an object (that is to say tokenize it) we need a strong hierarchy and an understanding of context.  We also need a clear definition of what the token is supposed to do - if the token is a pointer to a dynamic data set then it\u2019s really just a gateway.  If, as we tend to think of it in NFT land, the token contains a hash of the data - or is a CID like in IPFS, then the token is possibly a snapshot of the state at some point in time.  These are two very different things.</p>","tags":["tokenization","supply chain"]},{"location":"2024/10/18/data-asset-token-vs-product-token/#a-compromise","title":"a compromise","text":"<p>I would propose a blend of the above.  A token can be defined that represents the attributes of the data set - these being the characteristics that, if changed, you would have a different object.  On this \u2018Data Token\u2019 we can assign rights and manage claims like we do for physical product tokens.  We can combine data tokens by burning and minting like we do for raw materials consumption.  We can associate data by placing the tokens in the same wallet (wallets as containers). However, these tokens can also have a pointer to a dynamic data set that, at any point in time, is the state.  The claims one has on the data token define how (or if) they are able to access the referenced data.  We can get creative with how to manage the state of the referenced data, but I don\u2019t think that needs to be over engineered - though we always need to consider that it is the responsibility of the data steward to make the referenced data available when a claim is proven\u2026 if they fail in that duty it doesn\u2019t matter what rights you have, you can't get your data.  Unless everyone is willing to self custody their data (which is probably the worst idea I can think of\u2026) data stewardship should be a service you pay for.  \u2014&gt; there is a reason I am a big fan of Filecoin and still hopeful for Eigen DA (not just because I run an EigenPod\u2026)</p> <p>In summary, I think the momentum is still gathering.  Data Asset Tokens offer much better control over data and a clear definition of rights - as well as a way to prove and/or transfer those rights in the form of claims.  When I write about supply chain I talk about the shift from process oriented data to product oriented data - perhaps we need to reframe that as a shift to asset oriented data or identity oriented data (though I\u2019m still uncomfortable thinking of identity as an asset\u2026)</p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-11-01</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>7823acb11af7bb43...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-11-01 James Canterbury Update data_asset_tokens.md 2024-11-01 James Canterbury posting data asset tokens blog and updates to BLT <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["tokenization","supply chain"]},{"location":"2025/03/03/enhancing-digital-identity-verification/","title":"Enhancing Digital Identity Verification","text":"<p>A Strategic Framework Against AI-Driven Identity Fraud</p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#abstract","title":"Abstract","text":"<p>The proliferation of AI-generated deepfakes has escalated threats of identity fraud in digital communications. This paper examines existing identity verification methods, introduces a strategic framework employing layered defenses to significantly increase attacker complexity, and proposes integrating cryptographic visual signatures alongside traditional verification methods. By analyzing attacker-defender dynamics using game theory, and referencing contemporary adversarial economics literature, we demonstrate the practical effectiveness of combining multiple verification modalities to deter identity fraud.</p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#1-introduction","title":"1. Introduction","text":"<p>The convenience of digital communication also introduces sophisticated threats, notably AI-driven deepfake identity fraud. Ensuring participant authenticity is paramount. Recent reports indicate that fraud attempts using deepfakes have increased by 2137% over the last three years, highlighting the urgency of addressing AI-driven identity fraud. \u200bsignicat.com This paper explores current identity verification strategies, presents an enhanced security framework, and advocates cryptographic visual signatures as superior solutions to traditional watermarking or biometric verification due to their resilience against real-time manipulation. The rise of generative AI has significantly increased the scale and sophistication of cybercrime, particularly identity theft and fraud, necessitating advanced verification methods. \u200bweforum.org</p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#2-definitions","title":"2. Definitions","text":"<ul> <li>Individual: An entity with a distinct identity, which may be a person or an artificial intelligence.  </li> <li>Entity: A group or organization represented by an individual or authorized representative.  </li> <li>Audience: A collective of individuals connected to an entity or individual.  </li> <li>Relationship: The connection between two or more individuals or entities.  </li> <li>Trust: Confidence in the authenticity and authority of an individual or entity.  </li> <li>Credential: Evidence of identity and authority.  </li> <li>Presenter: An individual asserting their identity or authority.   <ul> <li>Honest Presenter: A legitimate individual seeking to prove that they are in fact themselves  </li> <li>Dishonest Presenter: A fraudster seeking to pass themselves off as someone else  </li> </ul> </li> <li>Verifier: An individual or group seeking to confirm the presenter's identity or authority.   <ul> <li>Honest Verifier: An individual who when presented evidence will accept that the presenter is honest  </li> <li>Dishonest Verifier: An adversary that will attempt to sew distrust by pretending an honest presenter is fraudulent.  </li> </ul> </li> <li>Identity Fraud: Deceptively assuming another's identity to prompt actions in individuals who trust that identity which are not generally in their interest.  </li> <li>Identity Theft: Assuming another's identity to act on their behalf without consent.</li> </ul>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#3-foundations-of-trusted-relationships","title":"3. Foundations of Trusted Relationships","text":"<p>Relationships between one, or many, individuals are formed at different levels.  A trusted relationship is one where you are confident that you \"know\" the individual you are interacting with.  Attackers exploit this confidence when committing identity fraud - i.e., trying to fool you into believing that they are someone whom they are not - someone whom you think you know.</p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#31-modes-of-relationships","title":"3.1 Modes of Relationships","text":"<p>There are four basic modes in which relationships between individuals can be described which reveal the symmetry or asymmetry in knowledge.  Each of these modes has different trust considerations:</p> No-Prior Mutual Knowledge:  Neither party has prior knowledge of the other.  Mutual Knowledge:  Both parties are familiar with each other.  Asymmetrical Knowledge:  One party is known to the other, but not vice versa.   Inherited Relationships:  Trust is established through a mutual third party.","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#32-theoretical-underpinnings","title":"3.2 Theoretical Underpinnings","text":"<ul> <li>Social Capital:  The value derived from an individual's network of relationships. In digital contexts, \u201ccapital\u201d might mean verified reviews, endorsements, credentials, or cryptographic proofs of identity.  It is a way of representing an individual's reputation or track record.  Social capital can take a long time to establish, but can be lost very quickly making it a highly valuable asset.  </li> <li>Game Theory: Given the nature of the asymmetric information involved in establishing relationships, aspects of game theory provide us a robust framework in which to evaluate the interactions between presenters and verifiers. Or more specifically, the manner in which dishonest presenters might attack honest verifiers, and vice versa. However, establishing a minimax strategy in game theory assumes some level of rational behavior and stable costs to the dishonest parties. In the case of identity fraud, the motives may be to simply cause chaos and therefore may not be rational.  Additionally the rapid advancements in technology make the costs unstable (they are dropping rapidly), making a purely game theoretical approach impractical. \u200bwelivesecurity.com That said, signalling games (a subset of game theory) are highly relevant in modeling the behavior of presenters and verifiers.  </li> <li> <p>Signaling Theory: Holds that trustworthy signals are those that are difficult or costly to fake. Examples: time-stamped cryptographic certificates, official government IDs with embedded biometric checks, or \u201cchallenge-response\u201d verifications in real-time. In a signalling game one player (the \u201csender\u201d) sends a signal that the other player (the\u201dreciever\u201d) uses to update their belief and choose an action.  In our contest the \u201cpresenter\u201d sends signals of identity (credentials, documents, biometric data) to convince the verifier of their legitimacy.  The concept of \u201ccostly signalling\u201d states that honest signals can be distinguished from fake ones if they entail costs or risks that a dishonest presenter is unwilling to bear.  </p> </li> <li> <p>Risk-Based Trust Models: Can be used to establish common practice across a spectrum of potential fraud types.</p> <ul> <li>Zero Trust Framework: Originally devised for network security, the principle is \u201ctrust nothing without verification.\u201d Every new session, device, or user must pass through identity checks for each interaction.  </li> <li>Likelihood vs. Impact Assessment: The more significant the consequences of a fraud, the more extensive the identity checks required. (You\u2019d probably require more rigorous verification for a high-stakes board meeting than for a casual team chat.</li> </ul> </li> <li> <p>Socio-Psychological Factors: People are people and are susceptible to making irrational decisions in different situations.  </p> <ul> <li>Social Proof: In the `inherited relationships` mode, People sometimes rely on references or recommendations from trusted third parties (e.g., LinkedIn endorsements, recommendations from friends, partner org attestations).  </li> <li>Familiarity &amp; Repetition: The mere-exposure effect can enhance trust. In a digital context, repeated presence with consistent identity signals can build credibility over time.  </li> <li>Distressed Situations: People tend to ignore established forms of trust when placed under certain pressures (i.e., a fear that a loved one is in danger) which may lead them to make irrational decisions.</li> </ul> </li> </ul>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#4-motivations-for-identity-fraud","title":"4. Motivations for identity fraud","text":"<p>The motivations for committing identity fraud are vast, but the majority of them are done in an effort to fool an individual, an entity, or an audience into doing something that they normally would not.  Examples include:</p> <ul> <li>Signing an authorization to transfer a large sum of money  </li> <li>Providing personal information that can later be used to conduct identity theft  </li> <li>Broadcasting fake statements about oneself in order to influence an audience  </li> <li>Introducing fake evidence into an otherwise trusted scenario in efforts to cause dissent and confusion.</li> </ul> <p>Recent examples show the proliferation of such motives:</p> <ul> <li>AI-generated selfie anomalies accounted for 34% of emerging biometric fraud cases, and deepfake incidents surged sevenfold between Q2 and Q4 2024. \u200bbiia.com </li> <li>Criminals are using generative AI tools to create videos for fictitious or misleading promotional materials for investment fraud schemes, underscoring the necessity for robust verification methods. \u200bic3.gov </li> <li>The rise of generative AI has significantly increased the scale and sophistication of cybercrime, particularly identity theft and fraud, necessitating advanced verification methods. \u200bweforum.org</li> </ul>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#41-motivations-to-take-preventive-action-against-identity-fraud","title":"4.1 Motivations to take preventive action against identity fraud","text":"<p>If an individual or entity has a reputation to maintain (perhaps they are a prominent political figure or an entity with an established brand), just the threat of identity fraud  is often enough to motivate one to take preventive action.  Over time there will likely be a flywheel effect where several actions become commonplace and those who do not do them are considered to have a lower reputation.</p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#5-methods-for-preventing-identity-fraud","title":"5. Methods for preventing identity fraud","text":"","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#51-relationship-modes-and-trust-building","title":"5.1 Relationship Modes and Trust-Building","text":"<p>In this section, we will examine various methods for establishing trust in each of the defined relationship modes. </p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#511-parties-have-no-prior-knowledge-of-each-other","title":"5.1.1 Parties Have No Prior Knowledge of Each Other","text":"<p>Identity Bootstrapping - Leveraging some common ground that both parties can draw trust from.  Note, there may be several things they have in common and each party may draw trust differently (for example  - a citizen may trust a police officer if they see them in a setting where they are expected and if they \"appear\" to be an office. The officer may trust a citizen if they present a valid form of ID and are acting in accordance with their expectations).  It is easy to see here the role that implicit and explicit bias play in identity bootstrapping.</p> <p>Studies have found in a digital setting the role of implicit and explicit bias is reduced, or at a minimum altered, as there are far fewer input references to trigger a bias and people tend to have an expectation of zero-trust. In a Digital context the typical tools for bootstrapping identity are:  </p> <ul> <li>Use of Verified Credentials: Each party might rely on an external authority (e.g., a certificate authority, or a digital ID service) to verify identity claims.  </li> <li>Multi-Factor Verification: Combining something you know (password/PIN), something you are (biometric data), and something you have (hardware token) in a form that only you can do.  </li> <li> <p>Usage of a Digital Identity: A cryptographic identifier for an individual. It may be used in conjunction with, or in addition to other tools.  A common example is a 'crypto wallet' where the owner must demonstrate knowledge of their private key in order to take some action and that demonstration is verifiable by any other individual.</p> </li> <li> <p>Mutual Introduction by a Trusted Third Party - This is a form of inherited trust which in a digital context is facilitated in two primary ways.  </p> </li> <li> <p>Usage of a trusted individual - Where two unknown parties have a mutual trusted relationship with a third individual who coordinates a meeting.  This is a straight forward digital representation of a physical introduction and relies on the mutual party to have some level of trust with each of the other two.  </p> </li> <li>Usage of a trusted platform - Leveraging a mutually trusted platform that requires some form of authentication for all parties to join.  For example a google meeting where participants sign in with their google ID, or an enterprise Microsoft Teams login Entra ID.  This approach places complete trust in the authentication method of the platform, which is often enterprise specific.  </li> </ul> <p>Note</p> <p>This form can be significantly improved by using a strong form of identity for logging into the meeting (I.e., sign-in with Ethereum, or the inclusion of a verified credential in a crypto wallet used for sign-in which can be displayed during the meeting.).  The challenge with this approach is standardization and the reliance of each video platform to adopt additional authentication procedures.   </p> <p>Meetings that require participants from several organizations, or where the participants may be unknown ahead of time typically rely on each participant possessing the meeting ID and passcode which is often distributed via email.  It's worth noting that most video platforms allow guest access, or the ability of individuals to modify their name as part of the log-in process.</p> <p>Real-Time Challenge-Response - In meetings that do not rely on platform logins participants can employ methods for verifying the identity of each other.    </p> <ul> <li>Identity Presentation: A weak form of identity where a presenter may need to display some form of ID or vouch for themselves (often it is not the ability for them to vouch, but the *manner* in which they vouch that the verifier can apply intuition to determine if the person is legitimate - while this may seem easy to spoof it is often more reliable than many other mechanisms)  </li> <li>Dynamic Codes &amp; Pass Phrases: A strong form of identification where a verifier can generate ephemeral codes, so the user must say or display them in real time.  These can include cryptographic signatures where a verifier may encrypt a secret using the presenters public key  </li> <li>Liveness Checks &amp; Proof of Human: Prompt a user to perform random actions (blink twice, move head side to side, hold hands in front of face) to prove it\u2019s a live human feed and not a deepfake or replay. These do not protect against application of real-time deep fakes where the presenter is a human, but are applying a filter to disguise their appearance.</li> </ul> <p>interesting</p> <p>Current practice for detecting nation-state actors where identity fraud is being perpetuated by Nation States requesting the presenter to make a statement that would be counter to their political mandates or carries severe social consequences in known countries can be an effective way of exposing identity fraud.</p> <p>Takeaway</p> <p>When parties are total strangers, you rely heavily on external trust frameworks and real-time verification methods.</p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#512-mutual-knowledge","title":"5.1.2 Mutual Knowledge","text":"<p>When all parties know each other one of the most reliable methods to detect identity fraud in real time is simple observation and comparative analysis to previous interactions.  However, this is challenging in several scenarios:</p> <ol> <li>Distressed Situations - if a presenter is in a situation that may make them act differently than they normally would inconsistent behavior may be dismissed by the verifier.  </li> <li>Social Engineering or A Long Con - if the presenter has managed to build a reputation with a fraudulent identity over time, this scenario may be based on false mutual knowledge (for example someone impersonates a prominent hedge fund investor - they will look and sound like publicly available media about that investor - and over time they build trust with a small group until they convince them to transfer a sum of money to \"invest\").  </li> </ol> <p>In these scenarios it is typically the presenter (the one perpetuating the identity fraud) that is reaching out to the verifiers, which implies that they will control the platform and have influence over weak forms of identity, they may even have a fraudulent form of digital identity.  A mutually trusted third party may provide some security, but a verified credential in combination with a strong digital identity will provide the best assurances.</p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#513-asymmetrical-knowledge","title":"5.1.3 Asymmetrical Knowledge","text":"<p>When one party is known to others (i.e., that individual has a reputation), but the others are not known to them it creates asymmetrical knowledge.  The primary mode of identity fraud prevention here depends on who initiates the interaction.</p> <p>If the unknown party initiates an interaction with the known party the unknown party will need to bootstrap the relationship using some of the tools described in section A.  The known party may not need to verify their identity as they were the ones being sought out.</p> <p>If the known party initiates interaction with an unknown party (the most likely scenario in the case of identity fraud) then the known party must reinforce their identity with tools described in section A.  The identity proof must be zero trust at this point as the known party would control the platform on which they are connecting.</p> <p>Where there are several known parties and several unknown parties the use of reputation and background context can be applied.</p> <ul> <li>The party with prior knowledge may reference why they trust the other (e.g., \u201cWe have worked with Acme Corp before; here\u2019s our history.\u201d)  </li> <li>Third-party confirmations (certificates, rating systems) can be shared to bring the less-informed side up to speed.  These are not necessarily \"verified credentials\", but they can be.</li> </ul> <p>Practical Example</p> <p>In a business partnership a potential client might \u201cknow\u201d your company from its public presence (website, press releases) but you have no knowledge of them. You can use standard due diligence checklists (KYC processes, references) to establish identity and reliability.</p> <p>Importantly the party sharing their identity should practice Selective Disclosure. Where the known party can selectively share verifiable credentials tailored for the relationship (e.g., a proof of membership in a professional organization, or a relevant security clearance).  This requires some special considerations in the manner in which identity is shared, it also exposes the importance of Explicit Consent and Data Privacy where if one side is known but the other side is effectively \u201cin the dark,\u201d the known party must be mindful about what info is revealed. Avoid oversharing personal data to reduce the risk of doxxing or misuse.</p> <p>Takeaway</p> <p>Establishing trust is partly about bridging the gap in knowledge with verifiable references and selectively disclosed proofs.</p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#5131-large-asymmetrical-knowledge","title":"5.1.3.1 Large Asymmetrical Knowledge","text":"<p>A subset of Asymmetrical Knowledge is the mode where an individual (likely someone with a reputation) is presenting to a large audience where they do not need to know the individual verifiers.  In this case the presenter wants to share a set of credentials such that the audience can reliably connect them to their reputation.  The same tools described above can be applied.</p> <p>Practical Example\"</p> <p>The CEO of a company is presenting during a quarterly earnings meeting from a remote location and wants to convey their realness to investors to eliminate the risk of identity fraud.</p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#52-additional-controls-that-can-be-applied","title":"5.2 Additional controls that can be applied","text":"<ol> <li>Authentication vs. Authorization: <ul> <li>Even if parties are personally familiar, robust authentication is still crucial to ensure nobody\u2019s impersonating a known contact (e.g., a deepfake of your CFO).  </li> <li>Authorization steps ensure that even recognized individuals only get the level of access they need.  </li> </ul> </li> <li>Routine Verification Protocols: <ul> <li>Voice Biometrics or Video Liveness Testing: Reinforce that \u201cjust because I recognize the face doesn\u2019t mean it\u2019s real.\u201d  </li> <li>Contextual Cues: Shared knowledge or \u201csecret phrases\u201d can confirm identity in real-time (like \u201cWhat was the name of that caf\u00e9 we met at last month?\u201d)   </li> </ul> </li> <li>Continuous Trust Monitoring: In repeated interactions (weekly calls, ongoing collaboration), it may suffice to do light verification. But for sensitive matters (signing big contracts, giving final approvals), add an extra step (e.g., digital signature, official stamp from an identity provider).</li> </ol> <p>Practical Example</p> <p>Organizational Teams Staff might know each other personally, but an attacker could be deepfaking the boss to request a wire transfer. A culture of verifying suspicious requests (via phone call or official corporate channel) helps combat such scams.</p> <p>Takeaway</p> <p>Don\u2019t skip security just because you\u2019re \u201cfriends.\u201d Familiarity-based trust is vulnerable to impersonation in the deepfake era.  </p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#6-out-of-band-cryptographic-signatures","title":"6. Out-of-Band Cryptographic Signatures","text":"<p>Reliance on traditional watermarking or secure platforms can lack real-time resilience.  Out-of-Band cryptographic signatures can dynamically authenticate identities during interactions making real-time deepfake attacks prohibitively complex.  This is when a message is transmitted between the presenter and the verifier, that is not reliant on the software used to facilitate the communication it is considered out of band.  An example might be where the presenter sends a text message or an email to the verifier.  It's important to note that this mode of communication is only effective in preventing real-time deepfakes when it is used contemporaneously during the interaction.  That is to say, If using a classic challenge-response mechanism, the challenge should be received and responded to while the presenter and the verifier are interacting.</p> <p>Another approach could include embedding dynamic, cryptographically generated patterns into visual feeds. This method combines visual confirmation with cryptographic security, making real-time deepfake attacks significantly more challenging. The integration of such signatures provides a robust defense against identity fraud.  Performing this out-of-band requires a novel approach to authentication.</p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#7-best-practices-and-future-directions","title":"7. Best Practices and Future Directions","text":"<p>Organizations should adopt a zero-trust mindset, continuously verifying identities regardless of prior interactions. Educating users on recognizing deepfakes and implementing cryptographic visual signatures can further enhance security. Ongoing research into AI-driven detection and standardized protocols will be essential in adapting to evolving threats.</p> <p>A flexible approach to identity verification involves selecting from a spectrum of methods based on context and risk:</p> <ul> <li>Basic Authentication: Usernames and passwords.  </li> <li>Enhanced Verification: MFA and digital certificates.  </li> <li>Real-Time Checks: Challenge-response mechanisms and liveness detection.  </li> <li>Advanced Security: Cryptographic visual signatures.</li> </ul> <p>Combining these methods strategically increases security and deters potential attackers.  Game theoretical approaches are still relevant and should be considered in the design of security measures, However, the rapid advance in technology for generating deepfakes in real-time significantly decreases the cost of such attacks and makes many game theory models difficult to apply.  Therefore, the best practice includes guidance from such models and a thorough application of experience and common sense. </p>","tags":["identity fraud","live streaming"]},{"location":"2025/03/03/enhancing-digital-identity-verification/#8-conclusion","title":"8. Conclusion","text":"<p>Combating AI-driven identity fraud requires a strategic combination of verification methods. By understanding the attacker-defender dynamic through game theory and integrating advanced solutions like cryptographic visual signatures, organizations can establish robust defenses that adapt to the evolving landscape of digital threats.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2025-03-03</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 6</p> <p>File SHA-256: <code>023d636f7539a7a6...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2025-03-03 James Canterbury posting digital identity verification <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["identity fraud","live streaming"]},{"location":"2023/11/01/digital-twin--product-token/","title":"Digital Twin \u2260 Product Token","text":"<p>TLDR \u2013 Digital twins are for running simulations and optimizing for future events.\u00a0 Product tokens are for transferring claims against an asset (ownership, custody, etc) and recording the state of past events.</p> <p>At the end of this post I pasted a very nice summary from ChatGPT on the definition of digital twin and product tokens - I think the LLM agrees with me.</p> <p>\u2014\u2014</p> <p>A product token is a medium of exchange that records the transfer of a claim on a Real World Asset (in this case a product) between one party and another.\u00a0 Holding a claim on a product token gives the holder certain rights, such as the ability to sign metadata about the product \u2013 over time the metadata + the history of claims transfers builds up a very robust data set about that product and its movements.</p> <p>A digital twin is meant to simulate a Real World Asset in digital form - while it is in fact a digital representation of a unique or of a group of similar products it does not in fact carry all of the product attributes that would typically be required to form a digital twin (while it\u2019s certainly possible to include anything you want in the metadata, it\u2019s unlikely that you would include information about physical properties (i.e., solubility or physical dimensions)).\u00a0 In the classical definition of digital twin these are important parameters that would feed into a simulation.</p> <p>But the two are not completely separate; in defining and monitoring a product token you can learn a lot about the products actual handling in the real world by studying the patterns of movements and data updates which could include typical temperature ranges number of handoffs or amount of time spent in transit - all of this real world data is very valuable in fine tuning your digital twin models and should be used as input into your simulations.</p> <p>However, because there is often a lot of intellectual property associated with a fully structured digital twin it may also make sense to represent that digital twin as a non fungible token. \u00a0This allows the owner of that NFT to claim ownership over the model based on that digital twin; it could allow for verification of results and easier sale or transfer of those digital twins if an organization has hundreds of thousands of digital twins. \u00a0Tokenization in this respect creates a very good way of cataloging an inventory the digital twins themselves and allows fine grained version control over changes to the attributes of each. \u00a0But again, this is not the same as using a product token which is a medium value exchange.</p> <p>Jamey: What is the definition of a digital twin?</p> <p>Chat GPT:</p> <p>A digital twin refers to a virtual or digital representation of a real-world entity, system, process, or object. It's essentially a digital counterpart that mirrors the physical counterpart, allowing for real-time synchronization and interaction between the two. This concept is commonly used in various industries, including manufacturing, engineering, healthcare, urban planning, and more. Here's a more detailed definition: A digital twin is a digital or computer-based representation of a physical entity or system. It encompasses both the physical aspects (geometry, structure, materials) and the functional aspects (behavior, performance, interactions) of the entity. The digital twin is created by collecting and integrating data from various sources, such as sensors, simulations, and historical records. This data is used to create a dynamic, real-time model that reflects the current state and behavior of the physical entity. The digital twin concept offers several benefits, including:</p> <ol> <li>Simulation and Testing: Digital twins enable simulation and testing of real-world scenarios without affecting the physical entity. This is useful in optimizing performance, predicting behavior, and identifying potential issues before they occur.</li> <li>Monitoring and Maintenance: By continuously collecting data from sensors embedded in the physical entity, the digital twin can be used to monitor its condition, performance, and health. This data can inform maintenance schedules and prevent downtime.</li> <li>Analysis and Optimization: Digital twins provide a platform for analyzing data and optimizing processes. They can be used to identify inefficiencies, improve resource allocation, and enhance overall performance.</li> <li>Predictive Capabilities: With historical and real-time data, digital twins can make predictions about future behavior and performance, aiding in decision-making and planning.</li> <li>Collaboration: Digital twins facilitate collaboration between various stakeholders, such as designers, engineers, and operators. They provide a shared platform for understanding and working with complex systems.</li> </ol> <p>Examples of digital twins range from individual products like cars or turbines to larger systems like smart cities or entire manufacturing plants. As technology advances, digital twins are becoming more sophisticated, incorporating elements of artificial intelligence, machine learning, and IoT (Internet of Things) connectivity to enhance their accuracy and capabilities.</p> <p>Jamey: What is the definition of a tokenized product on a blockchain</p> <p>Chat GPT:</p> <p>A tokenized product on a blockchain refers to a digital representation of a physical or digital item that has been converted into a unique digital token on a blockchain network. These tokens are often used to represent ownership, authenticity, or some form of value associated with the underlying product. Tokenization involves assigning a unique identifier or token to a specific product and recording that information on a blockchain. Here's a breakdown of the key concepts:</p> <ol> <li>Tokenization: Tokenization is the process of creating a digital token that represents a real-world asset, item, or product. In the context of products, tokenization involves converting the attributes and ownership of a physical or digital item into a digital token that can be tracked, traded, and verified on a blockchain.</li> <li>Product: The term \"product\" in this context can refer to a wide range of items, including physical goods (e.g., luxury items, collectibles, real estate) or digital assets (e.g., software licenses, digital art, music).</li> <li>Blockchain: A blockchain is a decentralized and immutable digital ledger that records transactions in a secure and transparent manner. Each transaction is grouped into a block and linked in chronological order, creating a chain of blocks. This technology provides security, transparency, and trust in the recorded data.</li> <li>Token: In the context of blockchain, a token is a unit of digital value or representation that can be stored and transferred on the blockchain. Tokens can have various functions, such as representing ownership, granting access to certain resources, or serving as a medium of exchange.</li> </ol> <p>Tokenized products offer several benefits, including:</p> <ul> <li>Ownership and Provenance: Tokenization provides a transparent and verifiable record of ownership and the history of the product. This can be especially useful for tracking the authenticity and provenance of luxury goods, collectibles, and art.</li> <li>Fractional Ownership: Tokenization allows for fractional ownership, enabling multiple individuals to own a share of a high-value item. This can increase accessibility to assets that might otherwise be out of reach.</li> <li>Liquidity: Tokens representing products can be traded on blockchain-based marketplaces, increasing the liquidity of traditionally illiquid assets.</li> <li>Interoperability: Tokenized products can be integrated with other blockchain-based systems and services, enhancing interoperability and creating new opportunities for innovation.</li> </ul> <p>It's important to note that while tokenization offers various advantages, it also comes with regulatory considerations and challenges related to ensuring the accuracy of the physical asset's representation, verifying authenticity, and complying with legal requirements in different jurisdictions.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-06-15</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>199fb1ec49fbc1c6...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-06-15 James Canterbury added a bunch of old blogs... <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["tokenization","supply chain"]},{"location":"2024/04/15/does-knowing-make-you-accountable/","title":"Does Knowing Make You Accountable?","text":"<p>In my recent post about supply chain a friend replied in the comments that \u201cI\u2019ve come to the realization that big business doesn\u2019t want that level of transparency, because transparency = accountability.\u201d.\u00a0 To which my quick counter reply was \u201cBeing able to keep transactions confidential is key, but even this creates accountability - so we need an incentive that can outweigh that risk\u00a0\u201d.\u00a0 I then suggested that this would be a topic for my next post, so here it is </p> <p>Does knowing make you accountable?\u00a0 What are the real motivations for supply chain transparency.</p> <p>I worked on a project early in my career where we were writing an algorithm to calculate the rate at which product complaints were received over a three-week rolling window so that we might predict if we would hit a certain threshold in week four; \u00a0which would trigger an investigation.\u00a0 Because an investigation required resources, this would help us better plan \u2013 we were going to have better, more timely inspections and save the organization millions of dollars due to more efficient resource planning.\u00a0 As a side note, we didn\u2019t call this AI at the time \u2013 it was just statistical trend analysis\u2026 The math worked, but the program was never implemented.\u00a0 Leadership realized that once they had knowledge that an investigation should be done they had an obligation to do it, and this opened them up to additional liabilities and potentially raised the bar on all kinds of predictive actions they should take.</p> <p>At the time I didn\u2019t quite agree that sometimes it makes better business sense to \u201cnot know something\u201d.\u00a0 15 years later I still don\u2019t quite agree, but I understand the position\u2026 everything is complicated.\u00a0 In my opinion to not do something because it could make you accountable for not doing enough, means you just aren\u2019t motivated to do the thing in the first place \u2013 even if you know it\u2019s the right thing to do. \u00a0And if it is the right thing to do eventually it will be done (one way or another).\u00a0 So, while the statement that \u201cBusinesses don\u2019t want traceability because traceability = accountability\u201d is generally correct (especially in a very narrow time frame) \u2013 but it\u2019s also an excuse to avoid doing the work today.\u00a0 Accountability is a risk, if you are not prepared to stand behind your actions, but it you are doing the work accountability can be a protection.</p> <p>To fully round out this perspective we must also consider \u201cto whom are you accountable\u201d.\u00a0 If the entire world is watching then that is a pretty high bar.\u00a0 But if it is your business partners, customers, and regulators then you can set expectations appropriately.\u00a0 This is why confidential transactions are \u201ckey\u201d and when we talk about traceability it\u2019s important to limit that visibility into only those things you should have access to see \u2013 IMO determining the \u201cshould\u201d is the role of claims as discussed in my previous post.</p> <p>But this really boils down to \u201chow do you get people to do the work\u201d to participate in a network that provides supply chain visibility to those who should have it.\u00a0 Its\u2019 worth reflecting here on the relationship between motivation (an internal driver) and incentive (an external driver).\u00a0 I believe incentives are a catalyst to get people (or enterprises) to act on latent motivations, and that not everyone will react to the same incentives.\u00a0 Conversely, if there is no motivation, then most incentives will be short lived.\u00a0 So, in order to create supply chain visibility, we must make 3 basic assumptions:</p> <ol> <li>There are at least some participants in the supply chain who are motivated to do this. These are likely the endpoints \u2013 producers and consumers.</li> <li>Not everyone in the supply chain will respond to the same incentives.</li> <li>The adoption of such a solution will be gradual \u2013 there will be a (hopefully not-to-long) period of time where not all supply chain participants will be motivated to participate.</li> </ol> <p>There is also one more important component here.\u00a0 Compliance is not a motivator; at best it should be one of several incentives that get enterprises to take action.\u00a0 I know this is an idealistic statement, but I am a firm believer that if we want to build a sustainable solution we cannot lead with the stick \u2013 supply chain visibility must provide business value first, regulations are the guardrails, and compliance is a byproduct.</p> <p>Incentives:</p> <p>In my experience incentives always fall into two categories:</p> <ol> <li>Reduce Cost</li> <li>Increase Revenue With the latter being the more significant driver, but reducing costs is usually the lower hanging fruit and is typically easier to quantify in terms of value. Here are just a few incentives that have come up through the supply chain projects I\u2019ve had the privilege to be a part of:</li> </ol>","tags":["tokenization","supply chain"]},{"location":"2024/04/15/does-knowing-make-you-accountable/#reducing-cost","title":"Reducing Cost","text":"<ul> <li>Reduction of friction: Anywhere that there are delays in moving products or where processes rely on some form of a reconciliation to \"true up\" after a certain period of time is an area of friction. I love the word \"friction\" in this context - to me is sums all the stuff that doesn't make sense but we do anyway. Supply chain visibility and a common ledger for settling inventory eliminates this kind of friction.</li> <li>Better planning: Imagine the impact on planning cycles if a manufacturer were able to \"see\" their products being consumed in near real time. Or how we could reduce safety stock requirements when we could see inventory flow accurately. One of my favorite use cases is inventory redistribution for products with a shelf life - if we know something is about to expire, lets send a replacement and get the near expired one to someone that will use it.</li> <li>Directed Flow: Dynamically routing products based on attributes and inputs from some trusted oracle. Admittedly this use case is a little further out there, but the same way that DeFi created programable money with smart contracts we will see programable inventory.</li> </ul>","tags":["tokenization","supply chain"]},{"location":"2024/04/15/does-knowing-make-you-accountable/#increasing-revenue","title":"Increasing Revenue","text":"<ul> <li>New Markets: With better product linage and the ability to \"prove\" compliance with local laws companies with true supply chain visibility will be better positioned to win new country level tenders and bring products to new markets faster.</li> <li>Better Payments and Discounts: The next logical step from trusted supply chain visibility will be a better system for payments triggered by real world events (ironically this is the antithesis of procurement based supply chain visibility...). In the pharma industry the world of rebates and chargebacks has long been targeted as an area that blockchain will disrupt. one could argue that payments and discounts are a cost savings, but I believe if done correctly these actually lead to unlocking additional markets and customers.</li> <li>Collaborations and Joint Ventures: Being able to pair your products with something that complements them allows you to specialize. Trusted supply chain visibility and a shared ledger of inventory settlements makes it WAY easier to collaborate and enforce cooperative agreements.</li> </ul> <p>I'm sure I could go on and on, but this is a good list to get started.  The point is that forward and backward traceability (trusted supply chain visibility) unlocks business value for those with the right perspective.  For those without that perspective it will likely disrupt their business.  So coming back to the original question of \"Does knowing make you accountable?\" I think the answer is \"no\"; you were always accountable, but you just didn't know it.  I'm of the opinion that more (trusted) information is always better, and that knowing as much (or more) than your competition is sound business strategy.  While there is a risk that comes with accountability, the from the right lens of incentives the benefits will far outweigh those risks.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-06-15</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>eacf33116b93ce41...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-06-15 James Canterbury added a bunch of old blogs... <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["tokenization","supply chain"]},{"location":"2024/12/12/are-enterprise-ai-agents-just-rpa-rebranded/","title":"Are Enterprise AI Agents just RPA rebranded?","text":"<p>There\u2019s nothing new under the sun\u2026 If 20 years of technology consulting has taught me anything, it\u2019s that a good story can be retold forever.  Sometimes we just need to update it with the current buzz words or fit it to current the narrative for it to seem relevant.  Today\u2019s narrative of choice, of course, is AI.  We hear a lot about how enterprise AI agents are going to revolutionize corporate organizations, but when I ask people how, it sounds a lot like the same old story.  </p> <p>\"First we will find areas of high friction, high cost, or highly repetitive/error prone work.  Then we will model out the process and build an AI agent that can do it for us, but the AI agent will do it better.\"</p> <p>In 2018 we would have said \"robotic process automation\" instead of \"AI agent\".  In 2002 we might have said \"ERP\", in 1980 we would have said \"computer system\".</p> <p>Just for context, I studied industrial engineering in college, then for a solid five years within my career at EY, I focused on Business Process Modeling and Automation (BPMA).  I moved on to blockchain before we started calling it Robotic Process Automation (RPA) which sounds much cooler.  I always viewed this type of work as the systematic breakdown of a process into it's functions and a way to define business rules that govern those functions.  Once we do that we can use some software to essentially duct tape those functions together in a way that made more sense.  Prior to BPMA I wrote a lot of Excel macros to pretty much do the same thing.  </p> <p>RPA, BPMA, Excel Macros... they were all a ways to bridge the gaps between disconnected processes and the underlying systems that governed them.  When things were too broken we would rip out and rebuild the underlying system (ERP implementation) - but it was usually easier to apply bandaids.</p> <p>If we are not careful Enterprise AI Agents are going to do the same thing over again.  I'm quite certain there are some consulting firms dusting off their old RPA pitch decks as I write this.</p>","tags":["process modeling","robotic process automation","personal AI agents","enterprise AI agents"]},{"location":"2024/12/12/are-enterprise-ai-agents-just-rpa-rebranded/#we-can-do-better-right","title":"we can do better... right?","text":"<p>It doesn't have to be a repeat of our favorite episode. AI Agents are different, or at least they can be.</p> <p>I think a lot about what an \"AI Agent\" actually is; I haven't quite made my mind up yet.  I'm about 50% through the excellent book \"Making Sense of Chaos\" by J. Doyne Farmer.[1]  Usually I like to finish a book before commenting on it, but I think some of his work is very relevant for this topic. The first few chapters gave me an appreciation for the role of \"agents\" in modeling complexity economics and introduces the concept of \"bounded rationality\".  I think that Farmer's \"agents\" are a close approximation of what \"AI Agents\" really are.  Except that his agents run within models that operate in controlled environments, are trained on carefully curated data, and simulate the outcome of various scenarios that are used to help humans make better choices in the the real world.  I feel like AI Agents (at least the ones most of us can get exposure to) are getting trained on socially curated datasets (x, telegram, discord, websites in general) which have the convenient effect of telling us what we want to hear, or what we can observe as a trend, but isn't necessarily true.  Now that the tools for launching agents have become widely available (I was playing with virtuals.io(https://virtuals.io) over the weekend) we are taking an \"f* around and find out approach\" as opposed to Farmers more disciplined and scientific method.</p> <p>Farmers book also re-enforces one of my observations about traditional process improvement: We make the assumption that business processes are deterministic.  The minute you draw a process map or a decision tree you are defining a set of valid end states and making the assumption that rational actors will always cause the process to end in a valid end state, if that doesn't occur then the model was wrong --&gt; go back to your brown paper lined conference room and start drawing on the walls again (ahh the memories...).   Farmer's work points out the limiting effect that the \"assumption of equilibrium\" has had on traditional economic models. And how a few simple heuristics, the use of boundaries instead of rules, reenforcement learning, and a bit of randomness can generate very accurate predictions of future events.  </p>","tags":["process modeling","robotic process automation","personal AI agents","enterprise AI agents"]},{"location":"2024/12/12/are-enterprise-ai-agents-just-rpa-rebranded/#which-way-do-we-go","title":"which way do we go?","text":"<p>This brings me to why I think we are at an inflection point, where agents which were previously the in the realm of data science and predictive analytics are beginning to interact with the real world.  I have deep concerns with this, because the \"social data\" that these agents are being trained on is in fact generated, or influenced by other agents.  I will try not to turn this post into that particular rant, but suffice it to say self-fulfilling feedback loops are dangerous.</p> <p>To keep it practical (at least in my optimistic opinion) this should be the goal of AI Agents:</p> <ol> <li>The agent should have a straight forward objective and a time horizon on which they should achieve it.</li> <li>The agent should have a world model that lets them understand what is going on in context of their objective.</li> <li>We should provide boundaries to keep the agent from making harmful mistakes.</li> <li>The agent should make predictions based on their world model and learn from their mistakes.</li> <li>Finally, we should give them agency to act on those predictions in efforts to achieve their objectives within their boundaries. </li> </ol> <p>Much easier said than done I know...  This is just a rehash of the classic AI alignment problem.  I am also resisting the urge to include Asimov's laws of robotics (the zeroth law for which my company is named after).  The only reason I mention these is that I think enterprises are going to choke on that last part. </p>","tags":["process modeling","robotic process automation","personal AI agents","enterprise AI agents"]},{"location":"2024/12/12/are-enterprise-ai-agents-just-rpa-rebranded/#maybe-this-would-work-if-not-for-the-humans","title":"maybe this would work, if not for the humans.","text":"<p>Having grown up in an audit and accounting firm I can appreciate the extent of effort that companies go through in order to maintain control - giving agency to a model (even if it is tightly controlled) is not in their DNA.  But don't worry we've already came up with a cute solution - we will always keep a \"human in the loop\".  This however has many flaws which all boil down to the fact that humans are, well, human.  Interestingly enough, this is one way to introduce bounded rationality into a deterministic model... but I don't think that is the intent.</p> <p>Rationality of humans aside, the issue with this approach that I want to draw attention to is the bottleneck it will create.  In my limited experience of building and interacting with AI agents it is their relentless focus on their objectives that I am most impressed with (I am not surprised by this, after all they are programmed to be that way) but impressed to see it in action.  A favorite Einstein quote of mine is \"it's not that I'm so smart, I just stay with problems longer\".  Concentration and focus, coupled with access to knowledge, is powerful indeed.  Even if AI Agents are not efficient, or if their training is flawed, they can trial and error their way to a desired result far faster than we can because of their relentless 24/7 focus - putting a human in the loop takes away that advantage.</p> <p>We used to say that a well designed controls were like breaks on a car.  A car with brakes can get from point A to point B much faster and more reliably than one without them.  The shortened version was that \"breaks let the car go faster\".  Human in the loop is not a break, it is speed limiter that can cut your throttle when you need it most.</p> <p>In traditional process modeling \"human in the loop\" was usually referred to a \"procedural control\" meaning it wasn't programable logic, but was left to the discretion of the operator to what they thought best - there were rules of course, but someone had to decide to follow them (or break them).  This is what gave process models the flexibility to adapt when the unexpected occurred.</p> <p>If we just let AI Agents run business processes that we've already defined (traditional process modeling with some AI pixie dust sprinkled on top) we will probably get the same results we got every other time we tried to do this.  \"Continuing to do exactly what you have done in the past is exactly good enough to get you exactly where you are\" (that's another one of my favorite consulting phrases).</p>","tags":["process modeling","robotic process automation","personal AI agents","enterprise AI agents"]},{"location":"2024/12/12/are-enterprise-ai-agents-just-rpa-rebranded/#a-pivot-and-a-proposal","title":"a pivot and a proposal","text":"<p>Of course I hate long posts that complain about a problem and don't offer any tangible solutions, so here is my latest - only partially tested out - thinking...</p> <p>Large enterprises will not have the stomach to let AI Agents represent them in any meaningful way like the social AI agents that are currently taking over X.  They will not be given \"agency\" to do much other than advise management what to do (this time it's \"leadership in the loop\"... even worse!).  As excited as I am, as a start up, about using AI marketing agents this is not going to fly for large companies who already have a brand.  Their customers are their most valuable asset, they are not likely going to give that up (though every time I have to \"chat with an online representative\" about my wireless bill it makes me think otherwise...)</p> <p>So I see two paths:  Personal AI Engagement and Employee Productivity</p>","tags":["process modeling","robotic process automation","personal AI agents","enterprise AI agents"]},{"location":"2024/12/12/are-enterprise-ai-agents-just-rpa-rebranded/#1-personal-ai-agent-engagement","title":"1) Personal AI Agent engagement","text":"<p>Where a corporation should be hesitant to let an AI agent represent them to their customers, customers will jump at the chance to let AI agents represent them to their providers (the corporates).  If I could ask my Personal AI Agent (PAIA) to interact with my cellular provider and get an explanation as to why I'm being charged for the equipment I returned months ago, I would do it in a heartbeat!  PAIAs are going to be the next wave of \"customers\" that enterprises need to market to.  Corporations have spent decades marketing to humans, humans with short attention spans and who lack relentless focus on objectives.  Their PAIAs won't care if \"there is an unusually high wait time\" or if they need to try 50 different coupon codes before they get one that isn't expired; they won't forget to cancel after that 3 week free trial is up either.  But that's just the easy stuff... the PAIA will try to negotiate with providers over everything, and they will be willing to agree to more complex terms and conditions (which they will actually read) than the typical consumer - and they will follow through with their end of the bargin and hold you to yours</p> <p>Enterprises need to be ready to negotiate back!  But you won't be able to do this through a customer service chatbot... you will need a \"counter PAIA\" that has the objective of an optimal outcome for both you and your customer.  The enterprise won't advertise this (or at least they don't have to) but they need to make it the first thing that a PAIA interacts with.  Over time, I believe, those enterprises that do this well will build up a reputation within the PAIA networks and will find ways to capitalize on that.</p> <p>This is a world in which personal AI agents abstract away all the tedious, frustrating, onerous activities that us humans waste our time on today.... I like that world.</p>","tags":["process modeling","robotic process automation","personal AI agents","enterprise AI agents"]},{"location":"2024/12/12/are-enterprise-ai-agents-just-rpa-rebranded/#2-enterprise-ai-agents-to-increase-employee-productivity","title":"2) Enterprise AI Agents to increase employee productivity","text":"<p>Another way to think of this is \"Enterprise AI Agents to Decrease Employee Frustration\".  This is a similar line of thought as the personal AI agents above, but with more of a focus on how the employee operates within the organization.  Where RPA set out to duct tape systems together and automate \"procedural controls\" it only made the lives of some employees easier. </p> <p>An Employee AI Agent (EAIA) would share in the responsibilities of the employee.  It would have all the same access to all the same systems that the employee does and over time it would likely have the same decision making authority.  I would be surprised if enterprises aren't training these agents by analyzing employee network activity, emails etc.  Now that nearly all our conference calls are recorded, there isn't much left to complete the \"data set\" of what an employee actually does.  Don't bother fighting this, in time it will seem perfectly normal...</p>","tags":["process modeling","robotic process automation","personal AI agents","enterprise AI agents"]},{"location":"2024/12/12/are-enterprise-ai-agents-just-rpa-rebranded/#where-does-this-leave-us-humans","title":"where does this leave us humans?","text":"<p>And I guess this brings me to the point of writing all these words... bare with me for a few more, I will get there I promise... </p> <p>We are already living in two different worlds - the physical world where our meat bag bodies have been evolving for thousands of years and the digital world which has unfolded in the last few decades.  Our physical world is complicated, it is governed by rules we can't control (natural laws); it is very difficult to get to the bottom of everything - I personally am very bad at fusing atoms; and the things we value are a complex mix of emotions, experience, environment, and probably some randomness. But the digital world is more definitive, in fact we can boil it down to two precise elements: 1 and 0 (at least until quantum throws a wrench into things).  At the very bottom of everything in our digital world are bits - and more importantly the combination of those bits that make data.  Data is created, exchanged, transformed, and deleted.  Humans interact with their data via computers and over the years we've built increasingly complex systems and networks to help us interact with and control our data.  In the early days system design was dominated by the business process and resource management.  Today our system design is dominated by the User Experience (UX).  Tomorrow our system design will be dominated by AI Experience (AIX).  Our PAIAs and our EAIAs will be our interface to our data.  This will have a profound impact on computer systems as we know them.  </p> <p>For starters all of the complexity we've created to optimize the UX or the business process will become irrelevant.  At first that EAIAs will take a page out of the RPA playbook and help to bridge the gap across multiple disparate systems.  But it won't belong before they will bypass the systems and just interact with the rule sets and the data.  Which brings up the second important point.</p> <p>Blockchains are good for this (come on you knew I would go there...).  We need networks that can enforce rules which are very hard to break in order to settle the state of our data.  It doesn't matter if that data is representing an asset in the real world (like nearly every other post I written here) or if it is a photograph of your dog.  Who's data it is and what can be done with it needs to be clearly defined and controlled.  We don't have this level of control often today because it's very hard, but either our PAIAs or our EAIAs will abstract the complexity and inefficiencies away from of our control systems to make this level of granular data contol common practice.  Our systems will devolve into rulesets that the AIs are asked to adhere to while trying to achieve their objectives which (hopefully) are aligned with our intent.  The AIs will take care of connecting all the dots and the humans will provide the \"bounded rationality\" and the \"lil-bit-o-randomenss\" needed to make the digital world evolve.</p>","tags":["process modeling","robotic process automation","personal AI agents","enterprise AI agents"]},{"location":"2024/12/12/are-enterprise-ai-agents-just-rpa-rebranded/#thanks-for-reading-ill-shut-up-now","title":"thanks for reading, I'll shut up now","text":"<p>Ok, that was a bit heavy and a tad dystopian, but I don't think it's a far stretch.  And at the rate we are advancing technology this could be here before we know it.  To bring this post to a conclusion and answer the question we set out to discuss.  Are Enterprise AI Agents (EAIAs) just Robotic Process Automation (RPA) rebranded?  Sure - they could be, and in many cases probably will, but not necessarily. At least I gave you a few new acronyms to include in your deck...</p> <p>If you were wondering what Zeroth Technology is doing about all this, we're working on it.  We see this push as the convergence of our three pillars - Zeroth Agents, Zeroth Attest, and Zeroth Settlement Solutions.  With Zeroth Agents we are helping enterprise prepare their internal rule sets (typically buried in procedual documentation) to help interact with both EAIAs and their customers PAIAs.  Zeroth Attest is launching the Vero device to help distinguish between a real human and an imposter.  And Zeroth Settlement Solutions is working to use blockchains to settle object state in a way that is both trusted and confidential.</p>","tags":["process modeling","robotic process automation","personal AI agents","enterprise AI agents"]},{"location":"2024/12/12/are-enterprise-ai-agents-just-rpa-rebranded/#bibliography","title":"Bibliography","text":"<p>[1] Farmer, J. D. (2023). Making Sense of Chaos: The Modeling Revolution in Economics. Little, Brown Spark.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-12-12</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 5</p> <p>File SHA-256: <code>0a50224800e9f887...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-12-13 James Canterbury title change 2024-12-12 James Canterbury added enterprise AI vs. RPA <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["process modeling","robotic process automation","personal AI agents","enterprise AI agents"]},{"location":"2024/05/10/interobjectivity-/","title":"Interobjectivity... ?","text":"<p>I love a good buzz word, especially one that sounds smart; could this be a new narrative that helps us express our intents in a digital world?  In exploring where this came from it helped me crystalize my understating of the difference between rules and laws in autonomous virtual worlds.  Even though that sounds like a stretch, this is gong to effect the way we do things in the real world sooner rather than later.</p> <p>Let me share with you a fun little rabbit hole I jumped into the other weekend around \"Interobjectivity\".  This peaked my interest after hearing the word thrown around on a couple of podcasts and me realizing that my assumed definition based on the context clues of how it was used was conflicted.  Because \"interobjective\" kind of sounds like \"interoperable\" I had assumed it meant some sort of common ground where multiple objectives could be met with minimal trade-offs.  Turns out that is not correct, but it's not completely wrong either.  Into the hole we go...</p> <p>First of all the word interobjectivity is a very rarely used outside of social sciences, and there seem to be several formal definitions of it.  From my limited research, the word first popped up in 1996 by the French philosopher Bruno Latour\u00a0 His paper \u201cOn Interobjectivity\u201d<sup>1</sup>.  Which is a fascinating foray into the mating habits of baboons and a drawn out example of him talking through a metal grate in order to buy postage stamps.  But within the 20 pages of philosophical prose there is a theorem there - humans (unlike other species) have a special kind of relationship with objects, we can use them to execute our intent even when we are not around.  Latour uses an example of a shepherd that builds a fence.  Previously the shepherd would need to constantly roam around and herd his sheep to keep them in position, now instead of the shepherd needing to interact with the sheep directly the fence does it for him and he can take a nap.   The relationship between the shepherd and the fence is interobjective, because of the fence's permanence it allows the shepherd to extend his influence on the actions of others (in this case the sheep) even while he is asleep, or (depending on how well he built the fence) long after he is gone.</p> <p>Oddly enough, though the paper is titled \"On Interobjectivity\" Latour does not actually define the term (or perhaps he did in the original French)... Later a grad student contributing to the \"Human Geography Knowledge Base\" gave interobjectivity the definition of: \"the common world of experience and meaning which is shared by groups of people\u00a0and\u00a0objects. \"<sup>2</sup></p> <p>There is a 2016 book \"On the Existance of Digital Objects\"<sup>3</sup> where Bernard Stiegler contributed a chaper \"The Time of Technical Systems\" where he uses the word interobjectivity in the context of information systems - I've ordered the book and added it to my summer reading list. The most official publicly available definition I could find was in the Encyclopedia of Critical Psychology and states that \"The concept of interobjectivity has been introduced and developed in the social sciences to account for the non-conscious engagement in the course of social interaction that occurs within a social field that is phenomenally objective for subjects and that includes interactions with objects.\" <sup>4</sup> A full write up of this definition can be found here. This probably makes sense to psychologists, but was a little murky for me.  Though it is pretty clear that the root of the word (objective) is referring to being able to sense something physical.</p> <p>Unfortunately I don't think the existing definition extends very well into how we've been trying to use the word in the crypto space - which is mostly digital.  When I think of Latour's paper the part that sticks with me was the ability to use objects to express intent and the lasting effect that it has on others to convey that intent.  The shepherd intended for the sheep to stay corralled, in his mind he made a rule stating \u201cthou sheep shall not pass\u201d and he built a fence to enforce that rule.  Therefore my layman's definition of interobjectivity would be \u201cthe way a human can use a non-human object to express their intent to other humans\".  Or given the state of technology today perhaps a more apt definition is \"the way an intelligence can use a non-intelligent object to express their intent to other intelligences\".</p> <p>This flows nicely from the definition of intersubjectivity which is a word and has been well defined and discussed in many sociology journals. The short definition is \"the intersection or relation between the cognitive perspectives of individuals\".  Interobjectivity places an object in between the individuals.</p> <p>Info</p> <p>Recently intersubjectivity made an appearance in crypto circles in EigenLayer's white paper describing the EIGEN token as an \u201cintersubjective work token\u201d basically saying that you can post some EIGEN as a bond to do some work that isn\u2019t particularly easy to measure. Later, if two or more reasonable people agree that you didn\u2019t do said work, you can lose your bond.  Thus widening the aperture of what a work token can be used for the current objective measures.</p> <p>This really has nothing to do with what we are discussing here, but it is an interesting reference.</p> <p>To stretch this into the digital we need to take the leap that objects are not just physical things.  In a pure coincidence of words, developers have been using \"object-oriented\" programming languages since the 60's for their ability to encapsulate code and extend functionality. So instead of the shepherd building a physical fence, a game developer could create an instance of an object class of fence and give it attributes such as height and length.  When a digital sheep encountered such an object it would be programmed to take some action such as turn around.  Whenever someone is playing that game the developer's intent conveys by the attributes of the object they define and how that object interacts with other objects within the game.</p> <p>But now the slope gets slippery... Intent is a difficult thing, if someone needs to interpret your intent it is by nature subjective and we are trying to build an interobjective world.  In the physical world intent often differs from the outcome, or causes unintended outcomes.  While the fence may keep the sheep in, it also might keep others out. In software development we do our best to be explicit with expressing intent in the form well defined requirements, but even this gets tricky because of unintended outcomes or the vast array of interactions with other objects.  The best way to avoid slipping down the slope is to use a language that is explicit and universal where all possible outcomes are deterministic - such as math.</p> <p>Autonomous Worlds:</p> <p>Here we are going to take a bit of a shift into digital or virtual worlds.  I'm not talking about the metaverse, I'm referring to worlds as an all encompassing container.  Back in 2022 ludens started writing about \"Autonomous Worlds\" as fully on-chain rulesets in the 0xparc.org blog and later went on to build entire frameworks and toolsets for creating virtual worlds on-chain with Lattice.  In the spirit of standing on shoulders - many of the concepts I discuss below have been more fully fleshed out by the Lattice team (I'm a big fan of their work).</p>","tags":["autonomous worlds","supply chain"]},{"location":"2024/05/10/interobjectivity-/#expressing-intent-rules-vs-laws","title":"Expressing Intent: Rules vs. Laws","text":"<p>In the physical world the transition from expressing intent to getting others to take a specific action is a matter of how well the object is designed and applied.  If the shepherd used a piece of string to corral the sheep, it is a weak form of expression, if he built a 10 foot wall it\u2019s a strong form.  But even the strongest wall can be breached if you had really determined sheep\u2026 though this is drifting away from our interobjective rant a bit, it\u2019s important to note that we don\u2019t always need a physical or digital object to express intent, we can simply create a rule and enforce it through some sort of social construct or norm.  In fact we often need to do this to account for weak forms of interobjective expression when strong forms are not practical.  The shepherd could hang a sign on the string fence to let the sheep know we don\u2019t want them to cross\u2026 and we could decide to strengthen the rule by making it a law and apply stiff penalties if violated. But alas, rules and laws are made to be broken (and most sheep can't read)...</p> <p>But not all laws can be broken\u2026 we live in a universe that is bound by physical or natural laws.  These are concise descriptions of natural phenomena that have existed long before us humans and will likely be intact long after we are gone.  Through the scientific process we have observed and tested these descriptions and to the best of our knowledge they cannot be broken.  These include constants like the speed of light, or the gravitational constant, but concise descriptions are often formulas that relate these constants to other things we can measure like \\(E=MC^2\\).  Because we don't know a reality where these constants don't exist we take them for granted and in many cases don't even recognize them - but they are always there.  The existence of natural laws shape our reality, they become our basis for establishing truth and there is an inherent belief that they will continue to operate in the same way in the future. My kid tried to ignore gravity once - that didn't end well... I doubt he's going to try that again.</p> <p>I find it interesting that in the study of physics the most concise way to describe a natural phenomena is through the use of a mathematical formula.  Math is the universal language and can be both explicit and deterministic.  There is a popular saying in crypto that \"code is law\" and as catchy as this saying is, if we want to apply it to natural laws as opposed to social norms we need to add a few conditions.</p>","tags":["autonomous worlds","supply chain"]},{"location":"2024/05/10/interobjectivity-/#establishing-natural-laws-in-an-interobjective-autonomous-world","title":"Establishing Natural Laws in an Interobjective Autonomous World","text":"<p>I was playing Minecraft with my son the other day and there were a few blocks of land suspended in the air.  It felt weird because I'm used to real-world physical laws, but that is how the developers designed the game - only certain blocks fall.  Accepting that reality is a kind of informal player contract that I agree to when playing the game.  Part of the fun is experimenting and discovering just what you can and can't do.  But it's only fun because Minecraft's physics are deterministic - once you understand the parameters they operate the same way all the time.  Sure there is randomness in how the worlds are laid out, but there is no randomness in how each block interoperates with each other.  If sometimes blocks fell up, and sometimes blocks fell down or some just randomly vanished, the game play would be completely different - it would be very frustrating...</p> <p>Important</p> <p>Interobjectivity relies on natural laws in a digital world, and those laws must be deterministic</p> <p>Deterministic natural laws set the foundation for how the digital world operates, they form the basis for that informal player contract which is elected into each time you play.  Their consistency allows players to make predictions about how their actions will effect the world and will effect other players or will allow a player to use an object to express their intent to other players.  If the world is to have some permanence (i.e., your actions effect others even when you are not playing) it is reliance on the natural laws that create an interobjective experience.</p> <p>Being deterministic also implies that things are rigid.  The art of designing a good game rests in finding the right balance between deterministic laws and rules which players opt into.</p> <p>Important</p> <p>Deterministic laws should be simple and basic - if done well they aren't even noticeable.</p> <p>What's not fun in Minecraft is when you've been grinding away for hours looking for diamonds and your kid turns on creative mode when your not around just to fills up his inventory.  The ability to turn on creative mode (unlimited inventory) or peaceful mode (you can't get killed) can add a lot to the experience, but when one or just a few people have \"God-Mode\" powers it really disincentives other players.  Just like in real life, changing the rules should not be arbitrary and requires some form of governance.</p> <p>Important</p> <p>Rules and laws are different.  Rules can be changed and require governance; laws are immutable.</p> <p>But both the law and the rules are just code running on some computer.  One cannot claim to have immutable code if they control the hardware and the technology stack - if the game is contained within a world that runs on a computer (which might be running multiple worlds), then whoever can turn off the computer has God-Mode over the universe!  Right now the majority of all digital worlds run on servers that are controlled by just a handful of enterprises.  It's easy to \"trust\" them when the stakes are low but they are most certainly looking out for their own interests (and the interests of their share holders).  Chris Dixon points this out in his fantastic book \"Read, Write, Own\". <sup>5</sup> He says the only way to prevent this is through protocol networks, or more specifically by thinking of blockchains as virtual world computers (not to be confused with individual computers running virtual worlds).  The \"protocols\" enforce the natural laws of the network across a vast array of computers (I.e., nodes in a decentralized network), it becomes very, very hard to break them.  There is no Universal-God-Mode only coordination among node operators.</p> <p>The other point that Dixon repeatedly makes (and I think is the most important aspect of blockchain in the context of interobjective autonomous worlds) is the unique ability of blockchain to make grantees about future execution of rules - only in a decentralized blockchain can you know that the laws and rules that exist today today will run the same way tomorrow - which makes them uniquely suited for enforcing natural laws.  In the context of interobjectivity this cannot be understated.  In the physical world I can build a fence to keep the sheep in, and I know that because the sheep can't defy gravity they are going to be bound by the fence.  In my digital world I need a guarantee that gravity is going to remain constant if I'm to use the same concept of an object (the fence) to control the sheep.</p> <p>And it's not just constants (which are the parameters) it's the actual execution of the math.  Different computers do math differently - mostly because of the difficulty in handling floating-point precision and various short cuts that computers can take to perform frequently used functions.  If you are looking for another rabbits hole to jump into, this old reddit thread is pretty fascinating.  Most of the time these differences are minute, but they can stack up.  Well designed blockchains execute on a wide array of hardware and software clients so there are always multiple nodes \"checking the math\".</p> <p>Interobjective Autonomous World needs 3 things:</p> <ol> <li> <p>A set of deterministic natural laws enforced via protocols</p> </li> <li> <p>A set of rules that can be opted into by players and maintained through governance</p> </li> <li> <p>Guarantees about the consistency in future execution of 1 &amp; 2</p> </li> </ol>","tags":["autonomous worlds","supply chain"]},{"location":"2024/05/10/interobjectivity-/#a-physical-world-bound-by-virtual-laws","title":"A Physical World Bound by Virtual Laws'...'","text":"<p>I usually write about supply chains and enterprise blockchain use cases.  So far this rant has been about obscure sociology papers and video games\u2026 is there a connection?</p> <p>Yeah, absolutely.  We shouldn't be surprised that gamers are leading the charge in pushing the role of networks for global coordination.  This could be another long post on it's own, but I think it comes down to the fact that we willingly suspend our concept of reality and accept the laws and rules defined by the game designers when we choose to play a game.   In the \"real-world\" game of supply chain (for example) it's not that easy.  There was no mastermind in designing global trade routes, there is no God-mode that can override the laws of nature, we are lucky if we can agree upon a governance structure for enforcing our man-made rules!  The reality is that enterprises exist in silos today and are primarily responsible for keeping their own house in order.  In some ways enterprise systems are like individual worlds that are trying to coordinate with other worlds, but there are no deterministic protocol level rules that give them a foundation to build on.  So every \"object\" they create to express their intent (a contract, a data standard, a packet of business logic) needs to be completely replicated in the other worlds.  And there is no guarantee that those other worlds will A) do it right in the first place, or B) continue to do it right in the future - so we have to jump through all kinds of hoops to \"prove\" our systems to each other.  </p> <p>supplier: \"I built a fence that keeps the sheep in.  It's 4.54 ft tall and surrounds 1.26 acres\"</p> <p>customer: \"ok, I'll build the exact same fence, but I can only go to 1 decimal place so its 4.5 ft tall and 1.3 acres\"</p> <p>supplier: \"hmm... well that's not exactly the same.  Lets just keep checking to see if the sheep can get out\"</p> <p>customer:  \"sounds good, I'll go hire an auditor\"</p> <p>Think about how different global supply chain would be if for certain business transactions the settlement was occurring in an interobjective autonomous world?  The areas of friction could be eliminated because the double checking that needed to occur between systems (or that requires an intermediary) could be eliminated.</p> <p>Just to be clear - I am not proposing a global platform (just the thought of that gives me an upset stomach).  For all the reasons stated above this does not work if there is a God-Mode.  What we is a way to break down business transactions into a series of deterministic natural laws and use a blockchain (or multiple blockchains) to execute those laws the same way for everyone for all time.  Participants can opt-in or opt-out for higher level \"rules\" which help manage their specific supply chains, but they can't break the laws.</p> <p>Tip</p> <p>Example Supply Chain Law: If I have custody over 10 units of inventory I can only transfer custody of 10, or fewer, units of inventory. Example Supply Chain Rule: If I want to destroy a unit I must have both custody and ownership of the unit.</p> <p>Both rules and laws are deterministic and can be described mathematically.  This is the subject of our supply chain work over at Zeroth.Technology.  We are working on a method to allow for confidential settlements with embedded rights for physical products leveraging protocol networks.</p>","tags":["autonomous worlds","supply chain"]},{"location":"2024/05/10/interobjectivity-/#tying-it-all-back-together","title":"Tying It All Back Together","text":"<p>The notion of interobjectivity is exciting in the context of autonomous worlds, but it requires some work. In the physical world we use objects to express our intent with various degrees of permanence, but even these need the support of societal norms and governance to be effective in the long run.  Applying this to the digital realm in a trustable, sustainable way challenges the underpinnings of existing networks and systems but is completely doable with well designed  blockchains that can execute deterministic laws at the protocol level and allow for better governance around participation rules..  </p>","tags":["autonomous worlds","supply chain"]},{"location":"2024/05/10/interobjectivity-/#references","title":"References","text":"\ud83d\udcdd Content Provenance      <p>Created: 2024-06-15</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 7</p> <p>File SHA-256: <code>78d39d14b0135524...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-06-21 James Canterbury Update interobjectivity.md 2024-06-15 James Canterbury added a bunch of old blogs... <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p> <ol> <li> <p>Latour, B. (1996), On interobjectivity. In: Mind, culture and activity. Volume 3, number 4. Retrieved from Scribd.com, Friday May 31st 2024. https://www.scribd.com/document/277023110/Bruno-Latour-On-interobjectivity-Mind-Culture-and-Activity-1996 \u21a9</p> </li> <li> <p>Hui, Y. (2016). The Time of Technical Systems. In On the Existence of Digital Objects (pp. 151-186). University of Minnesota Press. https://doi.org/10.5749/minnesota/9780816698905.003.0005\u00a0\u21a9</p> </li> <li> <p>Human Geography Knowledge Base. (2012). Interobjectivity. Retrieved June 10, 2024, from https://geography.ruhosting.nl/geography/index.php?title=Interobjectivity\u00a0\u21a9</p> </li> <li> <p>Sammut, G., Moghaddam, F. (2014). Interobjectivity. In: Teo, T. (eds) Encyclopedia of Critical Psychology. Springer, New York, NY. https://doi.org/10.1007/978-1-4614-5583-7_158 \u21a9</p> </li> <li> <p>Dixon, Chris.\u00a0Read Write\u00a0Own: Building\u00a0the Next Era\u00a0of the Internet_. Random House\u00a0Publishing Group, 2024.\u00a0\u21a9</p> </li> </ol>","tags":["autonomous worlds","supply chain"]},{"location":"2025/08/01/llms-dont-have-a-memory-problem--they-have-a-provenance-problem/","title":"LLMs Don\u2019t Have a Memory Problem \u2014 They Have a Provenance Problem","text":"<p>The 2025 Context Engineering Survey, which reviewed more than 200 research papers and enterprise pilots, cautions: \u201cSimply enlarging an LLM\u2019s context window does not guarantee reliable attribution or auditability; we still need explanation systems, audit mechanisms, and governance structures.\u201d (Context Engineering Survey 2025 \u00a74). Put differently, the problem isn\u2019t raw memory capacity \u2014 it\u2019s the provenance of the information we cram inside.  This is exactly the rationale we followed when designing Context Units for our Pyrana platform.</p>","tags":["ai research","context engineering","llm","enterprise ai","provenance"]},{"location":"2025/08/01/llms-dont-have-a-memory-problem--they-have-a-provenance-problem/#the-new-field-of-context-engineering-and-whats-missing","title":"The New Field of Context Engineering (and What\u2019s Missing)","text":"<p>Reviewing the survey hardened my stance that there are 'those building the models' and 'those using the models'.  for the past year or two the users were focused on prompt engineering - how do we ask a model to do something in a way that gets the results I want.  We quickly realized that this is not a one-shot experience and users began iterating with the model, engaging with it (or arguing with it as my colleagues and I like to say) and this little back and forth started to max out the context window pretty quick.  So the builders came up with novel ways (mostly short cuts) to cram more content into the context window - hence the term context engineering.  These efforts include developing longer context windows, external memory buffers, structured prompts, retrieval augmentation \u2014 the list goes on. The current thinking is simple: feed our models more information so they remember more. Advanced Memory Systems now let LLMs retain persistent information across chats, transforming them from stateless bots into agents with (simulated) long\u2011term memory. And yes, these systems can be impressive \u2014 they page knowledge in and out of an LLM much like a computer\u2019s virtual memory, helping the model carry over facts from one query to the next.</p> <p>So, more memory should solve everything, right? Not exactly. The survey uncovered a stubborn gap: even as we stuff models with more context, they struggle to generate equally sophisticated long outputs. In other words, cramming more into an LLM\u2019s head doesn\u2019t guarantee it can use that knowledge effectively. And from a practical perspective, simply increasing the raw amount of context has diminishing returns. Why? Because the real world isn\u2019t just about hoarding facts; it\u2019s about trusting them.  </p> <p>In an enterprise setting, it\u2019s not enough for an AI to recall a policy document from six months ago \u2014 we need to know which version of the policy it used, who approved that content, and whether the model\u2019s answer can be traced back to a reliable source. That\u2019s provenance.</p> <p>If you\u2019ve ever debugged an AI\u2019s output and wished you could see an itemized receipt of \u201chere\u2019s exactly what I was fed and where it came from,\u201d you\u2019re feeling the provenance problem. Sure, memory bandwidth is great, but if an AI\u2019s \u201cmemories\u201d are a jumbled scrapbook with no labels, we\u2019re just moving the mess from one place to another.</p> <p>The missing piece in this context engineering is governance. How do we control and track what goes into these ever\u2011expanding context windows? The 2025 survey authors stress the need for \u201cexplanation systems, audit mechanisms, and governance structures\u201d to keep our context engineering on the rails. In plain terms: we don\u2019t just need bigger memory; we need better memory management. Enter Context Units (CxUs).</p>","tags":["ai research","context engineering","llm","enterprise ai","provenance"]},{"location":"2025/08/01/llms-dont-have-a-memory-problem--they-have-a-provenance-problem/#context-units-packaging-knowledge-with-provenance","title":"Context Units: Packaging Knowledge with Provenance","text":"<p>I\u2019ve written before about Context Units (CxUs) as the \u201catomic fact cards\u201d of a knowledge base. Think of a CxU as a bite\u2011sized, immutable packet that captures exactly one idea or fact you care about \u2014 plus the evidence and metadata that back it up. Each CxU is self\u2011contained (claim and context travel together) and hash\u2011addressed, meaning its unique ID is a cryptographic hash of its content. Why go to those lengths? Because it gives each unit a stable identity and tamper\u2011evidence. If anyone changes a CxU\u2019s content by even one character, its hash (and ID) changes too. No silent edits, no sneaky tweaks. In other words, truth comes with a checksum.</p> <p>Each CxU carries metadata about its origin and status: who created it, when, what version, and even links to source documents or other CxUs it builds on. This built\u2011in provenance means you don\u2019t just have a blob of text; you have a verifiable, structured packet of knowledge with an audit trail. It\u2019s like the difference between a paragraph on a blog site versus a notarized fact on an index card \u2014 one is just text, the other is text with a certificate attached. (except for our blog site since we retain a git commit for every post...)</p> <p>Concrete example Suppose we have a regulatory guideline: \u201cIn the EU, a shipped product must include a Declaration of Conformity before customs clearance.\u201d In a CxU form, this would be stored as:</p> <ul> <li>Claim: \u201cIn the EU, a shipped product must include a Declaration of Conformity before customs clearance.\u201d  </li> <li>Supporting Context: \u201cDefined in EU Regulation\u00a0765/2008, Article\u00a030, to ensure product safety and compliance.\u201d  </li> <li>References: EUR\u2011Lex\u00a0765/2008\u00a0Article\u00a030.  </li> <li>Metadata: tagged as <code>regulation</code>, <code>EU</code>, <code>compliance</code>, with a version number and timestamp.</li> </ul> <p>Because this CxU is hashed and immutable, if the EU updates their regulation next year, that update becomes a new CxU with a different ID (and a pointer back to the prior version). The old fact doesn\u2019t silently morph; it\u2019s versioned like code in Git. We can always trace which version of the rule was applied in any AI interaction. Over time you get a version chain \u2013 essentially a linked list of how that fact evolved. If an AI answer was based on the\u00a02024 rule and not the\u00a02025 amendment, you\u2019ll know, and you can decide if that\u2019s a problem. This is context governance in action.</p> <p>Crucially, CxUs also let us compress knowledge without losing trust. A 3\u202f000\u2011word policy document might boil down to, say, ten well\u2011crafted CxUs capturing its key points. You\u2019ve just shrunk the context by an order of magnitude, but each CxU still carries a link back to the relevant source text. You gain knowledge density (more signal per token) without sacrificing provenance. The model\u2019s prompt stays focused and efficient, and you maintain an audit trail for every snippet of info included. More context isn\u2019t always better; better\u2011curated context is better.</p> <p>CxUs aren\u2019t a silver bullet. They\u2019re a design tool \u2014 a discipline that enforces trustable context.</p>","tags":["ai research","context engineering","llm","enterprise ai","provenance"]},{"location":"2025/08/01/llms-dont-have-a-memory-problem--they-have-a-provenance-problem/#the-cxu-workflow-in-pyrana","title":"The CxU Workflow in Pyrana","text":"<ol> <li>Author / Extract\u00a0\u2192 draft CxUs from docs, meetings, or code.  </li> <li>Validate\u00a0\u2192 schema + rule checks (e.g., conflict detection, required fields).  </li> <li>Store\u00a0\u2192 content\u2011addressable, deduplicated, immutable.  </li> <li>Retrieve\u00a0\u2192 query by tags, semantic similarity, or explicit IDs.  </li> <li>Pack\u00a0\u2192 inject selected CxUs (or just their claims+IDs) into the LLM prompt.  </li> <li>Audit\u00a0\u2192 log every <code>cxu_id</code> used so you can reconstruct the chain of thought.</li> </ol> <p>Every step is deterministic and inspectable \u2014 a far cry from dumping a PDF into a prompt and hoping the model pays attention to the right parts.</p>","tags":["ai research","context engineering","llm","enterprise ai","provenance"]},{"location":"2025/08/01/llms-dont-have-a-memory-problem--they-have-a-provenance-problem/#from-lab-to-enterprise-cxus-in-action","title":"From Lab to Enterprise: CxUs in Action","text":"<p>Let\u2019s make this concrete. Imagine an AI assistant for supply\u2011chain compliance. Instead of stuffing it with every regulation PDF, we curate a library of compliance CxUs up front. When a user asks, \u201cCan I ship this product to Germany?\u201d, our retrieval picks five or six relevant CxUs \u2014 the EU declaration rule, a German customs clause, and an internal carrier policy. Those units go into the prompt, and the model replies with an answer that cites them:</p> <p>\u201cYes, you can ship to Germany, but you must include a Declaration of Conformity per EU Regulation\u00a0765/2008\u00a0Article\u00a030 and use a certified carrier per Internal Policy\u00a0XYZ.\u201d</p> <p>Because we logged the CxU IDs, a compliance officer can audit exactly which facts were used. The model isn\u2019t just spouting off \u2014 it\u2019s effectively citing its chain of thought.</p> <p>This scale\u2011up beautifully in agent\u2011oriented architectures. Multiple agents can pass CxUs (or references) instead of free\u2011form text, preserving traceability across a complex workflow. Think of it as end\u2011to\u2011end provenance plumbing for AI.</p>","tags":["ai research","context engineering","llm","enterprise ai","provenance"]},{"location":"2025/08/01/llms-dont-have-a-memory-problem--they-have-a-provenance-problem/#implications-and-open-questions-for-researchers","title":"Implications and Open Questions for Researchers","text":"<p>Is this really new? In one sense, no \u2014 we\u2019re merging ideas from knowledge graphs, content\u2011addressable storage, and version control. But the integration is timely. It answers the survey\u2019s call for explanation systems and audit mechanisms in context engineering.</p> <p>Research frontiers</p> <ul> <li>Memory Architectures: Can long\u2011term LLM memories store provenance\u2011rich items (CxUs or indices) rather than fuzzy text blobs?  </li> <li>Selective Forgetting: With provenance metadata, an agent could decide what to keep or purge intelligently.  </li> <li>Standardization: Could we define a cross\u2011org CxU schema so companies share knowledge like open\u2011source libraries?  </li> <li>Human\u2011in\u2011the\u2011Loop: What interfaces make authoring and maintaining CxUs painless enough that teams actually do it?</li> </ul>","tags":["ai research","context engineering","llm","enterprise ai","provenance"]},{"location":"2025/08/01/llms-dont-have-a-memory-problem--they-have-a-provenance-problem/#conclusion-toward-trustworthy-context-engineering","title":"Conclusion: Toward Trustworthy Context Engineering","text":"<p>More memory isn\u2019t a cure if your memories are a mess. Provenance, versioning, traceability \u2014 these boring, hard\u2011to\u2011implement facets are what make the difference between a demo and a dependable tool. Context Units bake those facets in from first principles. They force us to decide what we actually know and where it came from before we ever prompt the AI.</p> <p>Key takeaway: Better context beats more context. LLMs don\u2019t need an infinite scrapbook; they need a curated set of facts that carry their own receipts. Focus on provenance, not just persistence, and you\u2019ll end up with AI systems you can trust \u2014 and maybe even take to an audit committee without breaking a sweat.</p> <p>Tried structured context or CxUs in your own projects? I\u2019d love to hear your war stories or questions. Drop them in the comments!</p>        \ud83d\udcdd Content Provenance      <p>Created: 2025-08-03</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>1004a421409b3abe...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2025-08-03 James Canterbury added LLM Provenance article <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["ai research","context engineering","llm","enterprise ai","provenance"]},{"location":"2023/08/20/product-oriented-data-structure/","title":"Product-Oriented Data Structure","text":"<p>Where is most of the effort going to be for enterprises to get on board with product tokenization for track &amp; trace?\u00a0 It\u2019s not the blockchain tech - while that part is cool, if we go with the assumption that any decentralized system worth building on needs to be open source and multi-purpose, it\u2019s not going to be the enterprise\u2019s role to develop or operate that network.\u00a0 It is however the enterprise\u2019s role to lay out clear business requirements that can be used to build modular software (building blocks) to achieve lofty traceability-related use cases.</p> <p>No, the hard work for the enterprise is going to be in organizing their data and mapping their supply chain events into universally understood buckets.\u00a0 The good news here is that this isn\u2019t a new issue, standards for communicating between supply chain systems have been around for decades, the EPCIS standard and the acceding Core Business Vocabulary by GS1 is probably the most widely leveraged and for the most parts fits the needs.\u00a0 My personal experience having implemented blockchain for supply chain solutions for over 5 years now, is that 80% of the effort is in extracting, transforming and loading (ETL) data on an on-going basis. The \u2018on-going\u2019 part also implies that enterprises understand the event triggers that start the ETL process that would feed the supply chain data into such a solution.</p> <p>You might ask: Why is this so hard?\u00a0 Haven\u2019t enterprises been investing in better data structure for years?\u00a0 What about my $20M data lake?\u00a0\u00a0</p> <p>My reply: Have you ever tried to pour a cup of water into a lake and then later scoop out that same cup of water?\u00a0 Modern data architectures are designed for aggregation, not for granularity.</p> <p>I blame it on the ERPs.\u00a0 Enterprises have done everything they can to optimize their business processes, which often starts with the introduction of Enterprise Resource Planning systems (ERPs) and this focus on efficient process has generated a \u2018process-oriented data structure\u2019. It\u2019s easy to see why. If you map out your manufacturing and distribution processes for just one plant, you can probably draw some pretty clear demarcations around which system is governing which part of the process - you likely have interfaces where one system hands off to the next.\u00a0 If you look at how the data is structured within that process, it is generally geared towards records and logs - the data describes what happened over this time period within this process.</p> <p>If you did the same exercise for multiple plants and distribution processes for the same time period and stacked the maps on top of each other (in a 1980s overhead projector style), you might even see how well they line up - but each individual flow is not necessarily connected to each other - you can actually see the data silos forming.\u00a0 This is ultimately the result of process-oriented data structures.\u00a0 Breaking it down properly to a more granular level is a complex matter of mapping and standardization.\u00a0 An easier solution is to just dump all the data into one bucket - or if you have enough of it, into one large lake\u2026\u00a0 But now extend that flow out past the manufacturing and distribution that you control - when the data \u201cbelongs\u201d to an entirely different set of systems (I.e., that of your customer, or your customers\u2019 customer) the idea of capturing all of their data in your data lake is untenable.</p>","tags":["tokenization","supply chain"]},{"location":"2023/08/20/product-oriented-data-structure/#enter-the-product-world","title":"Enter the product world","text":"<p>In the real-world products behave differently than processes in the sense that if you follow a product \u201ccradle to grave\u201d you cross over several (sometimes many) different processes, and very rarely do any two products follow the same path.\u00a0 I have written a lot about product tokens and the role that public blockchains will play in detangling this mess - that is a more in-depth discussion.\u00a0 For the purposes of this post, I just want to focus on the product token as the common thread that connects data across an ecosystem as the product moves through its lifecycle.\u00a0 If we wanted to be a little technical, you could think of the product token as the digital ID which represents a single product, and is the index against which all various databases containing information about that product can be queried.\u00a0 Keeping consistent with our GS1 example above it\u2019s easy to picture the product token ID as a Serialized Global Trade Identification Number (SGTIN).</p> <p>Of course, it\u2019s not as easy as just reindexing databases (as if that were even feasible); we need a mechanism to serve up data about any given product token from any database where it is stored and because this is trustless blockchain land, we want a mechanism to prove that the data has remained unchanged since the time the supply chain event occurred.\u00a0 Let\u2019s unpack that a bit\u2026</p> <p>For starters we will refer to product data as metadata - and this is simply to imply that there is a dataset separate of any other information that enshrines everything a process-oriented data structure knows about a given product.\u00a0 Depending on your use case there are endless \u201cstandards\u201d of what data should be included and what format it needs to take., IMHO this will never end, there will always be some new piece of information that is critical to know about a product so that something different can happen somewhere way down the supply chain.\u00a0 The point is metadata will not be statically defined, and enterprises will need data governance for product data in and of itself.</p> <p>The second point is product metadata will not all live in one place. - As a product goes from production to consumption it crosses many governing systems, each of those systems may add some information to the product metadata, but it\u2019s very unlikely that they will ever write that data back to the initial repository.\u00a0 I like to picture this as a trail of breadcrumbs that a product leaves, but the product itself is the key to linking all the breadcrumbs.</p> <p>It\u2019s worth noting that I do believe decentralized data storage could be a very good answer to keeping product metadata aggregated and accessible when needed - but I don\u2019t think we can assume everyone will be willing to do that. - it should always be the right of the data owner to deny access to the product data, and while there may be consequences to enacting that right, it should be available\u2026</p> <p>Whether each ecosystem partner maintains their own product metadata or you choose to use a decentralized solution, the enterprise will still need to produce the product metadata in the first place.\u00a0 It has become common place for the metadata to be stored in a .json blob.\u00a0 For those non-technical readers, this is just a structured data set consisting of a key value pair.\u00a0 The key being the name of the data element (think field name or column header) and the value being the actual data.\u00a0 Conveying both the key and the value every time may seem like additional overhead, but in the long run it solves sooo many issues by letting the metadata live independently of a fixed the database schema.</p> <p>The other nice thing about using a .json blob is that we can create a hash of it (a one-way encryption, or a digital fingerprint) that can later be used to prove the metadata remained unchanged since the time it was posted.\u00a0 I can also have a portion of the .json blob encrypted using a specific public key (say that of a government regulator) who could at a later date decrypt that section and see some information about the product that is available only to them, but have the confidence that it was there all along and has remained unaltered.</p> <p>Just to reiterate the point.\u00a0 A Product-Oriented Data Structure consists of a universal product identifier for a specific unique product (or batch of identical products) - this is the token ID - and a series of .json blobs containing data specific to that product and likely stored in several different databases managed by different members of the ecosystem.\u00a0 Leveraging the product token as the access mechanism for that data is a process that I call Token Claim Derived Authority (TCDA) which you can read all about in some of my other posts. </p>","tags":["tokenization","supply chain"]},{"location":"2023/08/20/product-oriented-data-structure/#so-what-does-this-mean-for-the-enterprise","title":"So what does this mean for the enterprise?","text":"<p>Over time I\u2019m hopeful that ERPs will begin making it easy to generate a product-oriented data structure for individual products (some claim to do this today, but I think there is still work to do).\u00a0 In the near-term this is the world of Extract Transform &amp; Load (ETL) scripts, and these can be very tricky\u2026. If you happen to be doing a supply chain transformation, and even if you don\u2019t buy into any of the product tokenization stuff (yet) it still cannot hurt you to develop the product-oriented data structure and the governance processes needed to support it.\u00a0 Worst case scenario is you begin using this method to archive your product data or transfer it to a buyer when you sell off the part of your business responsible for that product. \u00a0 Best case scenario, is this becomes second nature to how you collect and store data, tying the data integrity of the product metadata directly to the physical product itself - in this way you begin to limit your liability of providing data on behalf of others in the ecosystem.</p> A short tangent\u2026 <p>There is an interesting aspect that comes up when we assume that not everyone will play nicely\u2026. Let\u2019s look at a couple of cases:</p> <ol> <li> <p>You buy materials from a supplier, but they don\u2019t provide you product metadata, or the metadata they do provide is incomplete (I.e., you trust what they give you, but they don\u2019t give you everything you need for your use case).\u00a0 Here, you would need to fill in the gaps. We already do this all the time today, but we tend to do it at an aggregate-level, and we do it infrequently.\u00a0 I would propose that enterprises develop a product data profile of what they would expect to receive and in which format (not a bad idea to share this with the supplier\u2026).\u00a0 Once enough data has been collected from different suppliers, then you can develop estimation formulas (this is one possible benefit of AI) that fill in the gaps for each new product you receive.\u00a0 It\u2019s really important in this scenario that those estimated data elements be flagged as such.</p> </li> <li> <p>You buy materials from a supplier, but they include fake data in their product metadata.\u00a0 Let\u2019s assume they do this maliciously because accidental mistakes should be easier to catch.\u00a0 This is where the blockchain can hold someone accountable - if the supplier signs that metadata they are in effect attesting to its accuracy.\u00a0 You as the buyer can definitively point to the source of the data as not being your own (let the legal updates to your purchasing agreements ensue\u2026).\u00a0 Further, if you have the estimation models from point 1 you can scan the metadata from each incoming product token and look for any outliers (this is why the accidents will be easier to catch).\u00a0 If over time things look too good to be true, they probably are\u2026. To that point it\u2019s not a bad idea to put something dynamic in the metadata so that your can detect copy/paste records.</p> </li> </ol> <p>Special thanks to my colleague Akhil Patil for some constructive feedback and many grammatical corrections  </p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-06-15</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>62ac47273170f612...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-06-15 James Canterbury added a bunch of old blogs... <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["tokenization","supply chain"]},{"location":"2023/10/18/product-token-metadata-profiling/","title":"Product Token Metadata Profiling","text":"<p>Often the topic of AI regarding blockchain comes up, often the topic is misguided\u2026</p> <p>A blockchain is not a learning system, it is not intelligent (smart contracts aren\u2019t even that smart\u2026).  It\u2019s more akin to a data historian, a network that maintains a state, and a record of all previous states.  It does not adapt on its own or self optimize - if it did it would undermine the trusted execution.</p> <p>There are a couple of places where AI and blockchain do meet, I\u2019m sure I\u2019m missing a few here, but these are top-of-mind:</p> <p>1) Attribution: as far as we know today the robots are not smart enough to crack our private key encryption and sign fraudulent transactions - so taking a piece of data and notarizing it on chain is a good way of attributing that data to a source.  If we also mix in a verified credential, the wallet that signed the data can carry some certification or reputation.  When training machine learning models this level of attribution will be increasingly important to avoid bias or to have a \u201cprovably legitimate\u201d training data set. 2) Data profiling:  personally I think this one is much more interesting.  We are envisioning a world where product tokens are accompanied by multiple .json blobs of data specific to that token.  The rest of this post builds off of this idea.</p> <p>If I am manufacturing a product - say a bottle of water - it\u2019s likely that I will have several different sets of metadata about that product each for different use cases.  For example I may have metadata about the source of the water.  I may have metadata about the global trade information. I may have metadata about the carbon footprint (though that should be captured in an allocated carbon token), I might even have metadata on the label or instructions for use (imagine that\u2026 how to use this water\u2026).</p> <p>Even though all of this metadata is originating from the manufacturer it\u2019s likely that it will be in different .json blob.  The schema of the .json will typically be determined by the use case, common schemas will be needed for interoperability.  If I want to do a source of origin use case for all bottled water, I will need to get some alignment on what data is required for source of origin and in what format am I expecting it.  We cannot fool ourselves that there will ever be a universal data standard for anything, try as we might.</p> <p>This is where the Machine Learning comes into play.  For any given use case there are a handful of data standards defined by one or more standards bodies.  Those templates can be fed to the algorithm.  Then for each use case specific metadata blob signed to a token that you have a claim on (read that sentence carefully) the algorithm can determine how closely the metadata fits into a known standard. If we switch from ML to AI the algorithm might even be able to do the mapping and fill in any gaps.  At a minimum it should work to catch any abnormalities, at best it should give you real-time feedback on how well your metadata tracks with similar metadata sets.</p> <p>There are two places that you can run this particular scan - the first being prior to signing the metadata, in which case you can include some level of certification into your signature that the metadata meets some set of requirements or standard (you could even have an audit firm do this for you #newbusinessopportuny).  Or you could profile the data after it has been posted, this is the more likely approach when you are receiving a product token that already has metadata signed to it by another party and you want some level of verification as to the legitimacy of the data; remember that signed data is always attributable back to the signing address, but the the actual data itself can be what ever the signer wants it to be (though once it is sighed we can always tell if it is changed after the fact.)</p> <p>One last rabbit's hole to jump down here is the possibility of using a zero knowledge proof to profile the metadata before signature - while this is a really intriguing concept it will carry a certain amount of overhead since generating a proof is computationally heavy, and it may provide limited benefit since the metadata is maintained off chain. But the basic premise goes:</p> <p>1) write a zk circuit that defines the metadata schema including the field names and data types. 2) add any additional data logic rules that might apply on a per element basis (I.e., if a quantity is entered a UOM is required, or only values within a range) 3) Sign the metadata, and ost the roof on-chain making it easy for a third party to verify without actually looking at the data. An alternative to this is to simply register a schema via EAS and reference that you leveraged that schema in your metadata, though here you loose the data verification checks.  Currently attesting under EAS exposes the data but perhaps doing that under privacy, or in conjunction with a 3rd party will be in the future.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-06-15</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>fc055712c350678a...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-06-15 James Canterbury added a bunch of old blogs... <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["tokenization","supply chain","machine learning"]},{"location":"2023/06/15/wallets-vs-token-metadata/","title":"Wallets vs. Token Metadata","text":"<p>With product tokens we generate a bread crumb trail of metadata that is specific to a given product.  In combination with the minting, transferring, and burning of claims on that token the metadata rounds out the full record of the products lineage and data.</p> <p>The way it works is an individual (a person or an entities) who has a claim on a token (I.e., the token is currently mapped to their wallet address in the latest state of the network) can post a transaction referencing the token id and typically including a CID or some other pointer in the transactions URL.  The CID is a hash of the content of the metadata set and (if using IPFS) is also used to locate the metadata.  Posting the hash on-chain is one way that we later prove the data was not changed.</p> <p>Currently we think of metadata sets by use case, for example - I want to produce a \u201cdigital product passport\u201d so i need the metadata to contain certain data elements required within the DPP.  If I want to use the token as collateral for a loan I need to include certain metadata that details the attributes necessary to assess the value of the asset. There may even be several different metadata sets signed by the by the same wallet to against the same token at the same time to support multiple use cases.</p> <p>Question</p> <p>Maybe we need a universal identifier at the top of a metadata set indicating what schema it is following in support of which use case - kind of like a multihash</p> <p>But sometimes we need to return information about the wallet that is holding the product token, and not just the product token.  For example if I am a manufacturer minting tokens for each product I produce and my wallet is registered to the physical establishment of my manufacturing plant.  There may be additional information about that establishment that I want refrenceable by each token it produces - such as address or facility name.  I may even hold a credential in my wallet stating that this establishment of licensed to produce this product for a given time period.</p> <p>In these cases it would not make sense to repost wallet level metadata each time a new set of product level metadata is signed. It would be better to simply be able to retrieve the wallet level metadata when querying the product token.  So the way this would work for say a digital product passport is that the end consumer would use TCDA to access the metadata about a given token for a given transaction.  It would also look to see if there were wallet level metadata that was applicable at the time of the transaction and return that as well.  </p> <p>Of course this adds a layer that f complexity in that the wallet level data is relevant to product level data only at a specific point in time.  If the wallet level data is updated there needs to be some version control to roll it back.</p> <p>But this approach unlocks another capability through the use of verified credentials.  We want the ability to have a VC issued to a wallet by a trusted institution that qualifies the wallet to do something.  This implies that when an end consumer is accessing metadata using TCDA they would be able to return the VC data as well - so having wallet level data (maybe held on a VC of sorts) is not really any additional work and carries with it the efficiency on only posting wallet level data once.</p> <p>Can we place a soul bound nft in a wallet that has establishment level  - could this be a credential?  This would remove a ton of duplicate metadata from the token.  If you change wallets later the old tokens would be mapped to the old wallet. Data availability might be an issue.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-06-15</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>1e7fda6604d977b7...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-06-15 James Canterbury added a bunch of old blogs... <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["tokenization","supply chain"]},{"location":"2025/09/18/friction-makes-smoke-and-where-theres-smoke/","title":"Friction Makes Smoke, and Where There's Smoke...","text":"<p>What seems like eons ago, back in March 2025 I wrote a probably-too-long paper about the nature of establishing relationships between humans over digital channels. Digital Identity Verification was an attempt at a framework to categorize how trust is formed between people \u2014 and then use that framework to systematically explain how bad actors manipulate that trust to perpetuate identity fraud.</p> <p>I still think the paper is accurate, though I missed a critical point. I had positioned my argument around people establishing trust with other people via technology. What I overlooked is that the majority of technical approaches today focus on establishing trust between devices and/or software and, by extension, the people using them. These approaches often assume that the identity of the person becomes inherent to the device.</p> <p>It is this critical gap \u2014 between human and technology \u2014 that we are looking to bridge with Vero. Building tools to facilitate real-time peer-to-peer authentication inherently creates friction in the process. But our friction is intentional: it\u2019s a demonstration of trust, a clear signal of intent. And that signal is the smoke. In our case, if you see the smoke, you can be confident there is no fire.</p>","tags":["digital-identity","trust","authentication","vero","blockchain","security"]},{"location":"2025/09/18/friction-makes-smoke-and-where-theres-smoke/#technology-trust-and-inevitability","title":"Technology, Trust, and Inevitability","text":"<p>Deepfakes have rung the alarm bell (it may not be blaring yet, but it has been rung) and are beginning to cause healthy skepticism in technology trust. Yet time and again it\u2019s proven \u2014 and I am a prime example \u2014 that even if people don\u2019t completely \u201ctrust\u201d the technology, they still lean into it.</p> <p>Take SMS-based two-factor authentication. It\u2019s well known to be vulnerable to SIM-swap attacks and interception, yet it remains the default option offered by banks, social media platforms, and even government systems. And despite knowing it\u2019s not the strongest protection, most people (myself included) continue to use it. We lean on the herd mentality \u2014 if a big platform says \u201cthis is secure enough,\u201d and it comes with a little green checkmark, we assume we\u2019re protected.</p> <p>Personally, I choose to think we are protected \u2014 having spent 20 years at an audit and accounting firm, I\u2019ve grown to appreciate the rigor large enterprises put into securing their tech. They do the best that can be done. But as a cryptographer friend of mine likes to say, \u201cgiven enough time and money, everything is breakable.\u201d</p> <p>Even if the technology were completely secure, social hacks would still exist. Con men thrived long before computers. When it comes to establishing trust between people, there is no single solution. We need a tool belt: instruments for different situations, integrations across forms of identity and reputation, and the ability to ratchet up security when something feels wrong.</p> <p>Preventing identity fraud isn\u2019t just about proving who you are \u2014 it\u2019s about quickly establishing trust. If done right, the two are inseparable. Trust is king. As I tell my kids: trust is like a sandcastle \u2014 it takes effort and time to build, but it can be wiped out very fast.</p> <p>From a digital perspective, identities can be granted and revoked. Trust, however, requires a track record and reputation. The challenge is making those elements usable for real-time verification. Because trust takes work, the real question is: how much work does it take?  Sometimes it doesn't take any work, but rather just your willingness to do it.</p>","tags":["digital-identity","trust","authentication","vero","blockchain","security"]},{"location":"2025/09/18/friction-makes-smoke-and-where-theres-smoke/#the-balance-of-friction","title":"The Balance of Friction","text":"<p>\u201cFrictionless\u201d was always one of my favorite consulting buzzwords. It implies a magic wand that takes away everything annoying about a process. But in reality, processes are rarely frictionless \u2014 at best we just shift the friction to a less painful place (or make it someone else\u2019s problem). And sometimes, introducing friction is the whole point. I knew several CFOs who held the opinion: nothing curbs spending better than a clunky procurement process.</p> <p>The key is applying friction on a graduated scale \u2014 with options. On a perilous slope, you might want more grip the further and faster you slide. Sometimes you need a handhold, sometimes an ice axe. Different situations call for different tools.</p> <p>So when people say Vero introduces friction to the authentication process, I agree. Nobody wants to flash a blinking light on their face while staring into a webcam. But those who are willing to do it are making a statement: \u201cI want to prove you can trust me.\u201d That willingness itself is a demonstration of intent.</p>","tags":["digital-identity","trust","authentication","vero","blockchain","security"]},{"location":"2025/09/18/friction-makes-smoke-and-where-theres-smoke/#behavioral-change-and-selective-friction","title":"Behavioral Change and Selective Friction","text":"<p>We also hear: \u201cVero requires a behavioral shift, and changing behavior is impossible.\u201d I agree \u2014 to an extent. If you expect people to change just because it\u2019s better for the system, good luck. If the change doesn\u2019t have an immediate, meaningful payoff for the individual, it won\u2019t happen either.</p> <p>But when people understand that trust requires doing something extra \u2014 and that the extra step only comes into play when it matters \u2014 that\u2019s a manageable level of friction. Better yet, if that extra step is tangible and immediately recognized by others as a signal of trust, adoption becomes easier.</p> <p>As scams grow more common and the fear of being defrauded rises, resistance to that friction will fall; and so people need the tools to be available.</p> <p>That\u2019s why Vero is designed to be optional. You can be in the middle of a conference call, notice something feels off, and request verification. Or you can use it before sharing sensitive information. Much like selective disclosure, Vero works best when users choose it \u2014 when it becomes their tool to show their intent.  The objective is to have the option to be available and to have users aware of its existence, which is why we are planning to build it into existing technology stacks.  Of course there would be nothing preventing users or institutions from requiring its use in certain circumstances, which supports our licensing strategy.</p>","tags":["digital-identity","trust","authentication","vero","blockchain","security"]},{"location":"2025/09/18/friction-makes-smoke-and-where-theres-smoke/#layers-of-proof","title":"Layers of Proof","text":"<p>Another common push back is: \u201cIf I can prove my identity via a Temporal Identity Proof, why do I need to blink a light on my face?\u201d</p> <p>Answer: you don\u2019t. Vero is a layered cake \u2014 you can consume as much as you want. The one-time biometric (the blinking light) is the icing.  While it may be the most novel part of our design it is not something we intend people to do every time they interact.</p> <p>This perspective also raises the point that the Temporal Identity Proof (TIP) and its real-time verification is valuable on its own.  To understand why, you need to ground yourself in one of our first principles-Vero does not seek to own your identity.  We think there are some great (and many not-so-great) identity solutions out there.  We also think the identity space will continue to evolve and that decentralized technologies like DIDs and ZK Passports are the way of the future; but these are generally not in our hands right now and there is still a very long learning curve before they are mainstream.  In the meantime, TIPs let you use the credentials and reputations you already control.</p> <p>And remember \u2014 reputation often matters more than formal identity. A driver\u2019s license might prove I\u2019m legally allowed to drive in New Jersey, but it doesn\u2019t prove I\u2019m the person writing this blog. My GitHub commit history does. Likewise, airport security doesn\u2019t care about my GitHub handle. Different contexts demand different proofs.</p> <p>That\u2019s why Vero\u2019s model is simple: bring your own identity.</p>","tags":["digital-identity","trust","authentication","vero","blockchain","security"]},{"location":"2025/09/18/friction-makes-smoke-and-where-theres-smoke/#how-you-bring-it-temporal-identity-proofs","title":"How You Bring It: Temporal Identity Proofs","text":"<p>We explain TIPs more deeply in our lightpaper, but here's the basic flow:</p> <ol> <li>Identify trust anchors. These may be reputational (GitHub, LinkedIn, X) or credential-based (Okta ID, ID.me, SpruceID, etc.). You'll need several.</li> <li>Reputation takes time (sandcastles \u2014 use sparingly).</li> <li> <p>Credentials come from third parties, often mandated. Even in a DID world, you'll likely juggle multiple credentials.</p> </li> <li> <p>Select what to use. You decide which anchors to disclose. Selective disclosure is the rule: you control what you share.</p> </li> <li> <p>Generate tokens. Some services issue signed tokens (JWTs, OAuth assertions, etc.) proving account control. Many still don't. At launch, Vero will support only a handful of the common ones.</p> </li> <li> <p>Set expiration. TIPs are temporal by design. They're valid in the moment, not forever. You may choose to acknowledge that verification at a future date, but the tokens used to establish it are no longer needed.</p> </li> <li> <p>Bundle and encrypt. Packaging multiple tokens together risks deanonymization, so we encrypt \u2014 establishing a shared secret with the verifier in the process (read the white paper to understand why this is important).</p> </li> <li> <p>Verification. The verifier checks the tokens. If they pass, you've proven control.</p> </li> </ol> <p>That\u2019s it. TIPs are created locally and destroyed after use. No persistent templates. No central store.   There is some design space here where we are exploring tools to assist in the generation of multiple TIPs from a single set of tokens.  We are also evaluating the potential to store 'TIP verified' people in your contact list.</p>","tags":["digital-identity","trust","authentication","vero","blockchain","security"]},{"location":"2025/09/18/friction-makes-smoke-and-where-theres-smoke/#wheres-the-fire","title":"Where's the Fire?","text":"<p>At the start I said: friction makes smoke. And in Vero, smoke is a good thing. It signals intent. It shows someone is willing to take extra steps to prove trustworthiness.</p> <p>We flip the proverb on its head: Where there is smoke, there is safety. And when there is no smoke \u2014 that\u2019s when you should look for fire.</p> <p>What's your take on friction in digital identity verification? Have you found yourself trusting technology more than you probably should? I'd love to hear about your experiences with authentication systems that actually make you feel more secure. Drop me a line or share your thoughts.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2025-09-18</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 4</p> <p>File SHA-256: <code>c1faaab465c08d51...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2025-09-18 James Canterbury Create where_theres_smoke.md <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["digital-identity","trust","authentication","vero","blockchain","security"]},{"location":"2024/03/30/why-hasnt-blockchain-solved-our-supply-chain-issues-yet/","title":"Why hasn\u2019t blockchain solved our supply chain issues yet?","text":"<p>This is a drum I\u2019ve been beating for many years now - In my opinion using decentralized networks to create trusted and controlled visibility into the supply chains that literally affect the lives of billions of people every day is inevitable; but we are not there yet\u2026. In this post I summarize my latest thinking on the topic.</p> <p>It\u2019s hard to believe it was almost 4 years ago when COVID-19 was declared a pandemic by the World Health Organization.  I can remember hearing the horrible news stories about how despite the heroic efforts of our healthcare providers thousands of people were dying because they couldn\u2019t get access to life saving medicines and medical devices - the hospitals didn\u2019t even have enough personal protective equipment to keep our doctors and nurses safe.  The reality of an inadequate supply chain was a major contributor the largest public health crisis of our generation.  It affected all of us - I\u2019m pretty sure my family is still using the toilet paper we stock piled during the pandemic\u2026</p> <p>COVID put a spotlight on a systemic pervasive issue with our pharmaceutical supply chain - but the issue is far from new.  Back in 2013 the US FDA passed the Drug Supply Chain Safety Act (DSCSA) which put into law requirements around track-and-trace; given the rapid \u201cdigitization\u201d of supply chain this the mandate should have been pretty easy - it\u2019s just a matter of data standards and interfaces right? 10 years later only portions of DSCSA are even enforceable...  This is not due to lack of effort! Tremendous strides have been made in untangling a decades old, complex, multi-tiered supply chain.  It\u2019s just that these attempts have fallen short.</p> <p>I will admit that this post is a bit lengthy and opinionated, but I think that in order to appreciate the magnitude of change that is needed we have to dig a bit deeper into root cause.</p>","tags":["tokenization","supply chain"]},{"location":"2024/03/30/why-hasnt-blockchain-solved-our-supply-chain-issues-yet/#addressing-the-problem-how-supply-chains-got-this-way","title":"Addressing the problem: how supply chains got this way\u2026","text":"<p>The public health issues due to \u201clack of transparency\u201d in pharmaceutical supply chain have been documented and discussed at length in recent years. The fact that many government agencies are still citing \u201cSupply chain visibility, monitoring, and data sharing processes and platforms\u201d as a topic that requires research and innovation is conviction enough that current approaches are not working.  To be clear, it is not for lack of approaches, many software platforms solve this issue quite well, it is lack of adoption that prevents them from scaling; but I would argue that lack of adoption is driven by poor approach.\u00a0 Any solution that is counting on pressure from business partners or regulators to drive it\u2019s acceptance is approaching the issue from a point of compliance and is incentivizing participation via the \u201cstick\u201d vs. the \u201ccarrot\u201d- they will never be sustainable.</p> <p>For the purposes of this post I would rather frame the question of \u201cwhy did it get like this\u201d as opposed to \u201chow bad is the issue\u201d, because in understanding the why we can perhaps identify the core changes that are needed to correct the problem.  This will result (I hope) in a series of foundational methods/techniques that work well all of the time (\u201dalways get the little things right\u201d) and transcend any particular system or platform.</p> <p>So here we go from the ground up\u2026 fundamentally supply chain is the exchange of assets in return for payment. As complex as our supply chains can be today, they all revolve around facilitating this basic transaction.\u00a0 The \u201cchain\u201d part of supply chain implies that it is a series of transactions that are required to produce and distribute a product.\u00a0 At the edges of a supply chain the exchange of the physical good and payment can take place simultaneously.</p> <p>Think about a farmer selling a harvested wheat crop to a granary on one end of the supply supply spectrum and a hungry person buying a loaf of bread at a bakery - and lets assume these are both cash transactions paid on the spot with no returns for simplicity sake.  This is a simple example of settlement - meaning that one good (wheat or bread) was exchanged for another (money) and there are no residual commitments; the transaction was settled, we are both in agreement with the outcome, lets move on\u2026 \u00a0But as we work our way either downstream or upstream from the ends of the supply chain the quantities of product and the aggregate amounts paid for them begin to increase dramatically (the value of all the grain in the granary is far greater than the value of the individual harvest - the value of all the flour at the bakery is much higher than the price of one loaf of bread) and this aggregation / disaggregation gives rise to a split in the fundamental transaction.  </p> <p>If the granary is able to sell their entire stock to a miller it is very unlikely they will be paid the moment the grain leaves their silos, the miller will may only want to pay upon receipt and inspection of the grain; and even then they may ask for a delay in payment (I had a client once who demanded 120 day payment terms!).  As soon as this happens the exchange of the product and the payment are no longer asynchronous \u2013 there is a separation between the financial ownership and the physical custody.  This results in two interrelated, but separate transactions that require settlement, and this seemingly small rift allows for a vast complexity of buying and selling arrangements.  This should be intuitive to the reader - there is noting new in this centuries old process, but what is interesting is how we evolved systems to address this.  Lynn Aldren\u2019s recent book \u201cBroken Money\u201d does a fantastic job of explaining the monetary side of this evolution.</p> <p>But on the supply chain side we should look more closely at the computer systems in manufacturing enterprises.  They were used for financial accounting long before they were used for managing inventory.\u00a0 As manufacturers required more supplies, they developed procurement processes to facilitate purchasing and to manage their costs.\u00a0 On the other side of the house as customers purchased more products, they developed sales processes to manage their revenues - the monetary system was (and still is) the primary driver of their development.\u00a0 Inventory management was (and mostly still is) an input or output of procurement and sales.\u00a0 ERPs developed to bridge this gap (Enterprise Resource Planning where the resource is many forms of capital - materials, human, knowledge, money, etc) to create efficiencies within the individual organization, but not across organizations.  Granted if the organization is big enough it may seem like the ERP is controlling an entire supply chain, but it is always internally focused in the end - the jurisdiction of the ERP stops at the boundaries of the enterprise.  But supply chain by definition is bigger than any one enterprise so to make this process more efficient they developed ways to connect their procurement and sales systems with their suppliers and customers.  If those supplier and customers happen to be running the same ERP software this is a bit easier (much to the software providers benefit), and although the proliferation of standard Application Programming Interfaces (APIs) and data standards have made things much better as well, it is still complex and clunky - and in some cases completely broken.  Yet these procurement and sales based interfaces are the best we have and we rely on them today to connect our incredibly complex supply chains.</p> <p>All of that is to say that, interfaces between trading partners exist for the primary purpose of settling payments, the transfer of physical inventory is not usually settled, it is just an input or an output (IMO this isn\u2019t a bad thing - a single ERP or system can never/should never govern an entire supply chain).</p> <p>In addition to the maintenance, complexity, and reconciliation challenges inherent in this web of interfaces (I like the term API spaghetti) there is another nuanced issue.\u00a0 Payments are fungible where inventory is not.\u00a0 If company A pays company B $6 US dollars for a product, that $6 is (eventually) debited from company A and credited to company B.\u00a0 There is no requirement (or reason) to determine if company B used that same $6, or a fraction of it, to buy something from company C.\u00a0 But this is not the case for inventory, even if the individual units of inventory themselves are interchangeable (i.e., one screw in a box of 100) they are identifiable at some level \u2013 unit, lot/batch, product, etc.).\u00a0 So, to transfer this type of knowledge between trading partners the EDI 867 Product Transfer and Resale Report was designed explicitly to pass along product information.  And there is a bit of handshake between the trading partners if they use a corresponding EDI 997 Functional Acknowledgment to show the EDI was received and in good format - but that receipt does not guarantee that the inventory was moved out of one system and into another we still have to rely on a separate system of checks and controls to keep the information in the digital records in sync with the physical movements of inventory.\u00a0 This manifests itself in the form of something like this:</p> <p>Seller: \u201cUpon shipment of x units of product y in relation to order z, I will debit x units of product y from my inventory system\u201d Buyer: \u201cUpon receipt of x units of product y in relation to order z, I will credit x units of product y to my inventory system (in which I may or may not refer to this product as product y)</p> <p>These statements are only made for the sake of the buyers\u2019 and sellers\u2019 internal inventory management systems which they use for their internal planning.  They are only as strong as their internal processes and controls - they are not settlements. Further, just as with the payment, the history of transfers related to that particular product is not inherently tied to the product record, it is buried in the paper trail of the procurement process - and that paper trail usually breaks each time the product is transferred.</p> <p>The core issue at the bottom of all of this is the lack of a way to \u201csettle inventory transfers\u201d and a way to \u201clog and share a history of those transfers\u201d between trading partners.</p>","tags":["tokenization","supply chain"]},{"location":"2024/03/30/why-hasnt-blockchain-solved-our-supply-chain-issues-yet/#past-and-current-attempts-to-solve-and-why-they-arent-working","title":"Past and current attempts to solve (and why they aren\u2019t working)","text":"<p>Of course, the above description is over generalized and as I mentioned these are not new issues; there are many solutions either available or under development that aim to address inventory settlement - often rolled up into solutions for \u201csupply chain visibility\u201d or \u201ctrack-and-trace\u201d in response to the current description of the problem.  </p> <p>Lets also establish up front the fact that inventory settlements between training partners are confidential transactions.  Meaning that anyone not involved in the transaction should not be able to infer any information about it.  It might be OK for it to be known that two entities are trading partners (in pharma we have the concept of Authorized Trading Partners already established) but which products, how many, and how often are trade secrets.  If a competitor were able to get that information there would be many opportunities to arbitrage the supply chain.</p> <p>I find that all current solutions fall into one of 3 categories:</p> <p>1) Direct peer to peer connections: These are interfaces that arose from procurement processes and are built and maintained by trading partners that pass information about their product from one system to another through some mutually agreed upon protocol and data standard. These have benefitted greatly over time through better use of Application Programming Interfaces (APIs) and data standards that have developed out of necessity from organizations like IEEE and GS1, as well as the EDI standards referenced above. In a multi-tiered supply chain, many peer-to-peer interfaces will need to be \u201cstitched together\u201d to form the complete linage of a given product.</p> <p>Primary Benefits:</p> <ul> <li>Only requires the coordination of two parties (the buyer and the seller) for any one interface.</li> <li>The information exchanged between them is relatively easy to secure (i.e., their transactions are confidential)</li> </ul> <p>Primary Drawbacks:</p> <ul> <li>Difficult to scale, a buyer or seller may need to develop and support many discrete interfaces.</li> <li>Multiple interfaces often result in significant master data management issues and requires reconciliations and mappings.</li> <li>Product data does not \u201cinherently pull-through\u201d when the same product is exchanged multiple times.</li> </ul> <p>2) Private/consortia-controlled platforms or networks: A centralized solution that serves as a coordinator, and in some cases a governor for the supply chain. Often referred to as a \u201ccontrol tower\u201d these solutions work well when trading partners can agree upon a common trusted intermediary.</p> <p>Primary Benefits:</p> <ul> <li>Creates a high level of regulatory control over certain industries.</li> <li>Transactions can be kept confidential between trading partners and the trusted intermediary.</li> </ul> <p>Primary Drawbacks:</p> <ul> <li>Often only provide value when at scale and can be very difficult/expensive to establish.</li> <li>Inevitably lead to the centralization of risk and control and are inherently bureaucratic.</li> <li>Typically focus on solving one pervasive industry issue \u2013 not generally composable with other solutions/business models which lead to limited business value.</li> <li>For large multinational organizations or for industries with many small participants and a few large ones these create a barrier to entry.</li> </ul> <p>3) Public networks: Attempts to leverage large open networks (primarily blockchains) for supply chain use cases. It is necessary to leverage networks that support code/rule execution within the network (e.g., smart contracts)</p> <p>Primary Benefits:</p> <ul> <li>Can leverage blockchains for what they are good for (global coordination and management of digital scarcity)</li> <li>The \u201ccommon ledger\u201d eliminates reconciliation issues between systems and the transaction history lets the data \u201cpull through\u201d with a high level of integrity.</li> <li>Unique tokens can be used to represent products and track simultaneous claims on them (i.e., ownership vs. custody)</li> </ul> <p>Primary Drawbacks:</p> <ul> <li>Public networks are, well, public\u2026 enforcing the confidential nature of transactions as described above often leads to solutions that are built on top of the public network, and those solutions fall into category 1 or 2.</li> <li>By their nature the smart contracts (and their business logic within) that would govern supply chain use cases are also public.</li> </ul> <p>There is yet another issue inherent to our current state that is often pervasive across all three of the categories above:\u00a0 Because we don\u2019t actually settle inventory transactions most solutions attempt to recreate the paper trail of our current or legacy supply chain systems.\u00a0 In this way they leverage the type of data that is already exchanged in a peer-to-peer interface (this is often the easiest source of data to tap into).\u00a0 As discussed, those systems are designed around the procurement process and digitize documents such as Sales Orders, Purchase Orders, Advanced Shipping Notifications, Bill of Ladings, Goods Receipts, and Invoices; each of these documents make a reference to the product and quantity being transferred (among other procurement data such as price and payment terms), but not all of them represent specific claims on the actual product.\u00a0 </p> <p>In the pharmaceutical supply chain, an additional level of detail can be included in the documents which lists the specific serial numbers that are being transferred.\u00a0 But even if the data in the documents are 100% accurate because the buyer and the seller operate on two different systems this doesn\u2019t equate to settlement - again, there is no programmatic guarantee that the inventory is debited from one system and credited to another; while this might be achievable between a small ecosystem of trading partners it is impossible to achieve at scale without leveraging category 2 or category 3 solutions above because trading partners manage inventory on separate books.</p>","tags":["tokenization","supply chain"]},{"location":"2024/03/30/why-hasnt-blockchain-solved-our-supply-chain-issues-yet/#a-new-foundational-approach-to-settle-inventory-transactions-is-needed","title":"A new foundational approach to settle inventory transactions is needed","text":"<p>If instead of focusing on \u201crecreating the procurement paper trail\u201d in a digital fashion we were to focus on a way to fundamentally settle inventory transactions we might be able to build a better mouse trap.  </p> <p>This starts by thinking about the relationship between supply chain participants (I.e., one side of the trading partners used throughout this post), their products (the asset), and events that occurs at a point in time - the who, what, when (where, why and how are also very important but not as pertinent to the settlement - EPCIS 2.0 does a great job explaining these factors).  That relationship can be described as a \u201cclaim\u201d and for any given product there may be multiple claims held my multiple participants and any point in time.</p> <p>The most common example of claims are ownership and custody.  If your friend borrows your car you are still the owner, but your friend has custody.  Same car, different claim claims.  Further if you happen to lease the car from a bank, then the bank has a claim on the car as well, in the form of a lien.  As the owner you have the right to sell your car, but the bank can impair your right to sell if you don\u2019t clear the lien first, just like your friend can impair your ability to sell if they don\u2019t give it back (or if they wreck it\u2026). There are several different types of claims that describe the rights a participant has on a product, and it is not a standard set - \u201cclaims\u201d are a construct that can be defined per product class as and have \u201cclaim rules\u201d that describe how those claims effect one another.</p> <p>Settling an inventory transaction is in effect settling a claim - or rather creating a shared state that maps the claims to participants for a given product at a point in time.  Changes to this state over time in the form of a claim-state log become the history of who, what, and when.  If we also add in the ability for a claim holder to attest to a piece of information about a product then we can round out the where, why, and how components. Basic rules like \u201ca claim for a given product can only be held by one participant at a time\u201d or \u201cyou must hold a claim in order to transfer it\u201d can enforce the settlement.  </p> <p>The last component that is needed is a way to traverse the claims-state log forward and backward for a given product.  This would give us the \u201ctrack-and-trace\u201d components that we have been seeking, and if we were able to limit that ability to only those participants who hold (or have held in the past) claims on that product we would preserve the confidential nature of the supply chain.  A technique like this would be a method for Confidential Transactions with Embedded Rights (CSER) and is a public good that Zeroth Technology is looking to create for the world.</p>        \ud83d\udcdd Content Provenance      <p>Created: 2024-06-15</p> <p>Last Modified: 2025-09-19</p> <p>Total Revisions: 3</p> <p>File SHA-256: <code>7afda274b353bd0f...</code></p> <p>Recent Changes:</p> Date Author Change 2025-09-19 James Canterbury Added the github \"Content Provenance\" onto each... 2024-06-15 James Canterbury added a bunch of old blogs... <p>            View Full History on GitHub \u2192          </p> <p> This metadata provides cryptographic proof of this document's creation and modification history.        The SHA-256 hash can be used to verify the document's integrity, while the Git history shows its evolution over time. </p>","tags":["tokenization","supply chain"]},{"location":"archive/2025/","title":"September 2025","text":""},{"location":"archive/2024/","title":"December 2024","text":""},{"location":"archive/2023/","title":"November 2023","text":""},{"location":"category/identity--authenticity/","title":"Identity &amp; Authenticity","text":""},{"location":"category/agentic-ai-research/","title":"Agentic AI Research","text":""},{"location":"category/supply-chain/","title":"Supply Chain","text":""},{"location":"page/2/","title":"Welcome to the Zeroth Technology blog space","text":""},{"location":"page/3/","title":"Welcome to the Zeroth Technology blog space","text":""}]}